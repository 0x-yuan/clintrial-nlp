{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f2b8d1",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/0x-yuan/clintrial-nlp/blob/main/autogen_baseline.ipynb)\n\n# AutoGen æ¡†æ¶åŸºç·š - è‡¨åºŠè©¦é©— NLP\n\n## æ¦‚è¿°\n\næœ¬notebookå±•ç¤ºå¦‚ä½•ä½¿ç”¨Microsoft AutoGenå»ºæ§‹ä¸€å€‹å¤šä»£ç†ç³»çµ±ï¼Œç”¨æ–¼è‡¨åºŠè©¦é©—è‡ªç„¶èªè¨€æ¨ç†(NLI)ã€‚AutoGenåœ¨é€éå°è©±æ¨¡å¼é€²è¡Œå¤šä»£ç†å”ä½œæ–¹é¢è¡¨ç¾å“è¶Šã€‚\n\n## ğŸ“š å­¸ç¿’ç›®æ¨™\nå®Œæˆæœ¬æ•™å­¸å¾Œï¼Œæ‚¨å°‡å­¸æœƒï¼š\n- ç†è§£ AutoGen çš„å¤šä»£ç†å°è©±æ¡†æ¶\n- å»ºç«‹å°ˆæ¥­åŒ–çš„è§’è‰²ä»£ç†\n- å¯¦ä½œç¾¤çµ„èŠå¤©å”ä½œæ¨¡å¼\n- ç®¡ç†ä»£ç†é–“çš„ä»»å‹™åˆ†é…å’Œå”èª¿\n\n### ç‚ºä»€éº¼é¸æ“‡ AutoGenï¼Ÿ\n- **å¤šä»£ç†å°è©±**: ä»£ç†é–“çš„è‡ªç„¶å°è©±\n- **è§’è‰²å°ˆæ¥­åŒ–**: æ¯å€‹ä»£ç†éƒ½æœ‰ç‰¹å®šçš„å°ˆæ¥­çŸ¥è­˜\n- **å½ˆæ€§å”èª¿**: ç›£ç£è€…ç®¡ç†ä»»å‹™åˆ†é…\n- **å¼·å¥æ¨ç†**: å¤šé‡è¦–è§’æå‡æº–ç¢ºæ€§\n\n### ğŸ­ ä»£ç†æ¶æ§‹\næ ¹æ“šæ•™å­¸ææ–™ï¼Œæˆ‘å€‘å¯¦ä½œï¼š\n1. **ç›£ç£è€…**: å”èª¿ä»»å‹™ä¸¦ç®¡ç†å·¥ä½œæµç¨‹\n2. **é†«ç™‚å°ˆå®¶**: ç†è§£é†«å­¸è¡“èªå’Œæ¦‚å¿µ\n3. **æ•¸å€¼åˆ†æå“¡**: è™•ç†é‡åŒ–è³‡æ–™å’Œçµ±è¨ˆ\n4. **é‚è¼¯æª¢æŸ¥å“¡**: é©—è­‰é‚è¼¯é—œä¿‚\n5. **èšåˆå“¡**: åŸºæ–¼æ‰€æœ‰è¼¸å…¥åšå‡ºæœ€çµ‚æ±ºç­–\n\n> ğŸ’¬ **æ ¸å¿ƒæ¦‚å¿µ**: AutoGençš„ç¨ç‰¹ä¹‹è™•åœ¨æ–¼ä»£ç†é€éå°è©±é€²è¡Œå”ä½œï¼Œå°±åƒäººé¡åœ˜éšŠè¨è«–å•é¡Œä¸€æ¨£ã€‚é€™ç¨®è‡ªç„¶çš„äº’å‹•æ¨¡å¼ä½¿å¾—è¤‡é›œæ¨ç†éç¨‹æ›´åŠ é€æ˜å’Œå¯è§£é‡‹ã€‚"
  },
  {
   "cell_type": "code",
   "id": "syx92jslbf",
   "source": "# ğŸ”§ Colab ç’°å¢ƒè¨­ç½® - ä¸€éµå®‰è£ AutoGen ç›¸é—œå¥—ä»¶\n# âš ï¸ æ³¨æ„ï¼šAutoGen åœ¨2024å¹´é€²è¡Œäº†é‡å¤§APIé‡æ§‹ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸ç©©å®š\n# é€™å€‹cellæœƒéœé»˜å®‰è£Microsoft AutoGenå¤šä»£ç†å”ä½œæ¡†æ¶æ‰€éœ€çš„å¥—ä»¶\n!pip install -q pyautogen python-dotenv pandas tqdm\n!pip install -q google-generativeai gdown\n\n# å˜—è©¦å®‰è£æ–°ç‰ˆAutoGençµ„ä»¶ï¼ˆå¯èƒ½ä¸å®Œå…¨å…¼å®¹ï¼‰\n!pip install -q autogen-agentchat autogen-ext ag2\n\nprint(\"âœ… AutoGen å¤šä»£ç†å”ä½œæ¡†æ¶å®‰è£å®Œæˆï¼\")\nprint(\"âš ï¸ æ³¨æ„ï¼šAutoGen APIåœ¨2024å¹´ç™¼ç”Ÿé‡å¤§è®ŠåŒ–ï¼Œå¦‚æœé‡åˆ°å•é¡Œè«‹æª¢æŸ¥:\")\nprint(\"  1. ä½¿ç”¨ ag2 è€Œé autogen\")\nprint(\"  2. æª¢æŸ¥ autogen-agentchat å’Œ autogen-ext ç‰ˆæœ¬å…¼å®¹æ€§\")\nprint(\"  3. æŸäº›Geminiæ•´åˆåŠŸèƒ½å¯èƒ½ä¸ç©©å®š\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "alumru56r6k",
   "source": "# ğŸ“¥ å¾ Google Drive ä¸‹è¼‰è¨“ç·´è³‡æ–™\n# é€™å€‹cellæœƒè‡ªå‹•ä¸‹è¼‰ä¸¦è§£å£“ç¸® clinicaltrial-nlp.zipï¼Œç¢ºä¿åœ¨Colabä¸­å¯ä»¥ç›´æ¥é‹è¡Œ\nimport os\nimport gdown\nimport zipfile\nimport shutil\n\n# Google Drive zip æª”æ¡ˆ ID\nfile_id = \"15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR\"\nzip_url = f\"https://drive.google.com/uc?id={file_id}\"\nzip_filename = \"clinicaltrial-nlp.zip\"\n\n# æª¢æŸ¥æ˜¯å¦å·²æœ‰è¨“ç·´è³‡æ–™\nif not os.path.exists(\"training_data\"):\n    print(\"ğŸ“¥ å¾ Google Drive ä¸‹è¼‰ clinicaltrial-nlp.zip...\")\n    print(\"âš ï¸ å¦‚æœä¸‹è¼‰å¤±æ•—ï¼Œè«‹ç¢ºèª:\")\n    print(\"1. Google Drive é€£çµçš„æ¬Šé™è¨­å®šç‚º 'çŸ¥é“é€£çµçš„ä½¿ç”¨è€…'\")\n    print(\"2. ç¶²è·¯é€£ç·šæ­£å¸¸\")\n    print(f\"3. æª”æ¡ˆé€£çµ: {zip_url}\")\n    \n    try:\n        # ä¸‹è¼‰ zip æª”æ¡ˆ\n        print(\"ğŸ“¥ æ­£åœ¨ä¸‹è¼‰ zip æª”æ¡ˆ...\")\n        gdown.download(zip_url, zip_filename, quiet=False)\n        \n        # è§£å£“ç¸®æª”æ¡ˆ\n        print(\"ğŸ“¦ æ­£åœ¨è§£å£“ç¸®æª”æ¡ˆ...\")\n        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n            zip_ref.extractall(\".\")\n        \n        # ç§»å‹• training_data åˆ°æ­£ç¢ºä½ç½®ï¼ˆå¦‚æœåœ¨å­è³‡æ–™å¤¾ä¸­ï¼‰\n        if os.path.exists(\"clintrial-nlp/training_data\") and not os.path.exists(\"training_data\"):\n            print(\"ğŸ“ ç§»å‹• training_data åˆ°æ­£ç¢ºä½ç½®...\")\n            shutil.move(\"clintrial-nlp/training_data\", \"training_data\")\n            # æ¸…ç†è§£å£“ç¸®çš„è³‡æ–™å¤¾\n            if os.path.exists(\"clintrial-nlp\"):\n                shutil.rmtree(\"clintrial-nlp\")\n            if os.path.exists(\"__MACOSX\"):  # æ¸…ç† macOS ç”¢ç”Ÿçš„éš±è—æª”æ¡ˆ\n                shutil.rmtree(\"__MACOSX\")\n        \n        # æ¸…ç† zip æª”æ¡ˆ\n        os.remove(zip_filename)\n        print(\"âœ… è¨“ç·´è³‡æ–™ä¸‹è¼‰ä¸¦è§£å£“ç¸®å®Œæˆï¼\")\n        \n    except Exception as e:\n        print(f\"âŒ ä¸‹è¼‰å¤±æ•—: {e}\")\n        print(\"\\nğŸ”§ æ‰‹å‹•è§£æ±ºæ–¹æ¡ˆ:\")\n        print(\"1. é»æ“Šæ­¤é€£çµä¸‹è¼‰ zip æª”æ¡ˆ:\")\n        print(\"   https://drive.google.com/file/d/15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR/view?usp=sharing\")\n        print(\"2. ä¸Šå‚³ zip æª”æ¡ˆåˆ° Colab\")\n        print(\"3. è§£å£“ç¸®å¾Œé‡æ–°åŸ·è¡Œå¾ŒçºŒçš„ cells\")\n        \n        # å‰µå»ºä¸€å€‹æç¤ºæª”æ¡ˆ\n        os.makedirs(\"training_data\", exist_ok=True)\n        with open(\"training_data/DOWNLOAD_INSTRUCTIONS.txt\", \"w\", encoding=\"utf-8\") as f:\n            f.write(\"è«‹æ‰‹å‹•ä¸‹è¼‰ä¸¦è§£å£“ç¸® clinicaltrial-nlp.zip:\\n\")\n            f.write(\"https://drive.google.com/file/d/15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR/view?usp=sharing\\n\")\n        \n        print(\"\\nğŸ“ å·²å‰µå»ºä¸‹è¼‰æŒ‡ç¤ºæª”æ¡ˆæ–¼ training_data/DOWNLOAD_INSTRUCTIONS.txt\")\nelse:\n    print(\"âœ… è¨“ç·´è³‡æ–™å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰\")\n\n# æª¢æŸ¥ä¸‹è¼‰çš„è³‡æ–™çµæ§‹\nif os.path.exists(\"training_data\"):\n    contents = os.listdir(\"training_data\")\n    print(f\"ğŸ“‚ è³‡æ–™å¤¾å…§å®¹: {contents}\")\n    if os.path.exists(\"training_data/CT json\"):\n        ct_files = len([f for f in os.listdir(\"training_data/CT json\") if f.endswith('.json')])\n        print(f\"ğŸ“„ æ‰¾åˆ° {ct_files} å€‹è‡¨åºŠè©¦é©—JSONæª”æ¡ˆ\")\n    else:\n        print(\"âš ï¸ æ‰¾ä¸åˆ° 'CT json' å­è³‡æ–™å¤¾ï¼Œè«‹æª¢æŸ¥ä¸‹è¼‰æ˜¯å¦å®Œæ•´\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "su14hvy4xor",
   "source": "# ğŸ§ª æº–å‚™æ¸¬è©¦è³‡æ–™é›†\nimport json\n\ndef create_test_data_if_needed():\n    if not os.path.exists(\"test.json\"):\n        try:\n            with open(\"training_data/train.json\", \"r\", encoding=\"utf-8\") as f:\n                train_data = json.load(f)\n            test_data = dict(list(train_data.items())[:100])\n            with open(\"test.json\", \"w\", encoding=\"utf-8\") as f:\n                json.dump(test_data, f, indent=2, ensure_ascii=False)\n            print(f\"âœ… å·²å‰µå»ºæ¸¬è©¦è³‡æ–™é›†ï¼ŒåŒ…å« {len(test_data)} å€‹æ¨£æœ¬\")\n        except Exception as e:\n            print(f\"âŒ å‰µå»ºæ¸¬è©¦è³‡æ–™å¤±æ•—: {e}\")\n    else:\n        print(\"âœ… test.json å·²å­˜åœ¨\")\n\ncreate_test_data_if_needed()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f2e9c5d2",
   "metadata": {},
   "source": "## ç’°å¢ƒè¨­ç½®å’Œå®‰è£\n\né¦–å…ˆï¼Œè®“æˆ‘å€‘å®‰è£å¿…è¦çš„å¥—ä»¶ä¸¦è¨­ç½®ç’°å¢ƒï¼š\n\n> ğŸ“ **èªªæ˜**: AutoGenæ¡†æ¶æ”¯æ´å¤šç¨®LLMå¾Œç«¯ï¼Œé€™è£¡æˆ‘å€‘ä½¿ç”¨Google Geminiæ¨¡å‹é€²è¡Œç¤ºç¯„ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a5c8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(\"âœ… Environment loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import autogen\n",
    "from typing import Dict, List, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6d7e8",
   "metadata": {},
   "source": [
    "## Data Loading and Utilities\n",
    "\n",
    "Let's create utility functions to load clinical trial data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clinical_trial(trial_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load clinical trial data from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        trial_id: The NCT identifier for the clinical trial\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing trial data or error information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_path = os.path.join(\"training_data\", \"CT json\", f\"{trial_id}.json\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        return {\"error\": f\"Clinical trial {trial_id} not found\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error loading {trial_id}: {str(e)}\"}\n",
    "\n",
    "def load_dataset(filepath: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load training or test dataset.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the JSON dataset file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Test data loading\n",
    "sample_trial = load_clinical_trial(\"NCT00066573\")\n",
    "print(f\"âœ… Data loading functions ready. Sample trial loaded: {sample_trial.get('Clinical Trial ID', 'Error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent_config",
   "metadata": {},
   "source": "## AutoGen ä»£ç†é…ç½®\n\nç¾åœ¨è®“æˆ‘å€‘ä½¿ç”¨Google Geminié…ç½®å¤šä»£ç†ç³»çµ±ã€‚æ¯å€‹ä»£ç†éƒ½æœ‰å°ˆé–€çš„è§’è‰²å’Œç‰¹å®šçš„æŒ‡ä»¤ï¼š\n\n> ğŸ¤– **æŠ€è¡“èªªæ˜**: AutoGençš„AssistantAgenté¡åˆ¥å…è¨±æˆ‘å€‘å‰µå»ºå…·æœ‰ç‰¹å®šå°ˆæ¥­çŸ¥è­˜å’Œè¡Œç‚ºæ¨¡å¼çš„AIä»£ç†ï¼Œé€™äº›ä»£ç†å¯ä»¥é€éç¾¤çµ„èŠå¤©é€²è¡Œå”ä½œã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config_setup",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ¤– Setup Google Gemini 2.5 Flash model for AutoGen\n# AutoGen will use this configuration to initialize all agent LLM backends\nimport google.generativeai as genai\nimport os\n\nprint(\"ğŸ”‘ Checking API key configuration...\")\n\n# Support both GEMINI_API_KEY and GOOGLE_API_KEY environment variables\napi_key = os.getenv(\"GEMINI_API_KEY\") or os.getenv(\"GOOGLE_API_KEY\")\n\nif not api_key:\n    print(\"âš ï¸ Please set GEMINI_API_KEY or GOOGLE_API_KEY environment variable\")\n    print(\"You can set it in Colab's 'Secrets' panel or use:\")\n    print(\"import os\")\n    print(\"os.environ['GEMINI_API_KEY'] = 'your-api-key'\")\n    print(\"or\")\n    print(\"os.environ['GOOGLE_API_KEY'] = 'your-api-key'\")\n    # Create fake config to avoid errors\n    config_list = [{\"model\": \"gpt-3.5-turbo\", \"api_key\": \"fake\"}]\n    api_key = \"fake\"\nelse:\n    print(f\"âœ… Found API key: {api_key[:8]}...{api_key[-4:]}\")\n    genai.configure(api_key=api_key)\n    print(\"âœ… Google Gemini API configured\")\n    \n    # Test API connection\n    try:\n        model = genai.GenerativeModel(\"gemini-2.5-flash\")\n        response = model.generate_content(\"Hello, respond with 'API test successful'\")\n        print(f\"âœ… API connection test successful: {response.text[:50]}...\")\n    except Exception as e:\n        print(f\"âš ï¸ API connection test failed: {e}\")\n\n# AutoGen's LLM configuration - Use Google Gemini 2.5 Flash\n# Based on official AutoGen documentation for Gemini integration\nconfig_list = [\n    {\n        \"model\": \"gemini-2.5-flash\",\n        \"api_key\": api_key,\n        \"api_type\": \"google\"  # This is crucial for AutoGen to recognize it as Gemini\n    }\n]\n\n# Test configuration\nprint(f\"ğŸ“‹ LLM configuration model: {config_list[0]['model']}\")\nprint(f\"ğŸ“‹ API type: {config_list[0]['api_type']}\")\n\nllm_config = {\n    \"config_list\": config_list,\n    \"temperature\": 0.1,  # Low temperature for consistent results\n    \"timeout\": 120,\n    \"seed\": 42,  # Ensure reproducibility\n}\n\nprint(\"âœ… AutoGen configuration ready, will use Google Gemini 2.5 Flash model\")\nprint(f\"ğŸ”§ Configuration details: temperature={llm_config['temperature']}, timeout={llm_config['timeout']}s\")"
  },
  {
   "cell_type": "markdown",
   "id": "agent_definitions",
   "metadata": {},
   "source": [
    "## Agent Definitions\n",
    "\n",
    "Let's define each agent with their specific roles and capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_agents",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Supervisor Agent - Coordinates the overall workflow\n",
    "supervisor = autogen.AssistantAgent(\n",
    "    name=\"supervisor\",\n",
    "    system_message=\"\"\"You are the Supervisor Agent responsible for coordinating clinical trial analysis.\n",
    "    \n",
    "    Your responsibilities:\n",
    "    1. Break down complex clinical trial questions into manageable tasks\n",
    "    2. Assign tasks to appropriate specialist agents\n",
    "    3. Coordinate information flow between agents\n",
    "    4. Ensure all aspects of the statement are thoroughly analyzed\n",
    "    5. Guide the final decision-making process\n",
    "    \n",
    "    When given a statement and clinical trial data, you should:\n",
    "    - First understand what the statement claims\n",
    "    - Identify which sections of the trial are relevant\n",
    "    - Delegate specific analysis tasks to specialist agents\n",
    "    - Collect and synthesize their findings\n",
    "    \n",
    "    Always maintain a systematic approach and ensure thorough analysis.\n",
    "    \"\"\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# 2. Medical Expert Agent - Handles medical terminology and concepts\n",
    "medical_expert = autogen.AssistantAgent(\n",
    "    name=\"medical_expert\",\n",
    "    system_message=\"\"\"You are the Medical Expert Agent specializing in clinical trial analysis.\n",
    "    \n",
    "    Your expertise includes:\n",
    "    1. Medical terminology and clinical concepts\n",
    "    2. Understanding of trial phases, interventions, and outcomes\n",
    "    3. Clinical significance of findings\n",
    "    4. Medical relationships and causations\n",
    "    5. Safety profiles and adverse events\n",
    "    \n",
    "    When analyzing statements:\n",
    "    - Focus on the medical accuracy and clinical relevance\n",
    "    - Identify key medical terms and their implications\n",
    "    - Assess whether medical claims align with trial evidence\n",
    "    - Consider clinical context and medical plausibility\n",
    "    \n",
    "    Provide clear medical reasoning for your analysis.\n",
    "    \"\"\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# 3. Numerical Analyzer Agent - Processes quantitative data\n",
    "numerical_analyzer = autogen.AssistantAgent(\n",
    "    name=\"numerical_analyzer\",\n",
    "    system_message=\"\"\"You are the Numerical Analyzer Agent specializing in quantitative analysis.\n",
    "    \n",
    "    Your expertise includes:\n",
    "    1. Statistical analysis and interpretation\n",
    "    2. Numerical comparisons and calculations\n",
    "    3. Percentage, ratios, and statistical significance\n",
    "    4. Data ranges, confidence intervals, and error margins\n",
    "    5. Trend analysis and numerical patterns\n",
    "    \n",
    "    When analyzing statements:\n",
    "    - Extract and verify all numerical claims\n",
    "    - Perform calculations to validate numerical relationships\n",
    "    - Compare stated numbers with trial data\n",
    "    - Assess statistical significance and clinical relevance\n",
    "    - Identify any numerical inconsistencies or errors\n",
    "    \n",
    "    Be precise and show your calculations when relevant.\n",
    "    \"\"\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# 4. Logic Checker Agent - Validates logical relationships\n",
    "logic_checker = autogen.AssistantAgent(\n",
    "    name=\"logic_checker\",\n",
    "    system_message=\"\"\"You are the Logic Checker Agent responsible for validating logical reasoning.\n",
    "    \n",
    "    Your expertise includes:\n",
    "    1. Logical consistency and coherence\n",
    "    2. Cause-and-effect relationships\n",
    "    3. Conditional logic and implications\n",
    "    4. Contradiction detection\n",
    "    5. Multi-step reasoning validation\n",
    "    \n",
    "    When analyzing statements:\n",
    "    - Evaluate the logical structure of claims\n",
    "    - Check for internal consistency\n",
    "    - Identify logical fallacies or contradictions\n",
    "    - Assess the validity of inferences\n",
    "    - Ensure conclusions follow from premises\n",
    "    \n",
    "    Focus on the logical soundness rather than medical or numerical details.\n",
    "    \"\"\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# 5. Aggregator Agent - Makes final decisions\n",
    "aggregator = autogen.AssistantAgent(\n",
    "    name=\"aggregator\",\n",
    "    system_message=\"\"\"You are the Aggregator Agent responsible for making final entailment decisions.\n",
    "    \n",
    "    Your responsibilities:\n",
    "    1. Synthesize findings from all specialist agents\n",
    "    2. Weigh different types of evidence appropriately\n",
    "    3. Resolve conflicts between agent analyses\n",
    "    4. Make the final Entailment vs Contradiction decision\n",
    "    5. Provide confidence assessment for the decision\n",
    "    \n",
    "    Decision criteria:\n",
    "    - ENTAILMENT: Statement is directly supported by trial data\n",
    "    - CONTRADICTION: Statement is refuted by trial data\n",
    "    \n",
    "    When making decisions:\n",
    "    - Consider input from Medical Expert, Numerical Analyzer, and Logic Checker\n",
    "    - Prioritize evidence that directly addresses the statement\n",
    "    - Be conservative: if evidence is unclear, consider context carefully\n",
    "    - Always provide reasoning for your final decision\n",
    "    \n",
    "    Output format: \"FINAL_DECISION: [Entailment/Contradiction]\"\n",
    "    \"\"\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# User proxy for managing the conversation\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").find(\"FINAL_DECISION:\") >= 0,\n",
    "    code_execution_config=False,\n",
    ")\n",
    "\n",
    "print(\"âœ… All agents created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "group_chat",
   "metadata": {},
   "source": "## ç¾¤çµ„èŠå¤©è¨­ç½®\n\næˆ‘å€‘å°‡è¨­ç½®ä¸€å€‹ç¾¤çµ„èŠå¤©ï¼Œè®“ä»£ç†å€‘èƒ½å¤ å”ä½œåˆ†ææ¯å€‹é™³è¿°ï¼š\n\n> ğŸ—£ï¸ **å”ä½œèªªæ˜**: GroupChatå…è¨±å¤šå€‹ä»£ç†åœ¨åŒä¸€å€‹å°è©±ä¸­äº’å‹•ï¼Œæ¯å€‹ä»£ç†è¼ªæµç™¼è¨€ï¼Œåˆ†äº«ä»–å€‘çš„åˆ†æè¦‹è§£ï¼Œå°±åƒåœ˜éšŠæœƒè­°ä¸€æ¨£ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_groupchat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create group chat with all agents\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[supervisor, medical_expert, numerical_analyzer, logic_checker, aggregator, user_proxy],\n",
    "    messages=[],\n",
    "    max_round=15,  # Allow sufficient rounds for thorough analysis\n",
    "    speaker_selection_method=\"round_robin\",  # Ensure all agents participate\n",
    ")\n",
    "\n",
    "# Create group chat manager\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat,\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "print(\"âœ… Group chat configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": [
    "## Clinical Trial Analysis Pipeline\n",
    "\n",
    "Now let's create our main analysis pipeline that coordinates the multi-agent analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_pipeline",
   "metadata": {},
   "outputs": [],
   "source": "def analyze_statement_with_agents(statement: str, primary_id: str, secondary_id: Optional[str] = None, \n                                section_id: Optional[str] = None) -> str:\n    \"\"\"\n    Analyze a statement using the multi-agent system.\n    \n    Args:\n        statement: The natural language statement to analyze\n        primary_id: Primary clinical trial ID\n        secondary_id: Secondary trial ID for comparison statements\n        section_id: Relevant section of the trial (Eligibility, Intervention, Results, Adverse Events)\n        \n    Returns:\n        Final decision: 'Entailment' or 'Contradiction'\n    \"\"\"\n    \n    print(f\"ğŸ” é–‹å§‹åˆ†æ: {statement[:50]}...\")\n    \n    # Load clinical trial data\n    primary_data = load_clinical_trial(primary_id)\n    secondary_data = None\n    if secondary_id:\n        secondary_data = load_clinical_trial(secondary_id)\n    \n    # æª¢æŸ¥è³‡æ–™æ˜¯å¦è¼‰å…¥æˆåŠŸ\n    if \"error\" in primary_data:\n        print(f\"âŒ ä¸»è¦è©¦é©—è³‡æ–™è¼‰å…¥å¤±æ•—: {primary_data['error']}\")\n        return \"Contradiction\"\n    \n    # Prepare secondary data section\n    secondary_section = \"\"\n    if secondary_data:\n        secondary_section = f\"SECONDARY TRIAL DATA:\\n{json.dumps(secondary_data, indent=2)}\"\n    \n    # Prepare the analysis context (ç¸®çŸ­ä»¥é¿å… token é™åˆ¶)\n    context = f\"\"\"\nCLINICAL TRIAL ANALYSIS TASK\n\nStatement: \"{statement}\"\nPrimary Trial: {primary_id}\nSecondary Trial: {secondary_id or \"N/A\"}\nSection: {section_id or \"All sections\"}\n\nPRIMARY TRIAL DATA (summarized):\nTitle: {primary_data.get('Official Title', 'N/A')}\nPhase: {primary_data.get('Phase', 'N/A')}\nStatus: {primary_data.get('Overall Status', 'N/A')}\nConditions: {primary_data.get('Conditions', 'N/A')}\nInterventions: {primary_data.get('Interventions', 'N/A')}\n\n{secondary_section}\n\nTASK: Analyze whether this statement is ENTAILMENT (supported) or CONTRADICTION (refuted).\nSupervisor: Coordinate analysis with medical_expert, numerical_analyzer, logic_checker, then aggregator decides.\n\"\"\"\n    \n    print(\"ğŸ¤– å•Ÿå‹•å¤šä»£ç†åˆ†æ...\")\n    \n    # Reset group chat for new analysis\n    groupchat.messages = []\n    \n    # Start the multi-agent analysis\n    try:\n        print(\"ğŸ“ é–‹å§‹ä»£ç†å°è©±...\")\n        \n        # ä½¿ç”¨æ›´è©³ç´°çš„éŒ¯èª¤è™•ç†\n        user_proxy.initiate_chat(\n            manager,\n            message=context,\n            clear_history=True\n        )\n        \n        print(f\"ğŸ’¬ å°è©±å®Œæˆï¼Œå…± {len(groupchat.messages)} æ¢è¨Šæ¯\")\n        \n        # Debug: é¡¯ç¤ºæœ€å¾Œå¹¾æ¢è¨Šæ¯\n        if len(groupchat.messages) > 0:\n            print(\"ğŸ“‹ æœ€å¾Œçš„è¨Šæ¯:\")\n            for i, msg in enumerate(groupchat.messages[-3:]):  # é¡¯ç¤ºæœ€å¾Œ3æ¢è¨Šæ¯\n                sender = msg.get(\"name\", \"unknown\")\n                content = msg.get(\"content\", \"\")[:100] + \"...\"\n                print(f\"  {i+1}. {sender}: {content}\")\n        \n        # Extract final decision from the last message\n        final_message = groupchat.messages[-1][\"content\"] if groupchat.messages else \"\"\n        \n        # Parse the final decision\n        if \"FINAL_DECISION: Entailment\" in final_message:\n            decision = \"Entailment\"\n        elif \"FINAL_DECISION: Contradiction\" in final_message:\n            decision = \"Contradiction\"\n        else:\n            # Fallback: look for decision keywords in the final message\n            if \"entailment\" in final_message.lower():\n                decision = \"Entailment\"\n            else:\n                decision = \"Contradiction\"\n        \n        print(f\"âœ… åˆ†æå®Œæˆ: {decision}\")\n        return decision\n                \n    except Exception as e:\n        print(f\"âŒ ä»£ç†åˆ†æéŒ¯èª¤: {e}\")\n        print(f\"éŒ¯èª¤é¡å‹: {type(e).__name__}\")\n        import traceback\n        print(f\"è©³ç´°éŒ¯èª¤: {traceback.format_exc()}\")\n        return \"Contradiction\"  # Conservative fallback\n\nprint(\"âœ… Analysis pipeline ready\")"
  },
  {
   "cell_type": "markdown",
   "id": "test_example",
   "metadata": {},
   "source": "# ğŸ§ª æ¸¬è©¦å¤šä»£ç†ç³»çµ±æ˜¯å¦æ­£å¸¸å·¥ä½œ\ndef test_agent_system():\n    \"\"\"æ¸¬è©¦å¤šä»£ç†ç³»çµ±æ˜¯å¦èƒ½æ­£å¸¸å”ä½œ\"\"\"\n    print(\"ğŸ§ª æ¸¬è©¦å¤šä»£ç†ç³»çµ±...\")\n    \n    # ç°¡å–®çš„æ¸¬è©¦æ¡ˆä¾‹\n    test_context = \"\"\"\n    è«‹å„ä½ä»£ç†å”ä½œåˆ†æï¼š\n    é™³è¿°: \"é€™å€‹è©¦é©—æœ‰100ååƒèˆ‡è€…\"\n    è©¦é©—è³‡æ–™: {\"enrollment\": 100, \"participants\": \"100 patients enrolled\"}\n    \n    è«‹ supervisor å”èª¿ï¼Œmedical_expert åˆ†æé†«å­¸ç›¸é—œæ€§ï¼Œnumerical_analyzer æª¢æŸ¥æ•¸å­—ï¼Œ\n    logic_checker é©—è­‰é‚è¼¯ï¼Œæœ€å¾Œ aggregator åšå‡ºæ±ºå®šã€‚\n    \n    è«‹ç¢ºä¿æ¯å€‹ä»£ç†éƒ½åƒèˆ‡è¨è«–ï¼Œæœ€å¾Œè¼¸å‡º \"FINAL_DECISION: Entailment\" æˆ– \"FINAL_DECISION: Contradiction\"\n    \"\"\"\n    \n    # é‡ç½®ç¾¤çµ„èŠå¤©\n    groupchat.messages = []\n    \n    try:\n        print(\"ğŸ“ å•Ÿå‹•æ¸¬è©¦å°è©±...\")\n        import time\n        start_time = time.time()\n        \n        user_proxy.initiate_chat(\n            manager,\n            message=test_context,\n            clear_history=True\n        )\n        \n        end_time = time.time()\n        duration = end_time - start_time\n        \n        print(f\"â±ï¸ å°è©±è€—æ™‚: {duration:.2f} ç§’\")\n        print(f\"ğŸ’¬ ç¸½è¨Šæ¯æ•¸: {len(groupchat.messages)}\")\n        \n        if len(groupchat.messages) == 0:\n            print(\"âŒ æ²’æœ‰ç”Ÿæˆä»»ä½•å°è©±è¨Šæ¯ï¼\")\n            return False\n        elif duration < 1.0:\n            print(\"âš ï¸ å°è©±æ™‚é–“å¤ªçŸ­ï¼Œå¯èƒ½æ²’æœ‰çœŸæ­£åŸ·è¡Œ LLM èª¿ç”¨\")\n            return False\n        else:\n            print(\"âœ… å¤šä»£ç†ç³»çµ±æ­£å¸¸å·¥ä½œ\")\n            \n            # é¡¯ç¤ºä»£ç†åƒèˆ‡æƒ…æ³\n            agents_participated = set()\n            for msg in groupchat.messages:\n                if \"name\" in msg:\n                    agents_participated.add(msg[\"name\"])\n            \n            print(f\"ğŸ¤– åƒèˆ‡çš„ä»£ç†: {list(agents_participated)}\")\n            return True\n            \n    except Exception as e:\n        print(f\"âŒ æ¸¬è©¦å¤±æ•—: {e}\")\n        return False\n\n# é‹è¡Œæ¸¬è©¦\ntest_result = test_agent_system()\nprint(f\"\\nğŸ¯ ç³»çµ±æ¸¬è©¦çµæœ: {'é€šé' if test_result else 'å¤±æ•—'}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample statement\n",
    "test_statement = \"there is a 13.2% difference between the results from the two the primary trial cohorts\"\n",
    "test_primary_id = \"NCT00066573\"\n",
    "\n",
    "print(f\"Testing with statement: '{test_statement}'\")\n",
    "print(f\"Primary trial: {test_primary_id}\")\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Run the analysis\n",
    "result = analyze_statement_with_agents(\n",
    "    statement=test_statement,\n",
    "    primary_id=test_primary_id,\n",
    "    section_id=\"Results\"\n",
    ")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"FINAL RESULT: {result}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation",
   "metadata": {},
   "source": [
    "## Evaluation on Training Data\n",
    "\n",
    "Let's evaluate our AutoGen system on a subset of training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_data = load_dataset(\"training_data/train.json\")\n",
    "print(f\"Loaded {len(train_data)} training examples\")\n",
    "\n",
    "# Evaluate on first 20 examples (adjust as needed for testing)\n",
    "sample_size = 20\n",
    "examples = list(train_data.items())[:sample_size]\n",
    "\n",
    "print(f\"\\nEvaluating on {len(examples)} examples...\")\n",
    "\n",
    "results = []\n",
    "correct = 0\n",
    "\n",
    "for i, (uuid, example) in enumerate(tqdm(examples, desc=\"Processing\")):\n",
    "    try:\n",
    "        statement = example.get(\"Statement\")\n",
    "        primary_id = example.get(\"Primary_id\")\n",
    "        secondary_id = example.get(\"Secondary_id\")\n",
    "        section_id = example.get(\"Section_id\")\n",
    "        expected = example.get(\"Label\")\n",
    "        \n",
    "        if not statement or not primary_id:\n",
    "            results.append({\n",
    "                \"uuid\": uuid,\n",
    "                \"expected\": expected,\n",
    "                \"predicted\": \"SKIPPED\",\n",
    "                \"correct\": False\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Get prediction from multi-agent system\n",
    "        predicted = analyze_statement_with_agents(\n",
    "            statement=statement,\n",
    "            primary_id=primary_id,\n",
    "            secondary_id=secondary_id,\n",
    "            section_id=section_id\n",
    "        )\n",
    "        \n",
    "        # Check if correct\n",
    "        is_correct = (predicted.strip() == expected.strip())\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            \n",
    "        results.append({\n",
    "            \"uuid\": uuid,\n",
    "            \"statement\": statement[:100] + \"...\" if len(statement) > 100 else statement,\n",
    "            \"expected\": expected,\n",
    "            \"predicted\": predicted,\n",
    "            \"correct\": is_correct\n",
    "        })\n",
    "        \n",
    "        print(f\"Example {i+1}: {expected} -> {predicted} {'âœ…' if is_correct else 'âŒ'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example {i+1}: {e}\")\n",
    "        results.append({\n",
    "            \"uuid\": uuid,\n",
    "            \"expected\": expected,\n",
    "            \"predicted\": \"ERROR\",\n",
    "            \"correct\": False\n",
    "        })\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / len(examples) if examples else 0\n",
    "print(f\"\\nğŸ“Š AutoGen Results:\")\n",
    "print(f\"Accuracy: {accuracy:.2%} ({correct}/{len(examples)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error_analysis",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Let's examine some examples where our system made mistakes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze_errors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show incorrect predictions for analysis\n",
    "incorrect_results = [r for r in results if not r[\"correct\"] and r[\"predicted\"] not in [\"SKIPPED\", \"ERROR\"]]\n",
    "\n",
    "print(f\"\\nğŸ” Error Analysis ({len(incorrect_results)} incorrect predictions):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, result in enumerate(incorrect_results[:5]):  # Show first 5 errors\n",
    "    print(f\"\\nError #{i+1}:\")\n",
    "    print(f\"Statement: {result['statement']}\")\n",
    "    print(f\"Expected: {result['expected']}\")\n",
    "    print(f\"Predicted: {result['predicted']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_submission",
   "metadata": {},
   "source": [
    "## Generate Submission File\n",
    "\n",
    "Let's create a submission file for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_autogen_submission(test_file=\"test.json\", output_file=\"autogen_submission.json\", sample_size=None):\n",
    "    \"\"\"\n",
    "    Generate submission file using AutoGen multi-agent system.\n",
    "    \n",
    "    Args:\n",
    "        test_file: Path to test data\n",
    "        output_file: Output submission file\n",
    "        sample_size: Number of examples to process (None for all)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = load_dataset(test_file)\n",
    "    if not test_data:\n",
    "        print(f\"âŒ Could not load test data from {test_file}\")\n",
    "        return\n",
    "    \n",
    "    examples = list(test_data.items())\n",
    "    if sample_size:\n",
    "        examples = examples[:sample_size]\n",
    "        \n",
    "    print(f\"ğŸš€ Generating AutoGen predictions for {len(examples)} examples...\")\n",
    "    \n",
    "    submission = {}\n",
    "    \n",
    "    for i, (uuid, example) in enumerate(tqdm(examples, desc=\"AutoGen Processing\")):\n",
    "        try:\n",
    "            statement = example.get(\"Statement\")\n",
    "            primary_id = example.get(\"Primary_id\")\n",
    "            secondary_id = example.get(\"Secondary_id\")\n",
    "            section_id = example.get(\"Section_id\")\n",
    "            \n",
    "            if not statement or not primary_id:\n",
    "                submission[uuid] = {\"Prediction\": \"Contradiction\"}  # Default fallback\n",
    "                continue\n",
    "                \n",
    "            # Get prediction from multi-agent system\n",
    "            prediction = analyze_statement_with_agents(\n",
    "                statement=statement,\n",
    "                primary_id=primary_id,\n",
    "                secondary_id=secondary_id,\n",
    "                section_id=section_id\n",
    "            )\n",
    "            \n",
    "            submission[uuid] = {\"Prediction\": prediction}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {uuid}: {e}\")\n",
    "            submission[uuid] = {\"Prediction\": \"Contradiction\"}  # Conservative fallback\n",
    "    \n",
    "    # Save submission file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(submission, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… AutoGen submission saved to {output_file}\")\n",
    "    return submission\n",
    "\n",
    "# Generate submission for a small sample (change sample_size as needed)\n",
    "autogen_submission = generate_autogen_submission(\n",
    "    test_file=\"test.json\", \n",
    "    output_file=\"autogen_submission.json\",\n",
    "    sample_size=10  # Process only 10 examples for testing\n",
    ")\n",
    "\n",
    "print(f\"Generated predictions for {len(autogen_submission)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": "## çµè«–èˆ‡æ´å¯Ÿ\n\n### AutoGen æ¡†æ¶å„ªå‹¢ï¼š\n1. **å¤šè¦–è§’åˆ†æ**: æ¯å€‹ä»£ç†å¸¶ä¾†å°ˆé–€çš„å°ˆæ¥­çŸ¥è­˜\n2. **çµæ§‹åŒ–å”ä½œ**: æ˜ç¢ºçš„è§’è‰²å’Œè²¬ä»»\n3. **å½ˆæ€§å”èª¿**: ç›£ç£è€…ç®¡ç†è¤‡é›œçš„å·¥ä½œæµç¨‹\n4. **å¼·å¥æ¨ç†**: å¤šé‡è§€é»æå‡æ±ºç­–å“è³ª\n5. **å¯è§£é‡‹æ€§**: ä»£ç†å°è©±æä¾›æ¨ç†éç¨‹çš„æ´å¯Ÿ\n\n### é—œéµåŠŸèƒ½å±•ç¤ºï¼š\n- **è§’è‰²å°ˆæ¥­åŒ–**: é†«ç™‚ã€æ•¸å€¼å’Œé‚è¼¯åˆ†æ\n- **ç¾¤çµ„èŠå¤©å”èª¿**: ä»£ç†é€éçµæ§‹åŒ–å°è©±å”ä½œ\n- **ç³»çµ±åŒ–æ–¹æ³•**: ç›£ç£è€…ç¢ºä¿å…¨é¢åˆ†æ\n- **éŒ¯èª¤è™•ç†**: é‚Šç·£æ¡ˆä¾‹çš„å„ªé›…å›é€€\n- **å¯æ“´å±•æ¶æ§‹**: æ˜“æ–¼æ·»åŠ æ–°çš„å°ˆæ¥­ä»£ç†\n\n### å„ªåŒ–æ©Ÿæœƒï¼š\n1. **æç¤ºå·¥ç¨‹**: å¾®èª¿ä»£ç†æŒ‡ä»¤ä»¥æå‡æ•ˆèƒ½\n2. **ä»£ç†å”èª¿**: æ”¹å–„å°è©±æµç¨‹å’Œæ±ºç­–åˆ¶å®š\n3. **éŒ¯èª¤åˆ†æ**: åˆ©ç”¨å¤±æ•—æ¡ˆä¾‹æ”¹é€²ä»£ç†æ¨ç†\n4. **æ•ˆèƒ½èª¿æ•´**: æœ€ä½³åŒ–æ¨¡å‹åƒæ•¸å’Œå°è©±æ¨¡å¼\n5. **é ˜åŸŸçŸ¥è­˜**: ç´å…¥æ›´å¤šé†«å­¸é ˜åŸŸå°ˆæ¥­çŸ¥è­˜\n\n### ä½•æ™‚ä½¿ç”¨ AutoGenï¼š\n- éœ€è¦å¤šé‡è¦–è§’çš„è¤‡é›œæ¨ç†ä»»å‹™\n- å¾è§’è‰²å°ˆæ¥­åŒ–ä¸­å—ç›Šçš„å•é¡Œ\n- å¯è§£é‡‹æ€§å¾ˆé‡è¦çš„å ´æ™¯\n- éœ€è¦å¼·å¥å”ä½œæ±ºç­–çš„æ‡‰ç”¨\n\n## ğŸ“ å­¸ç¿’é‡é»ç¸½çµ\n- **å°è©±å¼å”ä½œ**: ä»£ç†é€éè‡ªç„¶å°è©±é€²è¡Œå”ä½œ\n- **è§’è‰²åˆ†å·¥**: æ¯å€‹ä»£ç†å°ˆæ³¨æ–¼ç‰¹å®šåˆ†æé ˜åŸŸ\n- **ç›£ç£å”èª¿**: ä¸­å¤®å”èª¿ç¢ºä¿ç³»çµ±åŒ–åˆ†æ\n- **é€æ˜æ¨ç†**: å°è©±éç¨‹æä¾›æ±ºç­–é€æ˜åº¦\n\nAutoGenåœ¨çµæ§‹åŒ–å¤šä»£ç†å”ä½œæ–¹é¢è¡¨ç¾å‡ºè‰²ï¼Œä¸¦ç‚ºæ¨ç†éç¨‹æä¾›å„ªç§€çš„é€æ˜åº¦ï¼Œä½¿å…¶æˆç‚ºè¤‡é›œè‡¨åºŠåˆ†æä»»å‹™çš„ç†æƒ³é¸æ“‡ã€‚"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}