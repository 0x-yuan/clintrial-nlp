{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/0x-yuan/clintrial-nlp/blob/main/langchain_baseline.ipynb)\n\n# LangChain/LangGraph æ¡†æ¶åŸºç·š - è‡¨åºŠè©¦é©— NLP\n\n## æ¦‚è¿°\n\næœ¬notebookå±•ç¤ºå¦‚ä½•ä½¿ç”¨LangChainå’ŒLangGraphå»ºæ§‹ä¸€å€‹æœ‰ç‹€æ…‹çš„ã€åŸºæ–¼åœ–çš„ä»£ç†ç³»çµ±ï¼Œç”¨æ–¼è‡¨åºŠè©¦é©—è‡ªç„¶èªè¨€æ¨ç†(NLI)ã€‚LangChainæä¾›äº†æœ€æˆç†Ÿå’ŒåŠŸèƒ½è±å¯Œçš„LLMæ‡‰ç”¨ç”Ÿæ…‹ç³»çµ±ã€‚\n\n## ğŸ“š å­¸ç¿’ç›®æ¨™\nå®Œæˆæœ¬æ•™å­¸å¾Œï¼Œæ‚¨å°‡å­¸æœƒï¼š\n- ç†è§£ LangChain ç”Ÿæ…‹ç³»çµ±å’Œ LangGraph çš„ç‹€æ…‹ç®¡ç†\n- å»ºæ§‹åŸºæ–¼åœ–çš„å·¥ä½œæµç¨‹å’Œç‹€æ…‹ç®¡ç†\n- å¯¦ä½œè¤‡é›œçš„å¤šæ­¥é©Ÿæ¨ç†æµç¨‹\n- ä½¿ç”¨ SQLite æª¢æŸ¥é»é€²è¡Œå°è©±æŒçºŒæ€§\n\n### ç‚ºä»€éº¼é¸æ“‡ LangChain/LangGraphï¼Ÿ\n- **æˆç†Ÿç”Ÿæ…‹ç³»çµ±**: æœ€å®Œå–„çš„LLMæ‡‰ç”¨æ¡†æ¶\n- **è±å¯Œæ•´åˆ**: å»£æ³›çš„å·¥å…·å’Œæœå‹™æ•´åˆ\n- **æœ‰ç‹€æ…‹å·¥ä½œæµç¨‹**: LangGraphæ”¯æ´è¤‡é›œçš„æœ‰ç‹€æ…‹ä»£ç†äº’å‹•\n- **é€²éšæ¨¡å¼**: æ”¯æ´è¤‡é›œçš„æ¨ç†å’Œæ±ºç­–æ¨¡å¼  \n- **ç¤¾ç¾¤æ”¯æ´**: é¾å¤§ç¤¾ç¾¤å’Œè±å¯Œæ–‡æª”\n- **ç”Ÿç”¢å°±ç·’**: åœ¨çœ¾å¤šå¯¦éš›æ‡‰ç”¨ä¸­ç¶“éå¯¦æˆ°é©—è­‰\n\n### ğŸ”„ ä½¿ç”¨ LangGraph çš„ä»£ç†æ¶æ§‹\næˆ‘å€‘å°‡å¯¦ä½œä¸€å€‹åŸºæ–¼åœ–çš„å·¥ä½œæµç¨‹ï¼ŒåŒ…å«æœ‰ç‹€æ…‹ä»£ç†ï¼š\n1. **è‡¨åºŠè³‡æ–™æå–å™¨**: è™•ç†å’Œçµæ§‹åŒ–è©¦é©—è³‡æ–™\n2. **é†«ç™‚åˆ†æç¯€é»**: å°ˆæ¥­é†«ç™‚æ¨ç†\n3. **çµ±è¨ˆåˆ†æç¯€é»**: æ•¸å€¼å’Œçµ±è¨ˆé©—è­‰\n4. **é‚è¼¯é©—è­‰ç¯€é»**: é‚è¼¯ä¸€è‡´æ€§æª¢æŸ¥\n5. **æ±ºç­–ç¶œåˆç¯€é»**: æœ€çµ‚è˜Šå«åˆ†é¡\n6. **ç‹€æ…‹ç®¡ç†**: æ•´å€‹åˆ†æå·¥ä½œæµç¨‹çš„æŒä¹…ç‹€æ…‹\n\n> ğŸ”— **é—œéµæ¦‚å¿µ**: LangGraph å°‡å·¥ä½œæµç¨‹è¦–ç‚ºæœ‰å‘åœ–ï¼Œå…¶ä¸­æ¯å€‹ç¯€é»éƒ½æ˜¯ä¸€å€‹åŠŸèƒ½ï¼Œé‚Šä»£è¡¨è³‡æ–™æµå‹•ã€‚é€™ä½¿å¾—è¤‡é›œçš„å¤šæ­¥é©Ÿæ¨ç†è®Šå¾—å¯ç®¡ç†å’Œå¯è¿½è¹¤ã€‚"
  },
  {
   "cell_type": "code",
   "id": "a324shquzxc",
   "source": "# ğŸ”§ Colab ç’°å¢ƒè¨­ç½® - ä¸€éµå®‰è£ LangChain/LangGraph ç›¸é—œå¥—ä»¶\n# é€™å€‹cellæœƒéœé»˜å®‰è£æ‰€æœ‰LangChainç”Ÿæ…‹ç³»çµ±æ‰€éœ€çš„å¥—ä»¶\n!pip install -q langchain langchain-google-genai langgraph python-dotenv pandas tqdm\n!pip install -q langchain-core langchain-community gdown\n\nprint(\"âœ… LangChain/LangGraph ç”Ÿæ…‹ç³»çµ±å®‰è£å®Œæˆï¼å¯ä»¥é–‹å§‹å»ºæ§‹æœ‰ç‹€æ…‹çš„ä»£ç†å·¥ä½œæµç¨‹äº†\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "rjfaidv2gzk",
   "source": "# ğŸ“¥ å¾ Google Drive ä¸‹è¼‰è¨“ç·´è³‡æ–™\n# é€™å€‹cellæœƒè‡ªå‹•ä¸‹è¼‰ä¸¦è§£å£“ç¸® clinicaltrial-nlp.zipï¼Œç¢ºä¿åœ¨Colabä¸­å¯ä»¥ç›´æ¥é‹è¡Œ\nimport os\nimport gdown\nimport zipfile\nimport shutil\n\n# Google Drive zip æª”æ¡ˆ ID\nfile_id = \"15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR\"\nzip_url = f\"https://drive.google.com/uc?id={file_id}\"\nzip_filename = \"clinicaltrial-nlp.zip\"\n\n# æª¢æŸ¥æ˜¯å¦å·²æœ‰è¨“ç·´è³‡æ–™\nif not os.path.exists(\"training_data\"):\n    print(\"ğŸ“¥ å¾ Google Drive ä¸‹è¼‰ clinicaltrial-nlp.zip...\")\n    print(\"âš ï¸ å¦‚æœä¸‹è¼‰å¤±æ•—ï¼Œè«‹ç¢ºèª:\")\n    print(\"1. Google Drive é€£çµçš„æ¬Šé™è¨­å®šç‚º 'çŸ¥é“é€£çµçš„ä½¿ç”¨è€…'\")\n    print(\"2. ç¶²è·¯é€£ç·šæ­£å¸¸\")\n    print(f\"3. æª”æ¡ˆé€£çµ: {zip_url}\")\n    \n    try:\n        # ä¸‹è¼‰ zip æª”æ¡ˆ\n        print(\"ğŸ“¥ æ­£åœ¨ä¸‹è¼‰ zip æª”æ¡ˆ...\")\n        gdown.download(zip_url, zip_filename, quiet=False)\n        \n        # è§£å£“ç¸®æª”æ¡ˆ\n        print(\"ğŸ“¦ æ­£åœ¨è§£å£“ç¸®æª”æ¡ˆ...\")\n        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n            zip_ref.extractall(\".\")\n        \n        # ç§»å‹• training_data åˆ°æ­£ç¢ºä½ç½®ï¼ˆå¦‚æœåœ¨å­è³‡æ–™å¤¾ä¸­ï¼‰\n        if os.path.exists(\"clintrial-nlp/training_data\") and not os.path.exists(\"training_data\"):\n            print(\"ğŸ“ ç§»å‹• training_data åˆ°æ­£ç¢ºä½ç½®...\")\n            shutil.move(\"clintrial-nlp/training_data\", \"training_data\")\n            # æ¸…ç†è§£å£“ç¸®çš„è³‡æ–™å¤¾\n            if os.path.exists(\"clintrial-nlp\"):\n                shutil.rmtree(\"clintrial-nlp\")\n            if os.path.exists(\"__MACOSX\"):  # æ¸…ç† macOS ç”¢ç”Ÿçš„éš±è—æª”æ¡ˆ\n                shutil.rmtree(\"__MACOSX\")\n        \n        # æ¸…ç† zip æª”æ¡ˆ\n        os.remove(zip_filename)\n        print(\"âœ… è¨“ç·´è³‡æ–™ä¸‹è¼‰ä¸¦è§£å£“ç¸®å®Œæˆï¼\")\n        \n    except Exception as e:\n        print(f\"âŒ ä¸‹è¼‰å¤±æ•—: {e}\")\n        print(\"\\nğŸ”§ æ‰‹å‹•è§£æ±ºæ–¹æ¡ˆ:\")\n        print(\"1. é»æ“Šæ­¤é€£çµä¸‹è¼‰ zip æª”æ¡ˆ:\")\n        print(\"   https://drive.google.com/file/d/15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR/view?usp=sharing\")\n        print(\"2. ä¸Šå‚³ zip æª”æ¡ˆåˆ° Colab\")\n        print(\"3. è§£å£“ç¸®å¾Œé‡æ–°åŸ·è¡Œå¾ŒçºŒçš„ cells\")\n        \n        # å‰µå»ºä¸€å€‹æç¤ºæª”æ¡ˆ\n        os.makedirs(\"training_data\", exist_ok=True)\n        with open(\"training_data/DOWNLOAD_INSTRUCTIONS.txt\", \"w\", encoding=\"utf-8\") as f:\n            f.write(\"è«‹æ‰‹å‹•ä¸‹è¼‰ä¸¦è§£å£“ç¸® clinicaltrial-nlp.zip:\\n\")\n            f.write(\"https://drive.google.com/file/d/15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR/view?usp=sharing\\n\")\n        \n        print(\"\\nğŸ“ å·²å‰µå»ºä¸‹è¼‰æŒ‡ç¤ºæª”æ¡ˆæ–¼ training_data/DOWNLOAD_INSTRUCTIONS.txt\")\nelse:\n    print(\"âœ… è¨“ç·´è³‡æ–™å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰\")\n\n# æª¢æŸ¥ä¸‹è¼‰çš„è³‡æ–™çµæ§‹\nif os.path.exists(\"training_data\"):\n    contents = os.listdir(\"training_data\")\n    print(f\"ğŸ“‚ è³‡æ–™å¤¾å…§å®¹: {contents}\")\n    if os.path.exists(\"training_data/CT json\"):\n        ct_files = len([f for f in os.listdir(\"training_data/CT json\") if f.endswith('.json')])\n        print(f\"ğŸ“„ æ‰¾åˆ° {ct_files} å€‹è‡¨åºŠè©¦é©—JSONæª”æ¡ˆ\")\n    else:\n        print(\"âš ï¸ æ‰¾ä¸åˆ° 'CT json' å­è³‡æ–™å¤¾ï¼Œè«‹æª¢æŸ¥ä¸‹è¼‰æ˜¯å¦å®Œæ•´\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bcdr6copqi",
   "source": "# ğŸ§ª æº–å‚™æ¸¬è©¦è³‡æ–™é›†\nimport json\n\ndef create_test_data_if_needed():\n    if not os.path.exists(\"test.json\"):\n        try:\n            with open(\"training_data/train.json\", \"r\", encoding=\"utf-8\") as f:\n                train_data = json.load(f)\n            test_data = dict(list(train_data.items())[:100])\n            with open(\"test.json\", \"w\", encoding=\"utf-8\") as f:\n                json.dump(test_data, f, indent=2, ensure_ascii=False)\n            print(f\"âœ… å·²å‰µå»ºæ¸¬è©¦è³‡æ–™é›†ï¼ŒåŒ…å« {len(test_data)} å€‹æ¨£æœ¬\")\n        except Exception as e:\n            print(f\"âŒ å‰µå»ºæ¸¬è©¦è³‡æ–™å¤±æ•—: {e}\")\n    else:\n        print(\"âœ… test.json å·²å­˜åœ¨\")\n\ncreate_test_data_if_needed()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": "## ç’°å¢ƒè¨­ç½®å’Œå®‰è£\n\né¦–å…ˆï¼Œè®“æˆ‘å€‘è¨­ç½®ç’°å¢ƒä¸¦åŒ¯å…¥å¿…è¦çš„å‡½å¼åº«ï¼š\n\n> ğŸ“ **èªªæ˜**: LangChain ç”Ÿæ…‹ç³»çµ±åŒ…å«å¤šå€‹çµ„ä»¶ï¼Œæˆ‘å€‘éœ€è¦åŒ¯å…¥æ ¸å¿ƒåŠŸèƒ½ã€Google Geminiæ•´åˆä»¥åŠLangGraphç‹€æ…‹ç®¡ç†ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(\"âœ… Environment loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependencies",
   "metadata": {},
   "outputs": [],
   "source": "# åŒ¯å…¥å¿…è¦çš„å‡½å¼åº«\n# LangChain ç”Ÿæ…‹ç³»çµ±æä¾›è±å¯Œçš„LLMæ‡‰ç”¨é–‹ç™¼å·¥å…·\nimport json\nimport pandas as pd\nfrom tqdm import tqdm\nfrom typing import Dict, List, Any, Optional, TypedDict, Annotated\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# LangChain æ ¸å¿ƒçµ„ä»¶\nfrom langchain_google_genai import ChatGoogleGenerativeAI  # Google Geminiæ•´åˆ\nfrom langchain_core.messages import HumanMessage, AIMessage, SystemMessage  # è¨Šæ¯é¡å‹\nfrom langchain_core.prompts import ChatPromptTemplate, PromptTemplate  # æç¤ºæ¨¡æ¿\nfrom langchain_core.output_parsers import StrOutputParser  # è¼¸å‡ºè§£æå™¨\nfrom langchain_core.runnables import RunnablePassthrough  # å¯é‹è¡Œéˆ\nfrom langchain.schema import Document  # æ–‡æª”çµæ§‹\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter  # æ–‡å­—åˆ†å‰²å™¨\n\n# LangGraph ç‹€æ…‹ç®¡ç†çµ„ä»¶\nfrom langgraph.graph import StateGraph, END  # ç‹€æ…‹åœ–å’ŒçµæŸç¯€é»\nfrom langgraph.checkpoint.sqlite import SqliteSaver  # SQLiteæª¢æŸ¥é»ä¿å­˜å™¨\nfrom langgraph.prebuilt import ToolExecutor  # å·¥å…·åŸ·è¡Œå™¨\nimport operator  # é‹ç®—ç¬¦\n\nprint(\"âœ… æ‰€æœ‰LangChain/LangGraphçµ„ä»¶åŒ¯å…¥æˆåŠŸ\")\n\n# ğŸ§© æ¶æ§‹èªªæ˜ï¼šLangChainæä¾›æ¨¡çµ„åŒ–çµ„ä»¶ï¼ŒLangGraphå¢åŠ ç‹€æ…‹ç®¡ç†å’Œåœ–åŸ·è¡Œèƒ½åŠ›"
  },
  {
   "cell_type": "markdown",
   "id": "data_utils",
   "metadata": {},
   "source": [
    "## Data Loading and Utilities\n",
    "\n",
    "Let's create utility functions for loading and processing clinical trial data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clinical_trial(trial_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load clinical trial data from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        trial_id: The NCT identifier for the clinical trial\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing trial data or error information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_path = os.path.join(\"training_data\", \"CT json\", f\"{trial_id}.json\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        return {\"error\": f\"Clinical trial {trial_id} not found\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error loading {trial_id}: {str(e)}\"}\n",
    "\n",
    "def load_dataset(filepath: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load training or test dataset.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the JSON dataset file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return {}\n",
    "\n",
    "def create_trial_documents(trial_data: Dict[str, Any]) -> List[Document]:\n",
    "    \"\"\"Create LangChain documents from trial data for better processing.\n",
    "    \n",
    "    Args:\n",
    "        trial_data: Clinical trial data dictionary\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects for LangChain processing\n",
    "    \"\"\"\n",
    "    if \"error\" in trial_data:\n",
    "        return [Document(page_content=f\"Error: {trial_data['error']}\", metadata={\"section\": \"error\"})]\n",
    "    \n",
    "    documents = []\n",
    "    trial_id = trial_data.get(\"Clinical Trial ID\", \"Unknown\")\n",
    "    \n",
    "    # Create documents for each section\n",
    "    sections = {\n",
    "        \"Eligibility\": trial_data.get(\"Eligibility\", []),\n",
    "        \"Intervention\": trial_data.get(\"Intervention\", []),\n",
    "        \"Results\": trial_data.get(\"Results\", []),\n",
    "        \"Adverse_Events\": trial_data.get(\"Adverse_Events\", [])\n",
    "    }\n",
    "    \n",
    "    for section_name, section_data in sections.items():\n",
    "        if section_data:\n",
    "            if isinstance(section_data, list):\n",
    "                content = \"\\n\".join(str(item) for item in section_data)\n",
    "            else:\n",
    "                content = str(section_data)\n",
    "            \n",
    "            documents.append(Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    \"trial_id\": trial_id,\n",
    "                    \"section\": section_name\n",
    "                }\n",
    "            ))\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Test utilities\n",
    "sample_trial = load_clinical_trial(\"NCT00066573\")\n",
    "sample_docs = create_trial_documents(sample_trial)\n",
    "print(f\"âœ… Data utilities ready. Sample trial: {sample_trial.get('Clinical Trial ID', 'Error')}\")\n",
    "print(f\"Created {len(sample_docs)} documents from sample trial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_setup",
   "metadata": {},
   "source": "## Model Configuration\n\nSet up the ChatGoogleGenerativeAI model for LangChain:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_config",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize ChatGoogleGenerativeAI model\nllm = ChatGoogleGenerativeAI(\n    model=\"gemini-2.5-flash\",\n    temperature=0.1,  # Low temperature for consistent results\n    google_api_key=os.getenv(\"GEMINI_API_KEY\")\n)\n\n# Initialize checkpointer for state persistence\ncheckpointer = SqliteSaver.from_conn_string(\":memory:\")\n\nprint(\"âœ… Model and checkpointer configured\")"
  },
  {
   "cell_type": "markdown",
   "id": "state_definition",
   "metadata": {},
   "source": "## ç‹€æ…‹å®šç¾©\n\nç‚ºæˆ‘å€‘çš„LangGraphå·¥ä½œæµç¨‹å®šç¾©ç‹€æ…‹çµæ§‹ï¼š\n\n> ğŸ”„ **ç‹€æ…‹ç®¡ç†èªªæ˜**: LangGraphçš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ç¶­è­·ä¸€å€‹åœ¨æ•´å€‹å·¥ä½œæµç¨‹ä¸­æŒçºŒå­˜åœ¨çš„ç‹€æ…‹ã€‚é€™å€‹ç‹€æ…‹åŒ…å«æ‰€æœ‰åˆ†ææ­¥é©Ÿçš„è¼¸å…¥ã€ä¸­é–“çµæœå’Œæœ€çµ‚è¼¸å‡ºã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "state_schema",
   "metadata": {},
   "outputs": [],
   "source": "class ClinicalAnalysisState(TypedDict):\n    \"\"\"è‡¨åºŠè©¦é©—åˆ†æå·¥ä½œæµç¨‹çš„ç‹€æ…‹æ¶æ§‹ã€‚\"\"\"\n    \n    # è¼¸å…¥è³‡æ–™\n    statement: str  # è¦åˆ†æçš„é™³è¿°\n    primary_trial_id: str  # ä¸»è¦è©¦é©—ID\n    secondary_trial_id: Optional[str]  # æ¬¡è¦è©¦é©—IDï¼ˆæ¯”è¼ƒæ™‚ä½¿ç”¨ï¼‰\n    focus_section: Optional[str]  # é—œæ³¨çš„è©¦é©—å€æ®µ\n    \n    # è©¦é©—è³‡æ–™\n    primary_trial_data: Dict[str, Any]  # ä¸»è¦è©¦é©—çš„å®Œæ•´è³‡æ–™\n    secondary_trial_data: Optional[Dict[str, Any]]  # æ¬¡è¦è©¦é©—è³‡æ–™\n    trial_documents: List[Document]  # LangChainæ–‡æª”æ ¼å¼çš„è©¦é©—è³‡æ–™\n    \n    # åˆ†æçµæœ\n    medical_analysis: Optional[str]  # é†«ç™‚å°ˆå®¶åˆ†æçµæœ\n    statistical_analysis: Optional[str]  # çµ±è¨ˆåˆ†æçµæœ\n    logical_analysis: Optional[str]  # é‚è¼¯åˆ†æçµæœ\n    \n    # æœ€çµ‚æ±ºç­–\n    final_decision: Optional[str]  # æœ€çµ‚çš„è˜Šå«/çŸ›ç›¾æ±ºç­–\n    confidence_score: Optional[float]  # ä¿¡å¿ƒåˆ†æ•¸(0-1)\n    \n    # å·¥ä½œæµç¨‹æ§åˆ¶\n    next_action: Optional[str]  # ä¸‹ä¸€å€‹è¦åŸ·è¡Œçš„å‹•ä½œ\n    error_messages: Annotated[List[str], operator.add]  # éŒ¯èª¤è¨Šæ¯ç´¯ç©\n\nprint(\"âœ… ç‹€æ…‹æ¶æ§‹å®šç¾©å®Œæˆ\")\n\n# ğŸ“Š ç‹€æ…‹èªªæ˜ï¼šé€™å€‹TypedDictå®šç¾©äº†å·¥ä½œæµç¨‹ä¸­æ¯å€‹ç¯€é»å¯ä»¥è®€å–å’Œä¿®æ”¹çš„è³‡æ–™çµæ§‹"
  },
  {
   "cell_type": "markdown",
   "id": "node_definitions",
   "metadata": {},
   "source": [
    "## Node Definitions\n",
    "\n",
    "Define the analysis nodes for our LangGraph workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_extractor_node",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clinical_data_extractor(state: ClinicalAnalysisState) -> ClinicalAnalysisState:\n",
    "    \"\"\"Extract and structure clinical trial data.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load primary trial data\n",
    "        primary_data = load_clinical_trial(state[\"primary_trial_id\"])\n",
    "        state[\"primary_trial_data\"] = primary_data\n",
    "        \n",
    "        # Load secondary trial data if provided\n",
    "        if state[\"secondary_trial_id\"]:\n",
    "            secondary_data = load_clinical_trial(state[\"secondary_trial_id\"])\n",
    "            state[\"secondary_trial_data\"] = secondary_data\n",
    "        \n",
    "        # Create documents for processing\n",
    "        documents = create_trial_documents(primary_data)\n",
    "        if state[\"secondary_trial_data\"]:\n",
    "            documents.extend(create_trial_documents(state[\"secondary_trial_data\"]))\n",
    "        \n",
    "        state[\"trial_documents\"] = documents\n",
    "        state[\"next_action\"] = \"medical_analysis\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        state[\"error_messages\"].append(f\"Data extraction error: {str(e)}\")\n",
    "        state[\"next_action\"] = \"end\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"âœ… Data extractor node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical_analysis_node",
   "metadata": {},
   "outputs": [],
   "source": [
    "def medical_analysis_node(state: ClinicalAnalysisState) -> ClinicalAnalysisState:\n",
    "    \"\"\"Perform medical analysis of the statement against trial data.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Create medical analysis prompt\n",
    "        medical_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=\"\"\"\n",
    "You are a Medical Expert specializing in clinical trial analysis.\n",
    "Your role is to analyze statements from a medical perspective and assess their accuracy against clinical trial evidence.\n",
    "\n",
    "Focus on:\n",
    "- Medical terminology accuracy\n",
    "- Clinical relevance and significance\n",
    "- Medical plausibility of claims\n",
    "- Clinical context and implications\n",
    "- Medical evidence alignment\n",
    "\n",
    "Provide a thorough medical analysis and end with:\n",
    "MEDICAL_VERDICT: [SUPPORTS/CONTRADICTS/UNCLEAR] - brief reasoning\n",
    "            \"\"\".strip()),\n",
    "            HumanMessage(content=\"\"\"\n",
    "STATEMENT TO ANALYZE: \"{statement}\"\n",
    "\n",
    "CLINICAL TRIAL EVIDENCE:\n",
    "{trial_evidence}\n",
    "\n",
    "Please provide your medical analysis of this statement against the trial evidence.\n",
    "            \"\"\".strip())\n",
    "        ])\n",
    "        \n",
    "        # Prepare trial evidence\n",
    "        trial_evidence = \"\"\n",
    "        for doc in state[\"trial_documents\"]:\n",
    "            if state[\"focus_section\"] and doc.metadata.get(\"section\") != state[\"focus_section\"]:\n",
    "                continue\n",
    "            trial_evidence += f\"\\n{doc.metadata.get('section', 'Unknown')}:\\n{doc.page_content}\\n\"\n",
    "        \n",
    "        # Run medical analysis\n",
    "        medical_chain = medical_prompt | llm | StrOutputParser()\n",
    "        medical_result = medical_chain.invoke({\n",
    "            \"statement\": state[\"statement\"],\n",
    "            \"trial_evidence\": trial_evidence\n",
    "        })\n",
    "        \n",
    "        state[\"medical_analysis\"] = medical_result\n",
    "        state[\"next_action\"] = \"statistical_analysis\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        state[\"error_messages\"].append(f\"Medical analysis error: {str(e)}\")\n",
    "        state[\"next_action\"] = \"statistical_analysis\"  # Continue with next analysis\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"âœ… Medical analysis node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statistical_analysis_node",
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_analysis_node(state: ClinicalAnalysisState) -> ClinicalAnalysisState:\n",
    "    \"\"\"Perform statistical and numerical analysis.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Create statistical analysis prompt\n",
    "        statistical_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=\"\"\"\n",
    "You are a Statistical Analyst specializing in clinical trial data analysis.\n",
    "Your role is to analyze numerical claims, statistics, and quantitative relationships in clinical trials.\n",
    "\n",
    "Focus on:\n",
    "- Numerical accuracy and verification\n",
    "- Statistical significance and validity\n",
    "- Quantitative relationships and comparisons\n",
    "- Data calculations and mathematical reasoning\n",
    "- Confidence intervals and error margins\n",
    "\n",
    "Perform detailed calculations and end with:\n",
    "STATISTICAL_VERDICT: [ACCURATE/INACCURATE/PARTIALLY_ACCURATE] - numerical reasoning\n",
    "            \"\"\".strip()),\n",
    "            HumanMessage(content=\"\"\"\n",
    "STATEMENT TO ANALYZE: \"{statement}\"\n",
    "\n",
    "CLINICAL TRIAL DATA:\n",
    "{trial_evidence}\n",
    "\n",
    "Please perform statistical analysis of the numerical claims in this statement.\n",
    "            \"\"\".strip())\n",
    "        ])\n",
    "        \n",
    "        # Prepare trial evidence (focus on Results section for statistical data)\n",
    "        trial_evidence = \"\"\n",
    "        for doc in state[\"trial_documents\"]:\n",
    "            # Prioritize Results and statistical sections\n",
    "            if doc.metadata.get(\"section\") in [\"Results\", \"Adverse_Events\"] or not state[\"focus_section\"]:\n",
    "                trial_evidence += f\"\\n{doc.metadata.get('section', 'Unknown')}:\\n{doc.page_content}\\n\"\n",
    "        \n",
    "        # Run statistical analysis\n",
    "        statistical_chain = statistical_prompt | llm | StrOutputParser()\n",
    "        statistical_result = statistical_chain.invoke({\n",
    "            \"statement\": state[\"statement\"],\n",
    "            \"trial_evidence\": trial_evidence\n",
    "        })\n",
    "        \n",
    "        state[\"statistical_analysis\"] = statistical_result\n",
    "        state[\"next_action\"] = \"logical_analysis\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        state[\"error_messages\"].append(f\"Statistical analysis error: {str(e)}\")\n",
    "        state[\"next_action\"] = \"logical_analysis\"  # Continue with next analysis\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"âœ… Statistical analysis node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical_analysis_node",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logical_analysis_node(state: ClinicalAnalysisState) -> ClinicalAnalysisState:\n",
    "    \"\"\"Perform logical consistency analysis.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Create logical analysis prompt\n",
    "        logical_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=\"\"\"\n",
    "You are a Logic Analyst specializing in reasoning validation and consistency checking.\n",
    "Your role is to validate logical relationships, consistency, and reasoning soundness.\n",
    "\n",
    "Focus on:\n",
    "- Logical structure and coherence\n",
    "- Cause-and-effect relationships\n",
    "- Internal consistency\n",
    "- Validity of inferences\n",
    "- Detection of logical fallacies\n",
    "- Reasoning pattern analysis\n",
    "\n",
    "Provide logical analysis and end with:\n",
    "LOGICAL_VERDICT: [SOUND/UNSOUND/QUESTIONABLE] - logical reasoning\n",
    "            \"\"\".strip()),\n",
    "            HumanMessage(content=\"\"\"\n",
    "STATEMENT TO ANALYZE: \"{statement}\"\n",
    "\n",
    "EVIDENCE CONTEXT:\n",
    "{trial_evidence}\n",
    "\n",
    "Please analyze the logical consistency and reasoning of this statement.\n",
    "            \"\"\".strip())\n",
    "        ])\n",
    "        \n",
    "        # Prepare trial evidence\n",
    "        trial_evidence = \"\"\n",
    "        for doc in state[\"trial_documents\"]:\n",
    "            if state[\"focus_section\"] and doc.metadata.get(\"section\") != state[\"focus_section\"]:\n",
    "                continue\n",
    "            trial_evidence += f\"\\n{doc.metadata.get('section', 'Unknown')}:\\n{doc.page_content[:500]}...\\n\"\n",
    "        \n",
    "        # Run logical analysis\n",
    "        logical_chain = logical_prompt | llm | StrOutputParser()\n",
    "        logical_result = logical_chain.invoke({\n",
    "            \"statement\": state[\"statement\"],\n",
    "            \"trial_evidence\": trial_evidence\n",
    "        })\n",
    "        \n",
    "        state[\"logical_analysis\"] = logical_result\n",
    "        state[\"next_action\"] = \"decision_synthesis\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        state[\"error_messages\"].append(f\"Logical analysis error: {str(e)}\")\n",
    "        state[\"next_action\"] = \"decision_synthesis\"  # Continue to final decision\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"âœ… Logical analysis node defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decision_synthesis_node",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_synthesis_node(state: ClinicalAnalysisState) -> ClinicalAnalysisState:\n",
    "    \"\"\"Synthesize all analyses and make final decision.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Create decision synthesis prompt\n",
    "        decision_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessage(content=\"\"\"\n",
    "You are the Decision Synthesizer responsible for making final entailment classifications.\n",
    "Your role is to synthesize expert analyses and determine the final verdict.\n",
    "\n",
    "Classification Rules:\n",
    "- ENTAILMENT: Statement is directly supported by the trial evidence\n",
    "- CONTRADICTION: Statement is refuted or contradicted by the trial evidence\n",
    "\n",
    "Weigh all evidence types:\n",
    "- Medical expert analysis (clinical accuracy)\n",
    "- Statistical analysis (numerical validity)\n",
    "- Logical analysis (reasoning soundness)\n",
    "\n",
    "Provide reasoning and confidence, then end with:\n",
    "FINAL_DECISION: [Entailment/Contradiction]\n",
    "CONFIDENCE: [0.0-1.0]\n",
    "            \"\"\".strip()),\n",
    "            HumanMessage(content=\"\"\"\n",
    "ORIGINAL STATEMENT: \"{statement}\"\n",
    "\n",
    "MEDICAL ANALYSIS:\n",
    "{medical_analysis}\n",
    "\n",
    "STATISTICAL ANALYSIS:\n",
    "{statistical_analysis}\n",
    "\n",
    "LOGICAL ANALYSIS:\n",
    "{logical_analysis}\n",
    "\n",
    "Based on these expert analyses, provide your final entailment decision.\n",
    "            \"\"\".strip())\n",
    "        ])\n",
    "        \n",
    "        # Run decision synthesis\n",
    "        decision_chain = decision_prompt | llm | StrOutputParser()\n",
    "        decision_result = decision_chain.invoke({\n",
    "            \"statement\": state[\"statement\"],\n",
    "            \"medical_analysis\": state.get(\"medical_analysis\", \"Not available\"),\n",
    "            \"statistical_analysis\": state.get(\"statistical_analysis\", \"Not available\"),\n",
    "            \"logical_analysis\": state.get(\"logical_analysis\", \"Not available\")\n",
    "        })\n",
    "        \n",
    "        # Parse decision and confidence\n",
    "        if \"FINAL_DECISION: Entailment\" in decision_result:\n",
    "            final_decision = \"Entailment\"\n",
    "        elif \"FINAL_DECISION: Contradiction\" in decision_result:\n",
    "            final_decision = \"Contradiction\"\n",
    "        else:\n",
    "            # Fallback parsing\n",
    "            if \"entailment\" in decision_result.lower() and \"contradiction\" not in decision_result.lower():\n",
    "                final_decision = \"Entailment\"\n",
    "            else:\n",
    "                final_decision = \"Contradiction\"\n",
    "        \n",
    "        # Extract confidence score\n",
    "        confidence = 0.5  # Default\n",
    "        try:\n",
    "            if \"CONFIDENCE:\" in decision_result:\n",
    "                confidence_str = decision_result.split(\"CONFIDENCE:\")[1].strip().split()[0]\n",
    "                confidence = float(confidence_str)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        state[\"final_decision\"] = final_decision\n",
    "        state[\"confidence_score\"] = confidence\n",
    "        state[\"next_action\"] = \"end\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        state[\"error_messages\"].append(f\"Decision synthesis error: {str(e)}\")\n",
    "        state[\"final_decision\"] = \"Contradiction\"  # Conservative fallback\n",
    "        state[\"confidence_score\"] = 0.1\n",
    "        state[\"next_action\"] = \"end\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"âœ… Decision synthesis node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "workflow_definition",
   "metadata": {},
   "source": "## å·¥ä½œæµç¨‹å®šç¾©\n\né€éé€£æ¥æ‰€æœ‰ç¯€é»ä¾†å‰µå»ºLangGraphå·¥ä½œæµç¨‹ï¼š\n\n> ğŸ”— **åœ–çµæ§‹èªªæ˜**: LangGraphå·¥ä½œæµç¨‹å°±åƒä¸€å€‹æµç¨‹åœ–ï¼Œæ¯å€‹ç¯€é»ä»£è¡¨ä¸€å€‹åˆ†ææ­¥é©Ÿï¼Œé‚Šç·£å®šç¾©è³‡æ–™å¦‚ä½•åœ¨ç¯€é»é–“æµå‹•ã€‚é€™ç¢ºä¿äº†å¯è¿½è¹¤å’Œå¯é‡ç¾çš„åˆ†æéç¨‹ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "workflow_creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clinical_analysis_workflow():\n",
    "    \"\"\"Create the clinical analysis workflow using LangGraph.\"\"\"\n",
    "    \n",
    "    # Create the StateGraph\n",
    "    workflow = StateGraph(ClinicalAnalysisState)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"data_extraction\", clinical_data_extractor)\n",
    "    workflow.add_node(\"medical_analysis\", medical_analysis_node)\n",
    "    workflow.add_node(\"statistical_analysis\", statistical_analysis_node)\n",
    "    workflow.add_node(\"logical_analysis\", logical_analysis_node)\n",
    "    workflow.add_node(\"decision_synthesis\", decision_synthesis_node)\n",
    "    \n",
    "    # Add edges\n",
    "    workflow.set_entry_point(\"data_extraction\")\n",
    "    workflow.add_edge(\"data_extraction\", \"medical_analysis\")\n",
    "    workflow.add_edge(\"medical_analysis\", \"statistical_analysis\")\n",
    "    workflow.add_edge(\"statistical_analysis\", \"logical_analysis\")\n",
    "    workflow.add_edge(\"logical_analysis\", \"decision_synthesis\")\n",
    "    workflow.add_edge(\"decision_synthesis\", END)\n",
    "    \n",
    "    # Compile the workflow\n",
    "    app = workflow.compile(checkpointer=checkpointer)\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Create the workflow\n",
    "clinical_workflow = create_clinical_analysis_workflow()\n",
    "\n",
    "print(\"âœ… LangGraph workflow created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": [
    "## Analysis Pipeline\n",
    "\n",
    "Create the main pipeline function that uses our LangGraph workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def langchain_analysis_pipeline(statement: str, primary_id: str, secondary_id: Optional[str] = None, \n",
    "                               section_id: Optional[str] = None, verbose: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Run the complete LangChain/LangGraph analysis pipeline.\n",
    "    \n",
    "    Args:\n",
    "        statement: The natural language statement to analyze\n",
    "        primary_id: Primary clinical trial ID\n",
    "        secondary_id: Secondary trial ID for comparison statements\n",
    "        section_id: Relevant section of the trial\n",
    "        verbose: Whether to print intermediate results\n",
    "        \n",
    "    Returns:\n",
    "        Final decision: 'Entailment' or 'Contradiction'\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        if verbose:\n",
    "            print(f\"ğŸ“„ Analyzing: {statement[:100]}...\")\n",
    "            print(f\"ğŸ¥ Primary Trial: {primary_id}\")\n",
    "            if secondary_id:\n",
    "                print(f\"ğŸ¥ Secondary Trial: {secondary_id}\")\n",
    "        \n",
    "        # Create initial state\n",
    "        initial_state = {\n",
    "            \"statement\": statement,\n",
    "            \"primary_trial_id\": primary_id,\n",
    "            \"secondary_trial_id\": secondary_id,\n",
    "            \"focus_section\": section_id,\n",
    "            \"primary_trial_data\": {},\n",
    "            \"secondary_trial_data\": None,\n",
    "            \"trial_documents\": [],\n",
    "            \"medical_analysis\": None,\n",
    "            \"statistical_analysis\": None,\n",
    "            \"logical_analysis\": None,\n",
    "            \"final_decision\": None,\n",
    "            \"confidence_score\": None,\n",
    "            \"next_action\": None,\n",
    "            \"error_messages\": []\n",
    "        }\n",
    "        \n",
    "        # Run the workflow\n",
    "        config = {\"configurable\": {\"thread_id\": f\"analysis_{hash(statement)}\"[:10]}}\n",
    "        result = clinical_workflow.invoke(initial_state, config)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"ğŸ©º Medical Analysis: {'âœ…' if result.get('medical_analysis') else 'âŒ'}\")\n",
    "            print(f\"ğŸ“Š Statistical Analysis: {'âœ…' if result.get('statistical_analysis') else 'âŒ'}\")\n",
    "            print(f\"ğŸ§  Logical Analysis: {'âœ…' if result.get('logical_analysis') else 'âŒ'}\")\n",
    "            print(f\"âš–ï¸ Final Decision: {result.get('final_decision', 'Unknown')}\")\n",
    "            print(f\"ğŸ¯ Confidence: {result.get('confidence_score', 0.0):.2f}\")\n",
    "            \n",
    "            if result.get(\"error_messages\"):\n",
    "                print(f\"âš ï¸ Errors: {len(result['error_messages'])}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        return result.get(\"final_decision\", \"Contradiction\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"âŒ Error in LangChain pipeline: {e}\")\n",
    "        return \"Contradiction\"  # Conservative fallback\n",
    "\n",
    "print(\"âœ… LangChain analysis pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_example",
   "metadata": {},
   "source": [
    "## Test Example\n",
    "\n",
    "Let's test our LangChain/LangGraph system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample statement\n",
    "test_statement = \"there is a 13.2% difference between the results from the two the primary trial cohorts\"\n",
    "test_primary_id = \"NCT00066573\"\n",
    "\n",
    "print(f\"Testing LangChain/LangGraph system with statement:\")\n",
    "print(f\"'{test_statement}'\")\n",
    "print(f\"Primary trial: {test_primary_id}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Run the analysis with verbose output\n",
    "result = langchain_analysis_pipeline(\n",
    "    statement=test_statement,\n",
    "    primary_id=test_primary_id,\n",
    "    section_id=\"Results\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ¯ LANGCHAIN RESULT: {result}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation",
   "metadata": {},
   "source": [
    "## Evaluation on Training Data\n",
    "\n",
    "Let's evaluate our LangChain/LangGraph system on training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_data = load_dataset(\"training_data/train.json\")\n",
    "print(f\"Loaded {len(train_data)} training examples\")\n",
    "\n",
    "# Evaluate on a sample (adjust sample_size as needed)\n",
    "sample_size = 20\n",
    "examples = list(train_data.items())[:sample_size]\n",
    "\n",
    "print(f\"\\nEvaluating LangChain/LangGraph system on {len(examples)} examples...\")\n",
    "\n",
    "results = []\n",
    "correct = 0\n",
    "\n",
    "for i, (uuid, example) in enumerate(tqdm(examples, desc=\"LangChain Processing\")):\n",
    "    try:\n",
    "        statement = example.get(\"Statement\")\n",
    "        primary_id = example.get(\"Primary_id\")\n",
    "        secondary_id = example.get(\"Secondary_id\")\n",
    "        section_id = example.get(\"Section_id\")\n",
    "        expected = example.get(\"Label\")\n",
    "        \n",
    "        if not statement or not primary_id:\n",
    "            results.append({\n",
    "                \"uuid\": uuid,\n",
    "                \"expected\": expected,\n",
    "                \"predicted\": \"SKIPPED\",\n",
    "                \"correct\": False\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Get prediction from LangChain/LangGraph system\n",
    "        predicted = langchain_analysis_pipeline(\n",
    "            statement=statement,\n",
    "            primary_id=primary_id,\n",
    "            secondary_id=secondary_id,\n",
    "            section_id=section_id,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Check if correct\n",
    "        is_correct = (predicted.strip() == expected.strip())\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            \n",
    "        results.append({\n",
    "            \"uuid\": uuid,\n",
    "            \"statement\": statement[:100] + \"...\" if len(statement) > 100 else statement,\n",
    "            \"expected\": expected,\n",
    "            \"predicted\": predicted,\n",
    "            \"correct\": is_correct\n",
    "        })\n",
    "        \n",
    "        status = \"âœ…\" if is_correct else \"âŒ\"\n",
    "        print(f\"Example {i+1:2d}: {expected:12} -> {predicted:12} {status}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example {i+1}: {e}\")\n",
    "        results.append({\n",
    "            \"uuid\": uuid,\n",
    "            \"expected\": expected,\n",
    "            \"predicted\": \"ERROR\",\n",
    "            \"correct\": False\n",
    "        })\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / len(examples) if examples else 0\n",
    "print(f\"\\nğŸ“Š LangChain/LangGraph Results:\")\n",
    "print(f\"Accuracy: {accuracy:.2%} ({correct}/{len(examples)})\")\n",
    "\n",
    "# Store results for comparison\n",
    "langchain_results = results.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "state_inspection",
   "metadata": {},
   "source": [
    "## State Inspection\n",
    "\n",
    "Let's inspect the stateful capabilities of our LangGraph workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inspect_state",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate state persistence and inspection\n",
    "print(\"ğŸ” LangGraph State Inspection:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run a simple analysis and inspect intermediate states\n",
    "test_config = {\"configurable\": {\"thread_id\": \"demo_analysis\"}}\n",
    "test_state = {\n",
    "    \"statement\": \"The primary endpoint was met\",\n",
    "    \"primary_trial_id\": \"NCT00066573\",\n",
    "    \"secondary_trial_id\": None,\n",
    "    \"focus_section\": \"Results\",\n",
    "    \"primary_trial_data\": {},\n",
    "    \"secondary_trial_data\": None,\n",
    "    \"trial_documents\": [],\n",
    "    \"medical_analysis\": None,\n",
    "    \"statistical_analysis\": None,\n",
    "    \"logical_analysis\": None,\n",
    "    \"final_decision\": None,\n",
    "    \"confidence_score\": None,\n",
    "    \"next_action\": None,\n",
    "    \"error_messages\": []\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Stream the workflow execution to see intermediate states\n",
    "    print(\"Streaming workflow execution:\")\n",
    "    for step in clinical_workflow.stream(test_state, test_config):\n",
    "        for node_name, node_output in step.items():\n",
    "            print(f\"\\nğŸ“ Node: {node_name}\")\n",
    "            if node_output.get(\"final_decision\"):\n",
    "                print(f\"   Decision: {node_output['final_decision']}\")\n",
    "                print(f\"   Confidence: {node_output.get('confidence_score', 'N/A')}\")\n",
    "            if node_output.get(\"error_messages\"):\n",
    "                print(f\"   Errors: {len(node_output['error_messages'])}\")\n",
    "            print(f\"   Next: {node_output.get('next_action', 'N/A')}\")\n",
    "    \n",
    "    # Get final state\n",
    "    final_state = clinical_workflow.get_state(test_config)\n",
    "    print(f\"\\nğŸ¯ Final State Summary:\")\n",
    "    print(f\"   Thread ID: {test_config['configurable']['thread_id']}\")\n",
    "    print(f\"   Final Decision: {final_state.values.get('final_decision')}\")\n",
    "    print(f\"   Total Errors: {len(final_state.values.get('error_messages', []))}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in state inspection: {e}\")\n",
    "\n",
    "print(\"\\nâœ… State inspection completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "submission",
   "metadata": {},
   "source": [
    "## Generate Submission File\n",
    "\n",
    "Let's generate a submission file using our LangChain/LangGraph system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_langchain_submission(test_file=\"test.json\", output_file=\"langchain_submission.json\", sample_size=None):\n",
    "    \"\"\"\n",
    "    Generate submission file using LangChain/LangGraph system.\n",
    "    \n",
    "    Args:\n",
    "        test_file: Path to test data\n",
    "        output_file: Output submission file\n",
    "        sample_size: Number of examples to process (None for all)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = load_dataset(test_file)\n",
    "    if not test_data:\n",
    "        print(f\"âŒ Could not load test data from {test_file}\")\n",
    "        return\n",
    "    \n",
    "    examples = list(test_data.items())\n",
    "    if sample_size:\n",
    "        examples = examples[:sample_size]\n",
    "        \n",
    "    print(f\"ğŸš€ Generating LangChain/LangGraph predictions for {len(examples)} examples...\")\n",
    "    \n",
    "    submission = {}\n",
    "    \n",
    "    for i, (uuid, example) in enumerate(tqdm(examples, desc=\"LangChain Processing\")):\n",
    "        try:\n",
    "            statement = example.get(\"Statement\")\n",
    "            primary_id = example.get(\"Primary_id\")\n",
    "            secondary_id = example.get(\"Secondary_id\")\n",
    "            section_id = example.get(\"Section_id\")\n",
    "            \n",
    "            if not statement or not primary_id:\n",
    "                submission[uuid] = {\"Prediction\": \"Contradiction\"}  # Default fallback\n",
    "                continue\n",
    "                \n",
    "            # Get prediction from LangChain/LangGraph system\n",
    "            prediction = langchain_analysis_pipeline(\n",
    "                statement=statement,\n",
    "                primary_id=primary_id,\n",
    "                secondary_id=secondary_id,\n",
    "                section_id=section_id,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            submission[uuid] = {\"Prediction\": prediction}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {uuid}: {e}\")\n",
    "            submission[uuid] = {\"Prediction\": \"Contradiction\"}  # Conservative fallback\n",
    "    \n",
    "    # Save submission file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(submission, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… LangChain submission saved to {output_file}\")\n",
    "    return submission\n",
    "\n",
    "# Generate submission for a small sample\n",
    "langchain_submission = generate_langchain_submission(\n",
    "    test_file=\"test.json\", \n",
    "    output_file=\"langchain_submission.json\",\n",
    "    sample_size=10  # Adjust as needed\n",
    ")\n",
    "\n",
    "print(f\"Generated predictions for {len(langchain_submission)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "workflow_visualization",
   "metadata": {},
   "source": [
    "## Workflow Visualization\n",
    "\n",
    "Let's visualize our LangGraph workflow structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize_workflow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display workflow information\n",
    "print(\"ğŸ”„ LangGraph Workflow Structure:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "workflow_steps = [\n",
    "    \"1. Data Extraction â†’ Load and structure clinical trial data\",\n",
    "    \"2. Medical Analysis â†’ Expert medical reasoning and assessment\",\n",
    "    \"3. Statistical Analysis â†’ Numerical validation and calculations\",\n",
    "    \"4. Logical Analysis â†’ Reasoning consistency and soundness\",\n",
    "    \"5. Decision Synthesis â†’ Final entailment classification\"\n",
    "]\n",
    "\n",
    "for step in workflow_steps:\n",
    "    print(f\"   {step}\")\n",
    "\n",
    "print(\"\\nğŸ“Š State Management Features:\")\n",
    "state_features = [\n",
    "    \"â€¢ Persistent state across workflow steps\",\n",
    "    \"â€¢ Error tracking and recovery mechanisms\",\n",
    "    \"â€¢ Confidence scoring and decision rationale\",\n",
    "    \"â€¢ Intermediate result storage and inspection\",\n",
    "    \"â€¢ Thread-based conversation management\"\n",
    "]\n",
    "\n",
    "for feature in state_features:\n",
    "    print(f\"   {feature}\")\n",
    "\n",
    "print(\"\\nâœ… Workflow visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": "## çµè«–èˆ‡æ´å¯Ÿ\n\n### LangChain/LangGraph æ¡†æ¶å„ªå‹¢ï¼š\n1. **æˆç†Ÿç”Ÿæ…‹ç³»çµ±**: æœ€å®Œå–„ä¸”åŠŸèƒ½è±å¯Œçš„LLMæ‡‰ç”¨æ¡†æ¶\n2. **æœ‰ç‹€æ…‹å·¥ä½œæµç¨‹**: LangGraphæ”¯æ´è¤‡é›œçš„æœ‰ç‹€æ…‹ä»£ç†äº’å‹•\n3. **è±å¯Œæ•´åˆ**: å»£æ³›çš„å·¥å…·ç”Ÿæ…‹ç³»çµ±å’Œæœå‹™æ•´åˆ\n4. **é€²éšæ¨¡å¼**: æ”¯æ´ç²¾å¯†çš„æ¨ç†å’Œæ±ºç­–æ¨¡å¼\n5. **ç¤¾ç¾¤æ”¯æ´**: é¾å¤§ç¤¾ç¾¤ã€è±å¯Œæ–‡æª”å’Œå¯¦éš›ç¯„ä¾‹\n6. **ç”Ÿç”¢å°±ç·’**: åœ¨çœ¾å¤šä¼æ¥­æ‡‰ç”¨ä¸­ç¶“éå¯¦æˆ°è€ƒé©—\n\n### é—œéµåŠŸèƒ½å±•ç¤ºï¼š\n- **åŸºæ–¼åœ–çš„å·¥ä½œæµç¨‹**: çµæ§‹åŒ–ã€æœ‰ç‹€æ…‹çš„åˆ†æç®¡é“\n- **ç‹€æ…‹æŒä¹…æ€§**: SQLiteæª¢æŸ¥é»ç”¨æ–¼å°è©±æŒçºŒæ€§\n- **ç¯€é»å°ˆæ¥­åŒ–**: é‡å°ä¸åŒé ˜åŸŸçš„å°ˆé–€åˆ†æç¯€é»\n- **éŒ¯èª¤è™•ç†**: å¼·å¥çš„éŒ¯èª¤è¿½è¹¤å’Œæ¢å¾©æ©Ÿåˆ¶\n- **ä¸²æµæ”¯æ´**: å³æ™‚å·¥ä½œæµç¨‹åŸ·è¡Œç›£æ§\n- **é…ç½®ç®¡ç†**: åŸºæ–¼ç·šç¨‹çš„ç‹€æ…‹ç®¡ç†\n\n### æ¶æ§‹å„ªå‹¢ï¼š\n- **å½ˆæ€§å·¥ä½œæµç¨‹**: æ˜“æ–¼ä¿®æ”¹å’Œæ“´å±•åˆ†æç®¡é“\n- **ç‹€æ…‹ç®¡ç†**: è·¨è¤‡é›œå¤šæ­¥é©Ÿæµç¨‹çš„æŒä¹…ä¸Šä¸‹æ–‡\n- **é™¤éŒ¯æ”¯æ´**: æ¸…æ™°çš„ç‹€æ…‹æª¢æŸ¥å’Œä¸­é–“çµæœè¿½è¹¤\n- **å¯æ“´å±•è¨­è¨ˆ**: ä¼æ¥­éƒ¨ç½²çš„ç”Ÿç”¢å°±ç·’æ¶æ§‹\n- **æ•´åˆå°±ç·’**: èˆ‡å¤–éƒ¨å·¥å…·å’Œæœå‹™çš„ç„¡ç¸«æ•´åˆ\n\n### å„ªåŒ–æ©Ÿæœƒï¼š\n1. **æç¤ºå·¥ç¨‹**: å¾®èª¿æ¯å€‹åˆ†æç¯€é»çš„æç¤º\n2. **æ¢ä»¶è·¯ç”±**: ç‚ºä¸åŒé™³è¿°é¡å‹æ·»åŠ æ¢ä»¶é‚è¼¯\n3. **å¹³è¡Œè™•ç†**: åœ¨å¯èƒ½çš„åœ°æ–¹å¯¦ä½œå¹³è¡Œåˆ†æç¯€é»\n4. **å·¥å…·æ•´åˆ**: åˆ©ç”¨LangChainå»£æ³›çš„å·¥å…·ç”Ÿæ…‹ç³»çµ±\n5. **é€²éšæ¨¡å¼**: å¯¦ä½œè‡ªæˆ‘åæ€å’Œè¿­ä»£æ”¹é€²\n\n### ä½•æ™‚ä½¿ç”¨ LangChain/LangGraphï¼š\n- éœ€è¦ç‹€æ…‹ç®¡ç†çš„è¤‡é›œå¤šæ­¥é©Ÿæ¨ç†å·¥ä½œæµç¨‹\n- éœ€è¦å»£æ³›å·¥å…·å’Œæœå‹™æ•´åˆçš„æ‡‰ç”¨\n- éœ€è¦å¼·å¥ç”Ÿç”¢å°±ç·’æ¡†æ¶çš„ä¼æ¥­ç³»çµ±\n- ç¤¾ç¾¤æ”¯æ´å’Œæ–‡æª”è‡³é—œé‡è¦çš„å°ˆæ¡ˆ\n- éœ€è¦ç²¾å¯†å·¥ä½œæµç¨‹æ¨¡å¼å’Œå®¢è£½åŒ–çš„å ´æ™¯\n\n### æ¡†æ¶æ¯”è¼ƒç¸½çµï¼š\n- **vs AutoGen**: æ›´é©åˆçµæ§‹åŒ–å·¥ä½œæµç¨‹ï¼ŒAutoGenæ›´é©åˆè‡ªç”±å½¢å¼å°è©±\n- **vs Atomic Agents**: æ›´å…¨é¢ä½†è¼ƒé‡ï¼ŒAtomicæ›´é©åˆç´”æ•ˆèƒ½\n- **vs Agno**: æ›´å»£æ³›çš„ç”Ÿæ…‹ç³»çµ±ï¼ŒAgnoåœ¨å…§å»ºè¨˜æ†¶é«”å’ŒçŸ¥è­˜æ–¹é¢æ›´ä½³\n\n### LangChain/LangGraph ç¨ç‰¹å„ªå‹¢ï¼š\n1. **åŸºæ–¼åœ–çš„æ¨ç†**: åŸç”Ÿæ”¯æ´è¤‡é›œçš„æ¢ä»¶å·¥ä½œæµç¨‹\n2. **ç‹€æ…‹æŒä¹…æ€§**: å…§å»ºæª¢æŸ¥é»å’Œå°è©±æŒçºŒæ€§\n3. **ç”Ÿæ…‹ç³»çµ±æˆç†Ÿåº¦**: å»£æ³›çš„å·¥å…·ã€æ•´åˆå’Œç¤¾ç¾¤æ”¯æ´\n4. **ä¼æ¥­åŠŸèƒ½**: å…·å‚™ç›£æ§ã€è¨˜éŒ„å’Œé™¤éŒ¯çš„ç”Ÿç”¢å°±ç·’\n5. **å½ˆæ€§**: é«˜åº¦å¯å®¢è£½åŒ–çš„å·¥ä½œæµç¨‹å’Œæ•´åˆæ¨¡å¼\n\n## ğŸ“ å­¸ç¿’é‡é»ç¸½çµ\n- **ç‹€æ…‹åœ–æ¦‚å¿µ**: å·¥ä½œæµç¨‹ä½œç‚ºæœ‰å‘åœ–ï¼Œç¯€é»æ˜¯åŠŸèƒ½ï¼Œé‚Šæ˜¯è³‡æ–™æµ\n- **æŒä¹…ç‹€æ…‹**: åœ¨æ•´å€‹åˆ†æéç¨‹ä¸­ç¶­è­·ä¸Šä¸‹æ–‡å’Œä¸­é–“çµæœ\n- **æ¨¡çµ„åŒ–è¨­è¨ˆ**: æ¯å€‹ç¯€é»å°ˆæ³¨æ–¼ç‰¹å®šåˆ†æä»»å‹™\n- **ä¼æ¥­å°±ç·’**: ç”Ÿç”¢ç’°å¢ƒçš„ç›£æ§ã€éŒ¯èª¤è™•ç†å’Œå¯æ“´å±•æ€§\n\nLangChain/LangGraphåœ¨è¤‡é›œçš„ç”Ÿç”¢ç’°å¢ƒä¸­è¡¨ç¾å“è¶Šï¼Œç‰¹åˆ¥é©åˆéœ€è¦çµæ§‹åŒ–å·¥ä½œæµç¨‹ã€ç‹€æ…‹ç®¡ç†å’Œå»£æ³›æ•´åˆçš„è‡¨åºŠNLPæ‡‰ç”¨ã€‚å®ƒæ˜¯éœ€è¦ç²¾å¯†æ¨ç†æ¨¡å¼å’Œå¼·å¥åŸºç¤è¨­æ–½æ”¯æ´çš„ä¼æ¥­ç´šæ‡‰ç”¨çš„ç†æƒ³é¸æ“‡ã€‚"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}