{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/0x-yuan/clintrial-nlp/blob/main/langchain_baseline.ipynb)\n",
    "\n",
    "# ç°¡åŒ–æµç¨‹æ¨ç†æ¡†æ¶åŸºç·š - è‡¨åºŠè©¦é©— NLP\n",
    "\n",
    "## æ¦‚è¿°\n",
    "\n",
    "æœ¬notebookå±•ç¤ºå¦‚ä½•ä½¿ç”¨ç°¡å–®çš„æ­¥é©Ÿå¼æ¨ç†æµç¨‹é€²è¡Œè‡¨åºŠè©¦é©—è‡ªç„¶èªè¨€æ¨ç†(NLI)ã€‚æˆ‘å€‘æ¨¡æ“¬\"æµç¨‹åŒ–æ€è€ƒ\"æ¦‚å¿µï¼Œé€šéçµæ§‹åŒ–çš„åˆ†ææ­¥é©Ÿé€æ­¥å¾—å‡ºçµè«–ã€‚\n",
    "\n",
    "## ğŸ“š å­¸ç¿’ç›®æ¨™\n",
    "- ç†è§£æ­¥é©Ÿå¼æ¨ç†çš„é‡è¦æ€§\n",
    "- å»ºç«‹çµæ§‹åŒ–çš„åˆ†ææµç¨‹\n",
    "- å¯¦ä½œå¾ªåºæ¼¸é€²çš„æ±ºç­–éç¨‹\n",
    "- è©•ä¼°ç³»çµ±æ•ˆèƒ½\n",
    "\n",
    "### ğŸ”„ æµç¨‹æ¨ç†æ¶æ§‹\n",
    "æˆ‘å€‘å¯¦ä½œ5å€‹é€£çºŒçš„åˆ†ææ­¥é©Ÿï¼š\n",
    "1. **è³‡æ–™æå–æ­¥é©Ÿ**: å¾è©¦é©—ä¸­æå–ç›¸é—œè³‡è¨Š\n",
    "2. **é†«å­¸åˆ†ææ­¥é©Ÿ**: å¾é†«å­¸è§’åº¦åˆ†æé™³è¿°\n",
    "3. **çµ±è¨ˆåˆ†ææ­¥é©Ÿ**: é©—è­‰æ•¸æ“šå’Œè¨ˆç®—\n",
    "4. **é‚è¼¯é©—è­‰æ­¥é©Ÿ**: æª¢æŸ¥æ¨ç†é‚è¼¯\n",
    "5. **æ±ºç­–ç¶œåˆæ­¥é©Ÿ**: æ•´åˆæ‰€æœ‰è³‡è¨Šåšå‡ºæœ€çµ‚æ±ºç­–\n",
    "\n",
    "> ğŸ’¡ **æ ¸å¿ƒæ¦‚å¿µ**: æ¯å€‹æ­¥é©Ÿéƒ½å»ºç«‹åœ¨å‰ä¸€æ­¥é©Ÿçš„çµæœä¹‹ä¸Šï¼Œå½¢æˆå®Œæ•´çš„æ¨ç†éˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ ç’°å¢ƒè¨­ç½® - ä¸€éµå®‰è£æ‰€éœ€å¥—ä»¶\n",
    "!pip install -q google-generativeai python-dotenv pandas tqdm gdown\n",
    "\n",
    "print(\"âœ… æ‰€æœ‰å¥—ä»¶å®‰è£å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¥ å¾ Google Drive ä¸‹è¼‰è¨“ç·´è³‡æ–™\n",
    "import os\n",
    "import gdown\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Google Drive zip æª”æ¡ˆ ID\n",
    "file_id = \"15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR\"\n",
    "zip_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "zip_filename = \"clinicaltrial-nlp.zip\"\n",
    "\n",
    "if not os.path.exists(\"training_data\"):\n",
    "    print(\"ğŸ“¥ å¾ Google Drive ä¸‹è¼‰ clinicaltrial-nlp.zip...\")\n",
    "    try:\n",
    "        gdown.download(zip_url, zip_filename, quiet=False)\n",
    "        \n",
    "        print(\"ğŸ“¦ æ­£åœ¨è§£å£“ç¸®æª”æ¡ˆ...\")\n",
    "        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\".\")\n",
    "        \n",
    "        if os.path.exists(\"clintrial-nlp/training_data\"):\n",
    "            shutil.move(\"clintrial-nlp/training_data\", \"training_data\")\n",
    "            if os.path.exists(\"clintrial-nlp\"):\n",
    "                shutil.rmtree(\"clintrial-nlp\")\n",
    "        \n",
    "        os.remove(zip_filename)\n",
    "        print(\"âœ… è¨“ç·´è³‡æ–™ä¸‹è¼‰ä¸¦è§£å£“ç¸®å®Œæˆï¼\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¸‹è¼‰å¤±æ•—: {e}\")\n",
    "        print(\"è«‹æ‰‹å‹•ä¸‹è¼‰: https://drive.google.com/file/d/15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR/view\")\n",
    "else:\n",
    "    print(\"âœ… è¨“ç·´è³‡æ–™å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰\")\n",
    "\n",
    "# æª¢æŸ¥ä¸‹è¼‰çš„è³‡æ–™\n",
    "if os.path.exists(\"training_data/CT json\"):\n",
    "    ct_files = len([f for f in os.listdir(\"training_data/CT json\") if f.endswith('.json')])\n",
    "    print(f\"ğŸ“„ æ‰¾åˆ° {ct_files} å€‹è‡¨åºŠè©¦é©—JSONæª”æ¡ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª æº–å‚™æ¸¬è©¦è³‡æ–™é›†\n",
    "import json\n",
    "\n",
    "def create_test_data_if_needed():\n",
    "    if not os.path.exists(\"test.json\"):\n",
    "        try:\n",
    "            with open(\"training_data/train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                train_data = json.load(f)\n",
    "            test_data = dict(list(train_data.items())[:100])\n",
    "            with open(\"test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(test_data, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"âœ… å·²å‰µå»ºæ¸¬è©¦è³‡æ–™é›†ï¼ŒåŒ…å« {len(test_data)} å€‹æ¨£æœ¬\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å‰µå»ºæ¸¬è©¦è³‡æ–™å¤±æ•—: {e}\")\n",
    "    else:\n",
    "        print(\"âœ… test.json å·²å­˜åœ¨\")\n",
    "\n",
    "create_test_data_if_needed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ç’°å¢ƒè®Šæ•¸å’Œå¿…è¦å‡½å¼åº«\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv()\n",
    "print(\"âœ… ç’°å¢ƒè®Šæ•¸è¼‰å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨¡å‹é…ç½®\n",
    "\n",
    "é…ç½®Google Geminiæ¨¡å‹é€²è¡Œæ¨ç†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\") or userdata.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"âš ï¸ è«‹è¨­å®š GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸\")\n",
    "    print(\"å¯ä»¥åœ¨ Colab å·¦å´é¢æ¿çš„ 'Secrets' ä¸­è¨­å®š\")\n",
    "    raise ValueError(\"ç¼ºå°‘ API é‡‘é‘°\")\n",
    "else:\n",
    "    print(f\"âœ… æ‰¾åˆ° API é‡‘é‘°: {api_key[:8]}...{api_key[-4:]}\")\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# æ¸¬è©¦ API é€£æ¥\n",
    "try:\n",
    "    test_model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "    test_response = test_model.generate_content(\"Hello, respond with 'API test successful'\")\n",
    "    print(f\"âœ… API é€£æ¥æ¸¬è©¦æˆåŠŸ: {test_response.text[:50]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ API é€£æ¥æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "    raise\n",
    "\n",
    "# å‰µå»º Gemini æ¨¡å‹å¯¦ä¾‹\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-2.5-flash\",\n",
    "    generation_config=genai.types.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        max_output_tokens=4096,\n",
    "        top_p=1,\n",
    "        top_k=1\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"âœ… Google Geminiæ¨¡å‹é…ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è³‡æ–™å·¥å…·å‡½å¼\n",
    "\n",
    "å»ºç«‹ç”¨æ–¼è¼‰å…¥å’Œè™•ç†è‡¨åºŠè©¦é©—è³‡æ–™çš„å·¥å…·å‡½å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clinical_trial(trial_id: str) -> dict:\n",
    "    \"\"\"è¼‰å…¥è‡¨åºŠè©¦é©—è³‡æ–™\"\"\"\n",
    "    try:\n",
    "        file_path = os.path.join(\"training_data\", \"CT json\", f\"{trial_id}.json\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        return {\"error\": f\"æ‰¾ä¸åˆ°è‡¨åºŠè©¦é©— {trial_id}\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"è¼‰å…¥ {trial_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\"}\n",
    "\n",
    "def format_trial_context(trial_data: dict, focus_section: str = None) -> str:\n",
    "    \"\"\"æ ¼å¼åŒ–è©¦é©—ä¸Šä¸‹æ–‡\"\"\"\n",
    "    if \"error\" in trial_data:\n",
    "        return f\"éŒ¯èª¤: {trial_data['error']}\"\n",
    "    \n",
    "    sections = {\n",
    "        \"Eligibility\": trial_data.get(\"Eligibility\", []),\n",
    "        \"Intervention\": trial_data.get(\"Intervention\", []),\n",
    "        \"Results\": trial_data.get(\"Results\", []),\n",
    "        \"Adverse Events\": trial_data.get(\"Adverse_Events\", [])\n",
    "    }\n",
    "    \n",
    "    result = [f\"è©¦é©—ID: {trial_data.get('Clinical Trial ID', 'Unknown')}\"]\n",
    "    \n",
    "    if focus_section and focus_section in sections:\n",
    "        section_data = sections[focus_section]\n",
    "        result.append(f\"\\n{focus_section} å€æ®µ:\")\n",
    "        if isinstance(section_data, list):\n",
    "            for item in section_data[:5]:  # é™åˆ¶é•·åº¦\n",
    "                result.append(f\"  - {item}\")\n",
    "            if len(section_data) > 5:\n",
    "                result.append(f\"  ... (é‚„æœ‰ {len(section_data)-5} é …)\")\n",
    "        else:\n",
    "            result.append(f\"  {section_data}\")\n",
    "    else:\n",
    "        for section_name, section_data in sections.items():\n",
    "            if section_data:\n",
    "                result.append(f\"\\n{section_name}:\")\n",
    "                if isinstance(section_data, list):\n",
    "                    for item in section_data[:3]:  # é™åˆ¶é•·åº¦\n",
    "                        result.append(f\"  - {item}\")\n",
    "                    if len(section_data) > 3:\n",
    "                        result.append(f\"  ... (é‚„æœ‰ {len(section_data)-3} é …)\")\n",
    "                else:\n",
    "                    result.append(f\"  {section_data}\")\n",
    "    \n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "# æ¸¬è©¦å·¥å…·å‡½å¼\n",
    "sample_trial = load_clinical_trial(\"NCT00066573\")\n",
    "print(f\"âœ… è³‡æ–™å·¥å…·å‡½å¼æº–å‚™å°±ç·’ã€‚ç¯„ä¾‹è©¦é©—: {sample_trial.get('Clinical Trial ID', 'éŒ¯èª¤')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æµç¨‹æ¨ç†æ­¥é©Ÿ\n",
    "\n",
    "å®šç¾©äº”å€‹é€£çºŒçš„åˆ†ææ­¥é©Ÿï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step1_data_extraction(statement: str, trial_context: str) -> str:\n",
    "    \"\"\"æ­¥é©Ÿ1: è³‡æ–™æå–\"\"\"\n",
    "    prompt = f\"\"\"ä½ æ˜¯è³‡æ–™æå–å°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å¾è‡¨åºŠè©¦é©—è³‡æ–™ä¸­æå–èˆ‡é™³è¿°ç›¸é—œçš„é—œéµè³‡è¨Šã€‚\n",
    "\n",
    "é™³è¿°: \"{statement}\"\n",
    "\n",
    "è©¦é©—è³‡æ–™:\n",
    "{trial_context}\n",
    "\n",
    "è«‹æå–ä»¥ä¸‹è³‡è¨Šï¼š\n",
    "1. èˆ‡é™³è¿°ç›¸é—œçš„é—œéµæ•¸æ“š\n",
    "2. ç›¸é—œçš„è©¦é©—çµæœ\n",
    "3. é‡è¦çš„çµ±è¨ˆæ•¸å­—\n",
    "4. ä»»ä½•ç›¸é—œçš„é†«å­¸æ¦‚å¿µ\n",
    "\n",
    "è«‹ä»¥çµæ§‹åŒ–æ–¹å¼åˆ—å‡ºæå–çš„è³‡è¨Šï¼Œæœ€å¾Œä»¥ã€Œæå–å®Œæˆã€çµå°¾ã€‚\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"è³‡æ–™æå–éŒ¯èª¤: {str(e)}\"\n",
    "\n",
    "def step2_medical_analysis(statement: str, extracted_data: str) -> str:\n",
    "    \"\"\"æ­¥é©Ÿ2: é†«å­¸åˆ†æ\"\"\"\n",
    "    prompt = f\"\"\"ä½ æ˜¯é†«å­¸å°ˆå®¶ã€‚åŸºæ–¼æå–çš„è³‡æ–™ï¼Œåˆ†æé™³è¿°çš„é†«å­¸æº–ç¢ºæ€§ã€‚\n",
    "\n",
    "é™³è¿°: \"{statement}\"\n",
    "\n",
    "æå–çš„è³‡æ–™:\n",
    "{extracted_data}\n",
    "\n",
    "è«‹åˆ†æï¼š\n",
    "1. é†«å­¸è¡“èªæ˜¯å¦æ­£ç¢ºä½¿ç”¨\n",
    "2. è‡¨åºŠæ¦‚å¿µæ˜¯å¦åˆç†\n",
    "3. é™³è¿°æ˜¯å¦ç¬¦åˆé†«å­¸é‚è¼¯\n",
    "4. èˆ‡è©¦é©—è³‡æ–™çš„é†«å­¸ä¸€è‡´æ€§\n",
    "\n",
    "æœ€å¾Œä»¥ã€Œé†«å­¸åˆ†æ: [æ”¯æŒ/åé§/ä¸ç¢ºå®š]ã€çµå°¾ã€‚\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"é†«å­¸åˆ†æéŒ¯èª¤: {str(e)}\"\n",
    "\n",
    "def step3_statistical_analysis(statement: str, extracted_data: str, medical_analysis: str) -> str:\n",
    "    \"\"\"æ­¥é©Ÿ3: çµ±è¨ˆåˆ†æ\"\"\"\n",
    "    prompt = f\"\"\"ä½ æ˜¯çµ±è¨ˆåˆ†æå°ˆå®¶ã€‚åŸºæ–¼å‰é¢çš„åˆ†æï¼Œé©—è­‰é™³è¿°ä¸­çš„æ•¸å€¼å’Œçµ±è¨ˆè³‡è¨Šã€‚\n",
    "\n",
    "é™³è¿°: \"{statement}\"\n",
    "\n",
    "æå–çš„è³‡æ–™:\n",
    "{extracted_data}\n",
    "\n",
    "é†«å­¸åˆ†æ:\n",
    "{medical_analysis}\n",
    "\n",
    "è«‹åˆ†æï¼š\n",
    "1. æ•¸å€¼è¨ˆç®—æ˜¯å¦æ­£ç¢º\n",
    "2. çµ±è¨ˆæ–¹æ³•æ˜¯å¦é©ç•¶\n",
    "3. ç™¾åˆ†æ¯”å’Œæ¯”ç‡æ˜¯å¦æº–ç¢º\n",
    "4. æ•¸æ“šè§£é‡‹æ˜¯å¦åˆç†\n",
    "\n",
    "æœ€å¾Œä»¥ã€Œçµ±è¨ˆåˆ†æ: [æ­£ç¢º/éŒ¯èª¤/éƒ¨åˆ†æ­£ç¢º]ã€çµå°¾ã€‚\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"çµ±è¨ˆåˆ†æéŒ¯èª¤: {str(e)}\"\n",
    "\n",
    "def step4_logic_verification(statement: str, medical_analysis: str, statistical_analysis: str) -> str:\n",
    "    \"\"\"æ­¥é©Ÿ4: é‚è¼¯é©—è­‰\"\"\"\n",
    "    prompt = f\"\"\"ä½ æ˜¯é‚è¼¯é©—è­‰å°ˆå®¶ã€‚åŸºæ–¼å‰é¢çš„åˆ†æï¼Œæª¢æŸ¥é™³è¿°çš„é‚è¼¯ä¸€è‡´æ€§ã€‚\n",
    "\n",
    "é™³è¿°: \"{statement}\"\n",
    "\n",
    "é†«å­¸åˆ†æ:\n",
    "{medical_analysis}\n",
    "\n",
    "çµ±è¨ˆåˆ†æ:\n",
    "{statistical_analysis}\n",
    "\n",
    "è«‹é©—è­‰ï¼š\n",
    "1. æ¨ç†é‚è¼¯æ˜¯å¦åˆç†\n",
    "2. å› æœé—œä¿‚æ˜¯å¦æ­£ç¢º\n",
    "3. çµè«–æ˜¯å¦å¾è­‰æ“šå¾—å‡º\n",
    "4. æ˜¯å¦å­˜åœ¨é‚è¼¯çŸ›ç›¾\n",
    "\n",
    "æœ€å¾Œä»¥ã€Œé‚è¼¯é©—è­‰: [åˆç†/ä¸åˆç†/æœ‰ç–‘æ…®]ã€çµå°¾ã€‚\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"é‚è¼¯é©—è­‰éŒ¯èª¤: {str(e)}\"\n",
    "\n",
    "def step5_decision_synthesis(statement: str, data_extraction: str, medical_analysis: str, \n",
    "                           statistical_analysis: str, logic_verification: str) -> str:\n",
    "    \"\"\"æ­¥é©Ÿ5: æ±ºç­–ç¶œåˆ\"\"\"\n",
    "    prompt = f\"\"\"ä½ æ˜¯æ±ºç­–ç¶œåˆå°ˆå®¶ã€‚åŸºæ–¼æ‰€æœ‰å‰é¢çš„åˆ†ææ­¥é©Ÿï¼Œåšå‡ºæœ€çµ‚çš„è˜Šå«åˆ¤æ–·ã€‚\n",
    "\n",
    "é™³è¿°: \"{statement}\"\n",
    "\n",
    "è³‡æ–™æå–:\n",
    "{data_extraction}\n",
    "\n",
    "é†«å­¸åˆ†æ:\n",
    "{medical_analysis}\n",
    "\n",
    "çµ±è¨ˆåˆ†æ:\n",
    "{statistical_analysis}\n",
    "\n",
    "é‚è¼¯é©—è­‰:\n",
    "{logic_verification}\n",
    "\n",
    "ä»»å‹™: åˆ¤æ–·é™³è¿°æ˜¯ã€Œè˜Šå«ã€(Entailment)é‚„æ˜¯ã€ŒçŸ›ç›¾ã€(Contradiction)\n",
    "- è˜Šå«: é™³è¿°è¢«è©¦é©—è­‰æ“šæ”¯æŒ\n",
    "- çŸ›ç›¾: é™³è¿°è¢«è©¦é©—è­‰æ“šåé§\n",
    "\n",
    "è«‹ç¶œåˆæ‰€æœ‰æ­¥é©Ÿçš„åˆ†æï¼Œæä¾›ç°¡è¦ç†ç”±ï¼Œç„¶å¾Œä»¥ã€Œæœ€çµ‚æ±ºç­–: [Entailment/Contradiction]ã€çµå°¾ã€‚\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"æ±ºç­–ç¶œåˆéŒ¯èª¤: {str(e)}\"\n",
    "\n",
    "print(\"âœ… äº”å€‹æµç¨‹æ¨ç†æ­¥é©Ÿå®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æµç¨‹æ¨ç†ç®¡é“\n",
    "\n",
    "å‰µå»ºåŸ·è¡Œæ‰€æœ‰æ­¥é©Ÿçš„å®Œæ•´åˆ†æç®¡é“ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def workflow_reasoning_pipeline(statement: str, primary_id: str, secondary_id: str = None, \n",
    "                              section_id: str = None, verbose: bool = False) -> str:\n",
    "    \"\"\"é‹è¡Œå®Œæ•´çš„æµç¨‹æ¨ç†ç®¡é“\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # æº–å‚™è©¦é©—ä¸Šä¸‹æ–‡\n",
    "        primary_data = load_clinical_trial(primary_id)\n",
    "        trial_context = format_trial_context(primary_data, section_id)\n",
    "        \n",
    "        if secondary_id:\n",
    "            secondary_data = load_clinical_trial(secondary_id)\n",
    "            secondary_context = format_trial_context(secondary_data, section_id)\n",
    "            trial_context += f\"\\n\\næ¬¡è¦è©¦é©—:\\n{secondary_context}\"\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"ğŸ“„ åˆ†æé™³è¿°: {statement[:100]}...\")\n",
    "            print(f\"ğŸ¥ ä¸»è¦è©¦é©—: {primary_id}\")\n",
    "            if secondary_id:\n",
    "                print(f\"ğŸ¥ æ¬¡è¦è©¦é©—: {secondary_id}\")\n",
    "        \n",
    "        # æ­¥é©Ÿ1: è³‡æ–™æå–\n",
    "        data_extraction = step1_data_extraction(statement, trial_context)\n",
    "        if verbose:\n",
    "            print(\"ğŸ“Š æ­¥é©Ÿ1: è³‡æ–™æå–å®Œæˆ\")\n",
    "        \n",
    "        # æ­¥é©Ÿ2: é†«å­¸åˆ†æ\n",
    "        medical_analysis = step2_medical_analysis(statement, data_extraction)\n",
    "        if verbose:\n",
    "            print(\"ğŸ©º æ­¥é©Ÿ2: é†«å­¸åˆ†æå®Œæˆ\")\n",
    "        \n",
    "        # æ­¥é©Ÿ3: çµ±è¨ˆåˆ†æ\n",
    "        statistical_analysis = step3_statistical_analysis(statement, data_extraction, medical_analysis)\n",
    "        if verbose:\n",
    "            print(\"ğŸ“ˆ æ­¥é©Ÿ3: çµ±è¨ˆåˆ†æå®Œæˆ\")\n",
    "        \n",
    "        # æ­¥é©Ÿ4: é‚è¼¯é©—è­‰\n",
    "        logic_verification = step4_logic_verification(statement, medical_analysis, statistical_analysis)\n",
    "        if verbose:\n",
    "            print(\"ğŸ§  æ­¥é©Ÿ4: é‚è¼¯é©—è­‰å®Œæˆ\")\n",
    "        \n",
    "        # æ­¥é©Ÿ5: æ±ºç­–ç¶œåˆ\n",
    "        final_decision = step5_decision_synthesis(\n",
    "            statement, data_extraction, medical_analysis, \n",
    "            statistical_analysis, logic_verification\n",
    "        )\n",
    "        \n",
    "        # æå–æœ€çµ‚æ±ºç­–\n",
    "        if \"æœ€çµ‚æ±ºç­–: Entailment\" in final_decision:\n",
    "            decision = \"Entailment\"\n",
    "        elif \"æœ€çµ‚æ±ºç­–: Contradiction\" in final_decision:\n",
    "            decision = \"Contradiction\"\n",
    "        else:\n",
    "            # å‚™ç”¨è§£æ\n",
    "            if \"entailment\" in final_decision.lower() and \"contradiction\" not in final_decision.lower():\n",
    "                decision = \"Entailment\"\n",
    "            else:\n",
    "                decision = \"Contradiction\"\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"âš–ï¸ æ­¥é©Ÿ5: æœ€çµ‚æ±ºç­– - {decision}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        return decision\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"âŒ ç®¡é“éŒ¯èª¤: {e}\")\n",
    "        return \"Contradiction\"  # ä¿å®ˆçš„å‚™ç”¨æ–¹æ¡ˆ\n",
    "\n",
    "print(\"âœ… æµç¨‹æ¨ç†åˆ†æç®¡é“æº–å‚™å°±ç·’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¸¬è©¦ç¯„ä¾‹\n",
    "\n",
    "æ¸¬è©¦æˆ‘å€‘çš„æµç¨‹æ¨ç†ç³»çµ±ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦ç¯„ä¾‹\n",
    "test_statement = \"there is a 13.2% difference between the results from the two the primary trial cohorts\"\n",
    "test_primary_id = \"NCT00066573\"\n",
    "\n",
    "print(f\"æ¸¬è©¦æµç¨‹æ¨ç†ç³»çµ±:\")\n",
    "print(f\"é™³è¿°: '{test_statement}'\")\n",
    "print(f\"ä¸»è¦è©¦é©—: {test_primary_id}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# åŸ·è¡Œåˆ†æ\n",
    "start_time = time.time()\n",
    "result = workflow_reasoning_pipeline(\n",
    "    statement=test_statement,\n",
    "    primary_id=test_primary_id,\n",
    "    section_id=\"Results\",\n",
    "    verbose=True\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nğŸ¯ æµç¨‹æ¨ç†çµæœ: {result}\")\n",
    "print(f\"â±ï¸ åŸ·è¡Œæ™‚é–“: {end_time - start_time:.2f} ç§’\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åœ¨è¨“ç·´è³‡æ–™ä¸Šè©•ä¼°\n",
    "\n",
    "åœ¨è¨“ç·´è³‡æ–™æ¨£æœ¬ä¸Šè©•ä¼°æˆ‘å€‘çš„ç³»çµ±ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥è¨“ç·´è³‡æ–™\n",
    "with open(\"training_data/train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "print(f\"è¼‰å…¥ {len(train_data)} å€‹è¨“ç·´ç¯„ä¾‹\")\n",
    "\n",
    "# åœ¨æ¨£æœ¬ä¸Šè©•ä¼°\n",
    "sample_size = 15\n",
    "examples = list(train_data.items())[:sample_size]\n",
    "\n",
    "print(f\"\\nåœ¨ {len(examples)} å€‹ç¯„ä¾‹ä¸Šè©•ä¼°æµç¨‹æ¨ç†ç³»çµ±...\")\n",
    "\n",
    "results = []\n",
    "correct = 0\n",
    "total_time = 0\n",
    "\n",
    "for i, (uuid, example) in enumerate(tqdm(examples, desc=\"æµç¨‹æ¨ç†è™•ç†\")):\n",
    "    try:\n",
    "        statement = example.get(\"Statement\")\n",
    "        primary_id = example.get(\"Primary_id\")\n",
    "        secondary_id = example.get(\"Secondary_id\")\n",
    "        section_id = example.get(\"Section_id\")\n",
    "        expected = example.get(\"Label\")\n",
    "        \n",
    "        if not statement or not primary_id:\n",
    "            results.append({\n",
    "                \"uuid\": uuid,\n",
    "                \"expected\": expected,\n",
    "                \"predicted\": \"SKIPPED\",\n",
    "                \"correct\": False,\n",
    "                \"time\": 0\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # ç²å–é æ¸¬\n",
    "        start_time = time.time()\n",
    "        predicted = workflow_reasoning_pipeline(\n",
    "            statement=statement,\n",
    "            primary_id=primary_id,\n",
    "            secondary_id=secondary_id,\n",
    "            section_id=section_id,\n",
    "            verbose=False\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        execution_time = end_time - start_time\n",
    "        total_time += execution_time\n",
    "        \n",
    "        # æª¢æŸ¥æ­£ç¢ºæ€§\n",
    "        is_correct = (predicted.strip() == expected.strip())\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            \n",
    "        results.append({\n",
    "            \"uuid\": uuid,\n",
    "            \"statement\": statement[:80] + \"...\" if len(statement) > 80 else statement,\n",
    "            \"expected\": expected,\n",
    "            \"predicted\": predicted,\n",
    "            \"correct\": is_correct,\n",
    "            \"time\": execution_time\n",
    "        })\n",
    "        \n",
    "        status = \"âœ…\" if is_correct else \"âŒ\"\n",
    "        print(f\"ç¯„ä¾‹ {i+1:2d}: {expected:12} -> {predicted:12} {status} ({execution_time:.1f}s)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"è™•ç†ç¯„ä¾‹ {i+1} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        results.append({\n",
    "            \"uuid\": uuid,\n",
    "            \"expected\": expected,\n",
    "            \"predicted\": \"ERROR\",\n",
    "            \"correct\": False,\n",
    "            \"time\": 0\n",
    "        })\n",
    "\n",
    "# è¨ˆç®—æº–ç¢ºç‡\n",
    "accuracy = correct / len(examples) if examples else 0\n",
    "avg_time = total_time / len(examples) if examples else 0\n",
    "\n",
    "print(f\"\\nğŸ“Š æµç¨‹æ¨ç†ç³»çµ±çµæœ:\")\n",
    "print(f\"æº–ç¢ºç‡: {accuracy:.2%} ({correct}/{len(examples)})\")\n",
    "print(f\"å¹³å‡åŸ·è¡Œæ™‚é–“: {avg_time:.2f} ç§’/ä¾‹\")\n",
    "print(f\"ç¸½åŸ·è¡Œæ™‚é–“: {total_time:.2f} ç§’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç”¢ç”Ÿæäº¤æª”æ¡ˆ\n",
    "\n",
    "ä½¿ç”¨æˆ‘å€‘çš„æµç¨‹æ¨ç†ç³»çµ±ç”¢ç”Ÿé æ¸¬çµæœï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_workflow_submission(test_file=\"test.json\", output_file=\"workflow_reasoning_submission.json\", sample_size=None):\n",
    "    \"\"\"ä½¿ç”¨æµç¨‹æ¨ç†ç³»çµ±ç”¢ç”Ÿæäº¤æª”æ¡ˆ\"\"\"\n",
    "    \n",
    "    # è¼‰å…¥æ¸¬è©¦è³‡æ–™\n",
    "    try:\n",
    "        with open(test_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            test_data = json.load(f)\n",
    "    except:\n",
    "        print(f\"âŒ ç„¡æ³•è¼‰å…¥æ¸¬è©¦è³‡æ–™ {test_file}\")\n",
    "        return\n",
    "    \n",
    "    examples = list(test_data.items())\n",
    "    if sample_size:\n",
    "        examples = examples[:sample_size]\n",
    "        \n",
    "    print(f\"ğŸš€ ç‚º {len(examples)} å€‹ç¯„ä¾‹ç”¢ç”Ÿæµç¨‹æ¨ç†é æ¸¬...\")\n",
    "    \n",
    "    submission = {}\n",
    "    \n",
    "    for i, (uuid, example) in enumerate(tqdm(examples, desc=\"æµç¨‹æ¨ç†è™•ç†\")):\n",
    "        try:\n",
    "            statement = example.get(\"Statement\")\n",
    "            primary_id = example.get(\"Primary_id\")\n",
    "            secondary_id = example.get(\"Secondary_id\")\n",
    "            section_id = example.get(\"Section_id\")\n",
    "            \n",
    "            if not statement or not primary_id:\n",
    "                submission[uuid] = {\"Prediction\": \"Contradiction\"}\n",
    "                continue\n",
    "                \n",
    "            # ç²å–é æ¸¬\n",
    "            prediction = workflow_reasoning_pipeline(\n",
    "                statement=statement,\n",
    "                primary_id=primary_id,\n",
    "                secondary_id=secondary_id,\n",
    "                section_id=section_id,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            submission[uuid] = {\"Prediction\": prediction}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"è™•ç† {uuid} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "            submission[uuid] = {\"Prediction\": \"Contradiction\"}\n",
    "    \n",
    "    # å„²å­˜æäº¤æª”æ¡ˆ\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(submission, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… æµç¨‹æ¨ç†æäº¤æª”æ¡ˆå·²å„²å­˜è‡³ {output_file}\")\n",
    "    return submission\n",
    "\n",
    "# ç”¢ç”Ÿå°æ¨£æœ¬æäº¤\n",
    "workflow_submission = generate_workflow_submission(\n",
    "    test_file=\"test.json\", \n",
    "    output_file=\"workflow_reasoning_submission.json\",\n",
    "    sample_size=10\n",
    ")\n",
    "\n",
    "print(f\"ç‚º {len(workflow_submission)} å€‹ç¯„ä¾‹ç”¢ç”Ÿäº†é æ¸¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## çµè«–\n",
    "\n",
    "### æµç¨‹æ¨ç†ç³»çµ±å„ªå‹¢ï¼š\n",
    "1. **çµæ§‹åŒ–æ€è€ƒ**: æ¸…æ™°çš„äº”æ­¥é©Ÿåˆ†ææµç¨‹\n",
    "2. **å¾ªåºæ¼¸é€²**: æ¯æ­¥é©Ÿå»ºç«‹åœ¨å‰ä¸€æ­¥åŸºç¤ä¸Š\n",
    "3. **å…¨é¢åˆ†æ**: æ¶µè“‹è³‡æ–™ã€é†«å­¸ã€çµ±è¨ˆã€é‚è¼¯å„å±¤é¢\n",
    "4. **é€æ˜åº¦é«˜**: æ¯å€‹æ¨ç†æ­¥é©Ÿéƒ½å¯è¿½è¹¤\n",
    "5. **æ¨¡çµ„åŒ–**: å¯ä»¥ç¨ç«‹æ”¹é€²æ¯å€‹æ­¥é©Ÿ\n",
    "\n",
    "### äº”æ­¥é©Ÿæ¨ç†æµç¨‹ï¼š\n",
    "1. **è³‡æ–™æå–**: å¾è©¦é©—è³‡æ–™ä¸­æå–é—œéµè³‡è¨Š\n",
    "2. **é†«å­¸åˆ†æ**: è©•ä¼°é†«å­¸æ¦‚å¿µå’Œè¡“èªçš„æº–ç¢ºæ€§\n",
    "3. **çµ±è¨ˆåˆ†æ**: é©—è­‰æ•¸å€¼è¨ˆç®—å’Œçµ±è¨ˆæ–¹æ³•\n",
    "4. **é‚è¼¯é©—è­‰**: æª¢æŸ¥æ¨ç†é‚è¼¯å’Œå› æœé—œä¿‚\n",
    "5. **æ±ºç­–ç¶œåˆ**: æ•´åˆæ‰€æœ‰åˆ†æåšå‡ºæœ€çµ‚åˆ¤æ–·\n",
    "\n",
    "### é©ç”¨å ´æ™¯ï¼š\n",
    "- éœ€è¦è©³ç´°æ¨ç†éç¨‹çš„åˆ†æä»»å‹™\n",
    "- è¤‡é›œçš„å¤šå±¤æ¬¡æ±ºç­–å•é¡Œ\n",
    "- éœ€è¦æ­¥é©Ÿè¿½è¹¤çš„å“è³ªç®¡æ§\n",
    "- æ•™å­¸å’Œè¨“ç·´ç”¨é€”\n",
    "\n",
    "é€™å€‹æµç¨‹æ¨ç†ç³»çµ±å±•ç¤ºäº†å¦‚ä½•é€šéçµæ§‹åŒ–çš„åˆ†ææ­¥é©Ÿä¾†è™•ç†è¤‡é›œçš„è‡¨åºŠè©¦é©—NLPä»»å‹™ï¼Œæ¯å€‹æ­¥é©Ÿéƒ½æœ‰æ˜ç¢ºçš„ç›®æ¨™å’Œè¼¸å‡ºï¼Œå½¢æˆå®Œæ•´çš„æ¨ç†éˆã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
