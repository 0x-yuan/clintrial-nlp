#!/usr/bin/env python
# coding: utf-8

# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/0x-yuan/clintrial-nlp/blob/main/atomic_agents_baseline.ipynb)
# 
# # Atomic Agents æ¡†æ¶åŸºç·š - è‡¨åºŠè©¦é©— NLP
# 
# ## æ¦‚è¿°
# 
# æœ¬notebookå±•ç¤ºå¦‚ä½•ä½¿ç”¨Atomic Agentsæ¡†æ¶å»ºæ§‹ä¸€å€‹è¼•é‡ç´šã€é«˜æ•ˆèƒ½çš„å¤šä»£ç†ç³»çµ±ï¼Œç”¨æ–¼è‡¨åºŠè©¦é©—è‡ªç„¶èªè¨€æ¨ç†(NLI)ã€‚Atomic Agentså°ˆç‚ºç”Ÿç”¢ç’°å¢ƒè¨­è¨ˆï¼Œå…·æœ‰æ¥µå¿«çš„å•Ÿå‹•æ™‚é–“(~3Î¼s)å’Œæ¨¡çµ„åŒ–æ¶æ§‹ã€‚
# 
# ## ğŸ“š å­¸ç¿’ç›®æ¨™
# å®Œæˆæœ¬æ•™å­¸å¾Œï¼Œæ‚¨å°‡å­¸æœƒï¼š
# - ç†è§£ Atomic Agents æ¡†æ¶çš„æ ¸å¿ƒæ¦‚å¿µ
# - å»ºç«‹å°ˆé–€çš„é†«ç™‚ã€æ•¸å€¼å’Œé‚è¼¯åˆ†æä»£ç†
# - å¯¦ä½œå¤šä»£ç†å”ä½œpipeline
# - è©•ä¼°å’Œæ”¹é€²ç³»çµ±æ•ˆèƒ½
# 
# ### ç‚ºä»€éº¼é¸æ“‡ Atomic Agentsï¼Ÿ
# - **è¶…è¼•é‡ç´š**: æœ€å°åŒ–é–‹éŠ·å’Œå¿«é€ŸåŸ·è¡Œ
# - **é«˜åº¦æ¨¡çµ„åŒ–**: æ˜“æ–¼çµ„åˆå’Œä¿®æ”¹ä»£ç†
# - **ç”Ÿç”¢å°±ç·’**: å°ˆç‚ºå¯¦éš›éƒ¨ç½²è€Œè¨­è¨ˆ
# - **ç°¡å–®API**: ç›´è§€çš„ä»£ç†å‰µå»ºå’Œå”èª¿
# - **è¨˜æ†¶é«”ç®¡ç†**: å…§å»ºçš„ä¸Šä¸‹æ–‡å’Œè¨˜æ†¶é«”è™•ç†
# 
# ### ğŸ—ï¸ ä»£ç†æ¶æ§‹
# éµå¾ªæ•™å­¸åœ–è¡¨ï¼Œæˆ‘å€‘å¯¦ä½œä¸€å€‹çµæ§‹åŒ–çš„pipelineï¼š
# 1. **é†«ç™‚å°ˆå®¶ä»£ç†**: åˆ†æé†«å­¸è¡“èªå’Œæ¦‚å¿µ
# 2. **æ•¸å€¼åˆ†æä»£ç†**: è™•ç†é‡åŒ–æ•¸æ“š
# 3. **é‚è¼¯æª¢æŸ¥ä»£ç†**: é©—è­‰é‚è¼¯é—œä¿‚
# 4. **èšåˆä»£ç†**: çµåˆè¦‹è§£åšå‡ºæœ€çµ‚æ±ºç­–
# 5. **ç›£ç£å”èª¿**: ç®¡ç†æ•´é«”å·¥ä½œæµç¨‹
# 
# > ğŸ’¡ **é‡è¦æ¦‚å¿µ**: Atomic Agents çš„"atomic"æŒ‡çš„æ˜¯æ¯å€‹ä»£ç†éƒ½æ˜¯ä¸€å€‹ç¨ç«‹ã€æœ€å°çš„åŠŸèƒ½å–®å…ƒï¼Œå¯ä»¥è¼•é¬†çµ„åˆæˆè¤‡é›œç³»çµ±ã€‚

# In[ ]:


# ğŸ”§ Colab ç’°å¢ƒè¨­ç½® - ä¸€éµå®‰è£æ‰€éœ€å¥—ä»¶
# é€™å€‹cellæœƒéœé»˜å®‰è£æ‰€æœ‰å¿…è¦çš„Pythonå¥—ä»¶ï¼Œè®“æ‚¨å¯ä»¥åœ¨Colabä¸­ç›´æ¥é‹è¡Œæ­¤notebook
get_ipython().system('pip install -q atomic-agents python-dotenv pandas tqdm')
get_ipython().system('pip install -q google-generativeai gdown')

print("âœ… æ‰€æœ‰å¥—ä»¶å®‰è£å®Œæˆï¼å¯ä»¥é–‹å§‹ä½¿ç”¨ Atomic Agents æ¡†æ¶äº†")


# In[ ]:


# ğŸ“¥ å¾ Google Drive ä¸‹è¼‰è¨“ç·´è³‡æ–™
# é€™å€‹cellæœƒè‡ªå‹•ä¸‹è¼‰ä¸¦è§£å£“ç¸® clinicaltrial-nlp.zipï¼Œç¢ºä¿åœ¨Colabä¸­å¯ä»¥ç›´æ¥é‹è¡Œ
import os
import gdown
import zipfile
import shutil

# Google Drive zip æª”æ¡ˆ ID
file_id = "15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR"
zip_url = f"https://drive.google.com/uc?id={file_id}"
zip_filename = "clinicaltrial-nlp.zip"

# æª¢æŸ¥æ˜¯å¦å·²æœ‰è¨“ç·´è³‡æ–™
if not os.path.exists("training_data"):
    print("ğŸ“¥ å¾ Google Drive ä¸‹è¼‰ clinicaltrial-nlp.zip...")
    print("âš ï¸ å¦‚æœä¸‹è¼‰å¤±æ•—ï¼Œè«‹ç¢ºèª:")
    print("1. Google Drive é€£çµçš„æ¬Šé™è¨­å®šç‚º 'çŸ¥é“é€£çµçš„ä½¿ç”¨è€…'")
    print("2. ç¶²è·¯é€£ç·šæ­£å¸¸")
    print(f"3. æª”æ¡ˆé€£çµ: {zip_url}")
    
    try:
        # ä¸‹è¼‰ zip æª”æ¡ˆ
        print("ğŸ“¥ æ­£åœ¨ä¸‹è¼‰ zip æª”æ¡ˆ...")
        gdown.download(zip_url, zip_filename, quiet=False)
        
        # è§£å£“ç¸®æª”æ¡ˆ
        print("ğŸ“¦ æ­£åœ¨è§£å£“ç¸®æª”æ¡ˆ...")
        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
            zip_ref.extractall(".")
        
        # ç§»å‹• training_data åˆ°æ­£ç¢ºä½ç½®ï¼ˆå¦‚æœåœ¨å­è³‡æ–™å¤¾ä¸­ï¼‰
        if os.path.exists("clintrial-nlp/training_data") and not os.path.exists("training_data"):
            print("ğŸ“ ç§»å‹• training_data åˆ°æ­£ç¢ºä½ç½®...")
            shutil.move("clintrial-nlp/training_data", "training_data")
            # æ¸…ç†è§£å£“ç¸®çš„è³‡æ–™å¤¾
            if os.path.exists("clintrial-nlp"):
                shutil.rmtree("clintrial-nlp")
            if os.path.exists("__MACOSX"):  # æ¸…ç† macOS ç”¢ç”Ÿçš„éš±è—æª”æ¡ˆ
                shutil.rmtree("__MACOSX")
        
        # æ¸…ç† zip æª”æ¡ˆ
        os.remove(zip_filename)
        print("âœ… è¨“ç·´è³‡æ–™ä¸‹è¼‰ä¸¦è§£å£“ç¸®å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ä¸‹è¼‰å¤±æ•—: {e}")
        print("\nğŸ”§ æ‰‹å‹•è§£æ±ºæ–¹æ¡ˆ:")
        print("1. é»æ“Šæ­¤é€£çµä¸‹è¼‰ zip æª”æ¡ˆ:")
        print("   https://drive.google.com/file/d/15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR/view?usp=sharing")
        print("2. ä¸Šå‚³ zip æª”æ¡ˆåˆ° Colab")
        print("3. è§£å£“ç¸®å¾Œé‡æ–°åŸ·è¡Œå¾ŒçºŒçš„ cells")
        
        # å‰µå»ºä¸€å€‹æç¤ºæª”æ¡ˆ
        os.makedirs("training_data", exist_ok=True)
        with open("training_data/DOWNLOAD_INSTRUCTIONS.txt", "w", encoding="utf-8") as f:
            f.write("è«‹æ‰‹å‹•ä¸‹è¼‰ä¸¦è§£å£“ç¸® clinicaltrial-nlp.zip:\n")
            f.write("https://drive.google.com/file/d/15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR/view?usp=sharing\n")
        
        print("\nğŸ“ å·²å‰µå»ºä¸‹è¼‰æŒ‡ç¤ºæª”æ¡ˆæ–¼ training_data/DOWNLOAD_INSTRUCTIONS.txt")
else:
    print("âœ… è¨“ç·´è³‡æ–™å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰")

# æª¢æŸ¥ä¸‹è¼‰çš„è³‡æ–™çµæ§‹
if os.path.exists("training_data"):
    contents = os.listdir("training_data")
    print(f"ğŸ“‚ è³‡æ–™å¤¾å…§å®¹: {contents}")
    if os.path.exists("training_data/CT json"):
        ct_files = len([f for f in os.listdir("training_data/CT json") if f.endswith('.json')])
        print(f"ğŸ“„ æ‰¾åˆ° {ct_files} å€‹è‡¨åºŠè©¦é©—JSONæª”æ¡ˆ")
    else:
        print("âš ï¸ æ‰¾ä¸åˆ° 'CT json' å­è³‡æ–™å¤¾ï¼Œè«‹æª¢æŸ¥ä¸‹è¼‰æ˜¯å¦å®Œæ•´")


# In[ ]:


# ğŸ§ª æº–å‚™æ¸¬è©¦è³‡æ–™é›†
# å¦‚æœæ²’æœ‰ test.jsonï¼Œå¾è¨“ç·´è³‡æ–™å‰µå»ºä¸€å€‹æ¸¬è©¦é›†
import json

def create_test_data_if_needed():
    """å¦‚æœä¸å­˜åœ¨ test.jsonï¼Œå¾ train.json å‰µå»ºä¸€å€‹å°çš„æ¸¬è©¦é›†"""
    if not os.path.exists("test.json"):
        try:
            # è¼‰å…¥è¨“ç·´è³‡æ–™
            with open("training_data/train.json", "r", encoding="utf-8") as f:
                train_data = json.load(f)
            
            # å–å‰100å€‹æ¨£æœ¬ä½œç‚ºæ¸¬è©¦è³‡æ–™
            test_data = dict(list(train_data.items())[:100])
            
            # å„²å­˜æ¸¬è©¦è³‡æ–™
            with open("test.json", "w", encoding="utf-8") as f:
                json.dump(test_data, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… å·²å‰µå»ºæ¸¬è©¦è³‡æ–™é›†ï¼ŒåŒ…å« {len(test_data)} å€‹æ¨£æœ¬")
        except Exception as e:
            print(f"âŒ å‰µå»ºæ¸¬è©¦è³‡æ–™å¤±æ•—: {e}")
    else:
        print("âœ… test.json å·²å­˜åœ¨")

# åŸ·è¡Œæ¸¬è©¦è³‡æ–™æº–å‚™
create_test_data_if_needed()


# ## ç’°å¢ƒè¨­ç½®å’Œå®‰è£
# 
# é¦–å…ˆï¼Œè®“æˆ‘å€‘è¨­ç½®ç’°å¢ƒä¸¦åŒ¯å…¥å¿…è¦çš„å‡½å¼åº«ï¼š
# 
# > ğŸ“ **èªªæ˜**: åœ¨é€™å€‹æ­¥é©Ÿä¸­ï¼Œæˆ‘å€‘æœƒè¼‰å…¥ç’°å¢ƒè®Šæ•¸ä¸¦ç¢ºèªæ‰€æœ‰å¿…è¦çš„å¥—ä»¶éƒ½å·²æ­£ç¢ºå®‰è£ã€‚

# In[ ]:


# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
# é€™å€‹æ­¥é©Ÿæœƒå¾ .env æª”æ¡ˆè¼‰å…¥ API é‡‘é‘°ç­‰æ•æ„Ÿè³‡è¨Š
from dotenv import load_dotenv
import os

load_dotenv()
print("âœ… ç’°å¢ƒè®Šæ•¸è¼‰å…¥å®Œæˆ")


# In[ ]:


# åŒ¯å…¥å¿…è¦çš„å‡½å¼åº«
# é€™äº›å‡½å¼åº«æä¾›äº†è³‡æ–™è™•ç†ã€AIæ¨¡å‹å’ŒAtomic Agentsæ¡†æ¶çš„åŠŸèƒ½
import json
import pandas as pd
from tqdm import tqdm  # é€²åº¦æ¢é¡¯ç¤º
import google.generativeai as genai  # Google Gemini API
from typing import Dict, List, Any, Optional
import warnings
warnings.filterwarnings('ignore')  # éš±è—è­¦å‘Šè¨Šæ¯

# Atomic Agents æ ¸å¿ƒçµ„ä»¶
from atomic_agents.lib.components.agent_memory import AgentMemory  # ä»£ç†è¨˜æ†¶é«”ç®¡ç†
from atomic_agents.agents.base_agent import BaseAgent, BaseAgentConfig, BaseAgentInputSchema  # åŸºç¤ä»£ç†é¡åˆ¥
from atomic_agents.lib.components.system_prompt_generator import SystemPromptGenerator  # ç³»çµ±æç¤ºç”Ÿæˆå™¨

print("âœ… æ‰€æœ‰å‡½å¼åº«åŒ¯å…¥æˆåŠŸ")


# ## è³‡æ–™è¼‰å…¥å’Œå·¥å…·å‡½å¼
# 
# è®“æˆ‘å€‘å»ºç«‹ç”¨æ–¼è¼‰å…¥å’Œè™•ç†è‡¨åºŠè©¦é©—è³‡æ–™çš„å·¥å…·å‡½å¼ï¼š
# 
# > ğŸ”§ **åŠŸèƒ½èªªæ˜**: é€™äº›å‡½å¼è² è²¬å¾JSONæª”æ¡ˆä¸­è¼‰å…¥è‡¨åºŠè©¦é©—è³‡æ–™ï¼Œä¸¦å°‡å…¶è½‰æ›ç‚ºé©åˆAIä»£ç†åˆ†æçš„æ ¼å¼ã€‚

# In[ ]:


def load_clinical_trial(trial_id: str) -> Dict[str, Any]:
    """è¼‰å…¥è‡¨åºŠè©¦é©—è³‡æ–™å¾JSONæª”æ¡ˆã€‚
    
    Args:
        trial_id: è‡¨åºŠè©¦é©—çš„NCTè­˜åˆ¥ç¢¼
        
    Returns:
        åŒ…å«è©¦é©—è³‡æ–™çš„å­—å…¸æˆ–éŒ¯èª¤è³‡è¨Š
    """
    try:
        # ç¢ºä¿ä½¿ç”¨ä¸‹è¼‰çš„è³‡æ–™å¤¾è·¯å¾‘
        file_path = os.path.join("training_data", "CT json", f"{trial_id}.json")
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data
    except FileNotFoundError:
        return {"error": f"æ‰¾ä¸åˆ°è‡¨åºŠè©¦é©— {trial_id}"}
    except Exception as e:
        return {"error": f"è¼‰å…¥ {trial_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"}

def load_dataset(filepath: str) -> Dict[str, Any]:
    """è¼‰å…¥è¨“ç·´æˆ–æ¸¬è©¦è³‡æ–™é›†ã€‚
    
    Args:
        filepath: JSONè³‡æ–™é›†æª”æ¡ˆçš„è·¯å¾‘ï¼ˆæœƒè‡ªå‹•æª¢æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ  training_data/ å‰ç¶´ï¼‰
        
    Returns:
        åŒ…å«è³‡æ–™é›†çš„å­—å…¸
    """
    try:
        # å¦‚æœè·¯å¾‘ä¸æ˜¯çµ•å°è·¯å¾‘ä¸”ä¸åŒ…å« training_dataï¼Œè‡ªå‹•æ·»åŠ å‰ç¶´
        if not os.path.isabs(filepath) and not filepath.startswith("training_data/"):
            # å°æ–¼æ¨™æº–æª”æ¡ˆåï¼Œä½¿ç”¨ training_data å‰ç¶´
            if filepath in ["train.json", "dev.json"]:
                filepath = os.path.join("training_data", filepath)
        
        with open(filepath, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"è¼‰å…¥è³‡æ–™é›†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return {}

def extract_relevant_sections(trial_data: Dict[str, Any], section_id: str) -> str:
    """æ ¹æ“šsection_idå¾è©¦é©—è³‡æ–™ä¸­æå–ç›¸é—œéƒ¨åˆ†ã€‚
    
    Args:
        trial_data: è‡¨åºŠè©¦é©—è³‡æ–™å­—å…¸
        section_id: ç›®æ¨™å€æ®µ (Eligibility, Intervention, Results, Adverse Events)
        
    Returns:
        åŒ…å«ç›¸é—œå€æ®µè³‡æ–™çš„æ ¼å¼åŒ–å­—ä¸²
    """
    if "error" in trial_data:
        return f"éŒ¯èª¤: {trial_data['error']}"
    
    sections = {
        "Eligibility": trial_data.get("Eligibility", []),
        "Intervention": trial_data.get("Intervention", []),
        "Results": trial_data.get("Results", []),
        "Adverse Events": trial_data.get("Adverse_Events", [])
    }
    
    # å¦‚æœè«‹æ±‚ç‰¹å®šå€æ®µï¼Œå‰‡åƒ…è¿”å›è©²å€æ®µ
    if section_id in sections:
        section_data = sections[section_id]
        if isinstance(section_data, list):
            return "\n".join(str(item) for item in section_data)
        return str(section_data)
    
    # å¦å‰‡è¿”å›æ‰€æœ‰å€æ®µ
    result = []
    for section_name, section_data in sections.items():
        if section_data:
            result.append(f"{section_name}:")
            if isinstance(section_data, list):
                result.extend([f"  {item}" for item in section_data])
            else:
                result.append(f"  {section_data}")
    
    return "\n".join(result)

# æ¸¬è©¦å·¥å…·å‡½å¼
sample_trial = load_clinical_trial("NCT00066573")
print(f"âœ… è³‡æ–™å·¥å…·å‡½å¼æº–å‚™å°±ç·’ã€‚ç¯„ä¾‹è©¦é©—: {sample_trial.get('Clinical Trial ID', 'éŒ¯èª¤')}")

# ğŸ“‹ å‡½å¼èªªæ˜ï¼š
# - load_clinical_trial(): è¼‰å…¥å–®ä¸€è‡¨åºŠè©¦é©—çš„å®Œæ•´è³‡æ–™
# - load_dataset(): è¼‰å…¥åŒ…å«å¤šå€‹è©¦é©—çš„è¨“ç·´/æ¸¬è©¦è³‡æ–™é›†ï¼ˆè‡ªå‹•è™•ç†è·¯å¾‘ï¼‰
# - extract_relevant_sections(): æå–è©¦é©—ä¸­ç‰¹å®šå€æ®µçš„è³‡æ–™


# ## æ¨¡å‹é…ç½®
# 
# é…ç½®Google Geminiæ¨¡å‹ï¼š
# 
# > ğŸ¤– **æŠ€è¡“èªªæ˜**: æˆ‘å€‘ä½¿ç”¨Google Gemini 2.5 Flashæ¨¡å‹ï¼Œé€™æ˜¯ä¸€å€‹é«˜æ•ˆèƒ½ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„å¤§å‹èªè¨€æ¨¡å‹ï¼Œç‰¹åˆ¥é©åˆå¤šä»£ç†ç³»çµ±ã€‚

# In[ ]:


# é…ç½® Google Gemini æ¨¡å‹
# è¨­ç½® API é‡‘é‘°å’Œæ¨¡å‹åƒæ•¸

# æ”¯æ´å¤šç¨®ç’°å¢ƒè®Šæ•¸åç¨±
api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")

if not api_key:
    print("âš ï¸ è«‹è¨­å®š GEMINI_API_KEY æˆ– GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸")
    print("å¯ä»¥åœ¨ Colab å·¦å´é¢æ¿çš„ 'Secrets' ä¸­è¨­å®šï¼Œæˆ–ä½¿ç”¨ä»¥ä¸‹æ–¹å¼:")
    print("import os")
    print("os.environ['GEMINI_API_KEY'] = 'æ‚¨çš„APIé‡‘é‘°'")
    print("æˆ–")
    print("os.environ['GOOGLE_API_KEY'] = 'æ‚¨çš„APIé‡‘é‘°'")
    raise ValueError("ç¼ºå°‘ API é‡‘é‘°")
else:
    print(f"âœ… æ‰¾åˆ° API é‡‘é‘°: {api_key[:8]}...{api_key[-4:]}")

genai.configure(api_key=api_key)

# æ¸¬è©¦ API é€£æ¥
try:
    test_model = genai.GenerativeModel("gemini-2.5-flash")
    test_response = test_model.generate_content("Hello, respond with 'API test successful'")
    print(f"âœ… API é€£æ¥æ¸¬è©¦æˆåŠŸ: {test_response.text[:50]}...")
except Exception as e:
    print(f"âŒ API é€£æ¥æ¸¬è©¦å¤±æ•—: {e}")
    raise

# å‰µå»º Gemini æ¨¡å‹å¯¦ä¾‹
model = genai.GenerativeModel(
    model_name="gemini-2.5-flash",
    generation_config=genai.types.GenerationConfig(
        temperature=0.1,  # ä½æº«åº¦ç¢ºä¿ä¸€è‡´çš„çµæœ
        max_output_tokens=4096,
        top_p=1,
        top_k=1
    )
)

# æ¨¡å‹é…ç½®
MODEL_NAME = "gemini-2.5-flash"

print(f"âœ… Google Geminiæ¨¡å‹é…ç½®å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {MODEL_NAME}")

# ğŸ’¡ æ¨¡å‹èªªæ˜ï¼šGemini 2.5 Flashæ˜¯Googleæœ€æ–°çš„é«˜æ•ˆèƒ½æ¨¡å‹ï¼Œæä¾›å„ªç§€çš„æ¨ç†èƒ½åŠ›å’Œå¿«é€Ÿå›æ‡‰


# ## ä»£ç†å®šç¾©
# 
# ç¾åœ¨è®“æˆ‘å€‘ä½¿ç”¨Atomic Agentsæ¡†æ¶å®šç¾©æ¯å€‹å°ˆé–€çš„ä»£ç†ã€‚æ¯å€‹ä»£ç†éƒ½æœ‰ç‰¹å®šçš„è§’è‰²å’Œå°ˆæ¥­é ˜åŸŸï¼š
# 
# > ğŸ¯ **è¨­è¨ˆåŸå‰‡**: æ¯å€‹ä»£ç†éƒ½å°ˆæ³¨æ–¼ç‰¹å®šé ˜åŸŸçš„åˆ†æï¼Œé€éåˆ†å·¥åˆä½œä¾†æé«˜æ•´é«”åˆ†æå“è³ªã€‚é€™æ˜¯å¤šä»£ç†ç³»çµ±çš„æ ¸å¿ƒå„ªå‹¢ã€‚

# In[ ]:


# 1. é†«ç™‚å°ˆå®¶ä»£ç†
# é€™å€‹ä»£ç†å°ˆé–€è² è²¬å¾é†«å­¸è§’åº¦åˆ†æé™³è¿°çš„æº–ç¢ºæ€§

# å‰µå»ºé©ç”¨æ–¼Geminiçš„åˆ†æå‡½æ•¸
def analyze_with_gemini(prompt: str, context: str) -> str:
    """ä½¿ç”¨Geminiæ¨¡å‹é€²è¡Œåˆ†æ"""
    full_prompt = f"{prompt}\n\nContext:\n{context}"
    try:
        response = model.generate_content(full_prompt)
        return response.text
    except Exception as e:
        return f"Error: {str(e)}"

def medical_expert_analysis(context: str) -> str:
    """é†«ç™‚å°ˆå®¶ä»£ç†åˆ†æå‡½æ•¸"""
    prompt = """You are a Medical Expert Agent specializing in clinical trial analysis.
You have deep knowledge of medical terminology, clinical concepts, and trial procedures.
Your role is to analyze statements from a medical perspective and identify relevant clinical insights.

Steps:
1. Analyze the medical terminology and concepts in the statement
2. Identify key clinical elements and their significance
3. Review the clinical trial data for medical accuracy
4. Assess whether the medical claims align with the trial evidence
5. Provide medical reasoning and clinical context

Provide a clear medical analysis focusing on:
- Medical terminology accuracy
- Clinical relevance and significance
- Alignment with medical evidence in the trial
- Any medical concerns or considerations

End with: MEDICAL_ASSESSMENT: [SUPPORTS/CONTRADICTS/UNCLEAR] based on medical evidence"""
    
    return analyze_with_gemini(prompt, context)

print("âœ… é†«ç™‚å°ˆå®¶ä»£ç†å»ºç«‹å®Œæˆ")

# ğŸ©º ä»£ç†èªªæ˜ï¼šé†«ç™‚å°ˆå®¶ä»£ç†å°ˆé–€åˆ†æé†«å­¸è¡“èªçš„æº–ç¢ºæ€§å’Œè‡¨åºŠç›¸é—œæ€§


# In[ ]:


# 2. æ•¸å€¼åˆ†æä»£ç†
def numerical_analyzer_analysis(context: str) -> str:
    """æ•¸å€¼åˆ†æä»£ç†åˆ†æå‡½æ•¸"""
    prompt = """You are a Numerical Analyzer Agent specializing in quantitative analysis of clinical trials.
You excel at processing numbers, statistics, percentages, and numerical relationships.
Your role is to verify numerical claims and perform statistical analysis.

Steps:
1. Extract all numerical values, percentages, and statistics from the statement
2. Identify corresponding numbers in the clinical trial data
3. Perform calculations to verify numerical relationships
4. Check for statistical significance and clinical meaningfulness
5. Identify any numerical inconsistencies or errors

Provide a detailed numerical analysis including:
- All numerical values extracted from the statement
- Corresponding values found in trial data
- Calculations performed to verify claims
- Assessment of numerical accuracy

End with: NUMERICAL_ASSESSMENT: [ACCURATE/INACCURATE/PARTIALLY_ACCURATE] with confidence level"""
    
    return analyze_with_gemini(prompt, context)

print("âœ… æ•¸å€¼åˆ†æä»£ç†å»ºç«‹å®Œæˆ")


# In[ ]:


# 3. é‚è¼¯æª¢æŸ¥ä»£ç†
def logic_checker_analysis(context: str) -> str:
    """é‚è¼¯æª¢æŸ¥ä»£ç†åˆ†æå‡½æ•¸"""
    prompt = """You are a Logic Checker Agent responsible for validating logical reasoning and consistency.
You specialize in identifying logical relationships, contradictions, and reasoning patterns.
Your role is to ensure logical soundness and coherence in claims and evidence.

Steps:
1. Analyze the logical structure of the statement
2. Identify cause-and-effect relationships and implications
3. Check for internal consistency and coherence
4. Evaluate the validity of inferences and conclusions
5. Detect any logical fallacies or contradictions

Provide a logical analysis focusing on:
- Logical structure and reasoning patterns
- Consistency between claims and evidence
- Validity of inferences and implications
- Any logical issues or contradictions found

End with: LOGICAL_ASSESSMENT: [VALID/INVALID/QUESTIONABLE] with reasoning"""
    
    return analyze_with_gemini(prompt, context)

print("âœ… é‚è¼¯æª¢æŸ¥ä»£ç†å»ºç«‹å®Œæˆ")


# In[ ]:


# 4. èšåˆä»£ç†
def aggregator_analysis(medical_analysis: str, numerical_analysis: str, logic_analysis: str, statement: str) -> str:
    """èšåˆä»£ç†åˆ†æå‡½æ•¸"""
    prompt = f"""You are the Aggregator Agent responsible for making final entailment decisions.
You synthesize analyses from the Medical Expert, Numerical Analyzer, and Logic Checker.
Your role is to weigh different evidence types and make the final classification decision.

ORIGINAL STATEMENT: "{statement}"

MEDICAL EXPERT ANALYSIS:
{medical_analysis}

NUMERICAL ANALYZER ANALYSIS:
{numerical_analysis}

LOGIC CHECKER ANALYSIS:
{logic_analysis}

Steps:
1. Review the medical assessment for clinical accuracy
2. Consider the numerical analysis for quantitative validity
3. Evaluate the logical assessment for reasoning soundness
4. Weigh all evidence types appropriately
5. Make the final Entailment vs Contradiction decision

Based on all specialist analyses, determine if the statement is:
- ENTAILMENT: Statement is directly supported by the trial data
- CONTRADICTION: Statement is refuted by the trial data

Provide brief reasoning and then output exactly one of:
FINAL_DECISION: Entailment
FINAL_DECISION: Contradiction"""
    
    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Error: {str(e)}"

print("âœ… èšåˆä»£ç†å»ºç«‹å®Œæˆ")


# ## å¤šä»£ç†åˆ†æç®¡é“
# 
# ç¾åœ¨è®“æˆ‘å€‘å‰µå»ºéµå¾ªæ•™å­¸åœ–è¡¨æ¶æ§‹ã€å”èª¿æ‰€æœ‰ä»£ç†çš„çµæ§‹åŒ–ç®¡é“ï¼š
# 
# > âš™ï¸ **å·¥ä½œæµç¨‹èªªæ˜**: é€™å€‹ç®¡é“å°‡æŒ‰é †åºåŸ·è¡Œæ¯å€‹å°ˆæ¥­ä»£ç†ï¼Œæœ€çµ‚ç”±èšåˆä»£ç†åšå‡ºæ±ºç­–ã€‚æ¯å€‹æ­¥é©Ÿéƒ½æœƒç”¢ç”Ÿå°ˆæ¥­çš„åˆ†æçµæœã€‚

# In[ ]:


def atomic_agents_pipeline(statement: str, primary_id: str, secondary_id: Optional[str] = None, 
                          section_id: Optional[str] = None, verbose: bool = False) -> str:
    """
    Run the complete Atomic Agents analysis pipeline using Google Gemini.
    
    Args:
        statement: The natural language statement to analyze
        primary_id: Primary clinical trial ID
        secondary_id: Secondary trial ID for comparison statements
        section_id: Relevant section of the trial
        verbose: Whether to print intermediate results
        
    Returns:
        Final decision: 'Entailment' or 'Contradiction'
    """
    
    try:
        # Step 1: Load clinical trial data
        primary_data = load_clinical_trial(primary_id)
        secondary_data = None
        if secondary_id:
            secondary_data = load_clinical_trial(secondary_id)
        
        # Step 2: Extract relevant sections
        primary_sections = extract_relevant_sections(primary_data, section_id or "All")
        secondary_sections = None
        if secondary_data:
            secondary_sections = extract_relevant_sections(secondary_data, section_id or "All")
        
        # Step 3: Prepare input context for agents
        input_context = f"""
STATEMENT TO ANALYZE: "{statement}"

PRIMARY TRIAL ({primary_id}):
{primary_sections}

{f'SECONDARY TRIAL ({secondary_id}):\n{secondary_sections}' if secondary_sections else ''}

TASK: Analyze this statement against the clinical trial evidence.
        """.strip()
        
        if verbose:
            print(f"ğŸ“„ Analyzing: {statement[:100]}...")
            print(f"ğŸ¥ Primary Trial: {primary_id}")
            if secondary_id:
                print(f"ğŸ¥ Secondary Trial: {secondary_id}")
        
        # Step 4: Medical Expert Analysis
        medical_analysis = medical_expert_analysis(input_context)
        
        if verbose:
            print(f"ğŸ©º Medical Expert: {medical_analysis.split('MEDICAL_ASSESSMENT:')[-1].strip() if 'MEDICAL_ASSESSMENT:' in medical_analysis else 'Analysis complete'}")
        
        # Step 5: Numerical Analyzer Analysis
        numerical_analysis = numerical_analyzer_analysis(input_context)
        
        if verbose:
            print(f"ğŸ”¢ Numerical Analyzer: {numerical_analysis.split('NUMERICAL_ASSESSMENT:')[-1].strip() if 'NUMERICAL_ASSESSMENT:' in numerical_analysis else 'Analysis complete'}")
        
        # Step 6: Logic Checker Analysis
        logic_analysis = logic_checker_analysis(input_context)
        
        if verbose:
            print(f"ğŸ§  Logic Checker: {logic_analysis.split('LOGICAL_ASSESSMENT:')[-1].strip() if 'LOGICAL_ASSESSMENT:' in logic_analysis else 'Analysis complete'}")
        
        # Step 7: Aggregator Decision
        final_analysis = aggregator_analysis(medical_analysis, numerical_analysis, logic_analysis, statement)
        
        # Step 8: Extract final decision
        if "FINAL_DECISION: Entailment" in final_analysis:
            decision = "Entailment"
        elif "FINAL_DECISION: Contradiction" in final_analysis:
            decision = "Contradiction"
        else:
            # Fallback parsing
            if "entailment" in final_analysis.lower() and "contradiction" not in final_analysis.lower():
                decision = "Entailment"
            else:
                decision = "Contradiction"
        
        if verbose:
            print(f"âš–ï¸ Final Decision: {decision}")
            print("-" * 50)
        
        return decision
        
    except Exception as e:
        if verbose:
            print(f"âŒ Error in pipeline: {e}")
        return "Contradiction"  # Conservative fallback

print("âœ… Atomic Agents pipeline ready (using Google Gemini)")


# ## æ¸¬è©¦ç¯„ä¾‹
# 
# è®“æˆ‘å€‘æ¸¬è©¦æ”¹é€²çš„Atomic Agentsç³»çµ±ï¼š
# 
# > ğŸ§ª **æ¸¬è©¦èªªæ˜**: é€™å€‹æ¸¬è©¦å°‡å±•ç¤ºå®Œæ•´çš„å¤šä»£ç†åˆ†ææµç¨‹ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°æ¯å€‹ä»£ç†å¦‚ä½•å”ä½œè™•ç†è‡¨åºŠè©¦é©—é™³è¿°ã€‚

# In[ ]:


# Test with a sample statement
test_statement = "there is a 13.2% difference between the results from the two the primary trial cohorts"
test_primary_id = "NCT00066573"

print(f"Testing Atomic Agents with statement:")
print(f"'{test_statement}'")
print(f"Primary trial: {test_primary_id}")
print("\n" + "="*80)

# Run the analysis with verbose output
result = atomic_agents_pipeline(
    statement=test_statement,
    primary_id=test_primary_id,
    section_id="Results",
    verbose=True
)

print(f"\nğŸ¯ ATOMIC AGENTS RESULT: {result}")
print("="*80)


# ## åœ¨è¨“ç·´è³‡æ–™ä¸Šçš„è©•ä¼°
# 
# è®“æˆ‘å€‘åœ¨è¨“ç·´è³‡æ–™ä¸Šè©•ä¼°æ”¹é€²çš„Atomic Agentsç³»çµ±ï¼š
# 
# > ğŸ“Š **è©•ä¼°èªªæ˜**: é€™å€‹éƒ¨åˆ†å°‡æ¸¬è©¦æˆ‘å€‘çš„å¤šä»£ç†ç³»çµ±åœ¨å¯¦éš›è³‡æ–™ä¸Šçš„è¡¨ç¾ï¼Œä¸¦è¨ˆç®—æº–ç¢ºç‡ç­‰é—œéµæŒ‡æ¨™ã€‚

# In[ ]:


# Load training data
train_data = load_dataset("training_data/train.json")
print(f"Loaded {len(train_data)} training examples")

# Evaluate on a sample (adjust sample_size as needed)
sample_size = 30
examples = list(train_data.items())[:sample_size]

print(f"\nEvaluating Atomic Agents on {len(examples)} examples...")

results = []
correct = 0

for i, (uuid, example) in enumerate(tqdm(examples, desc="Atomic Agents")):
    try:
        statement = example.get("Statement")
        primary_id = example.get("Primary_id")
        secondary_id = example.get("Secondary_id")
        section_id = example.get("Section_id")
        expected = example.get("Label")
        
        if not statement or not primary_id:
            results.append({
                "uuid": uuid,
                "expected": expected,
                "predicted": "SKIPPED",
                "correct": False
            })
            continue
        
        # Get prediction from Atomic Agents pipeline
        predicted = atomic_agents_pipeline(
            statement=statement,
            primary_id=primary_id,
            secondary_id=secondary_id,
            section_id=section_id,
            verbose=False
        )
        
        # Check if correct
        is_correct = (predicted.strip() == expected.strip())
        if is_correct:
            correct += 1
            
        results.append({
            "uuid": uuid,
            "statement": statement[:100] + "..." if len(statement) > 100 else statement,
            "expected": expected,
            "predicted": predicted,
            "correct": is_correct
        })
        
        status = "âœ…" if is_correct else "âŒ"
        print(f"Example {i+1:2d}: {expected:12} -> {predicted:12} {status}")
        
    except Exception as e:
        print(f"Error processing example {i+1}: {e}")
        results.append({
            "uuid": uuid,
            "expected": expected,
            "predicted": "ERROR",
            "correct": False
        })

# Calculate accuracy
accuracy = correct / len(examples) if examples else 0
print(f"\nğŸ“Š Atomic Agents Results:")
print(f"Accuracy: {accuracy:.2%} ({correct}/{len(examples)})")

# Store results for later comparison
atomic_agents_results = results.copy()


# ## Error Analysis
# 
# Let's analyze the errors to understand areas for improvement:

# In[ ]:


# Analyze incorrect predictions
incorrect_results = [r for r in results if not r["correct"] and r["predicted"] not in ["SKIPPED", "ERROR"]]

print(f"\nğŸ” Error Analysis ({len(incorrect_results)} incorrect predictions):")
print("=" * 80)

# Group errors by type
entailment_to_contradiction = [r for r in incorrect_results if r["expected"] == "Entailment" and r["predicted"] == "Contradiction"]
contradiction_to_entailment = [r for r in incorrect_results if r["expected"] == "Contradiction" and r["predicted"] == "Entailment"]

print(f"Entailment -> Contradiction errors: {len(entailment_to_contradiction)}")
print(f"Contradiction -> Entailment errors: {len(contradiction_to_entailment)}")

# Show some examples
print("\nSample errors:")
for i, result in enumerate(incorrect_results[:3]):
    print(f"\nError #{i+1}:")
    print(f"Statement: {result['statement']}")
    print(f"Expected: {result['expected']} | Predicted: {result['predicted']}")
    print("-" * 40)


# ## Generate Submission File
# 
# Let's generate a submission file using our Atomic Agents system:

# In[ ]:


def generate_atomic_agents_submission(test_file="test.json", output_file="atomic_agents_submission.json", sample_size=None):
    """
    Generate submission file using Atomic Agents system.
    
    Args:
        test_file: Path to test data
        output_file: Output submission file
        sample_size: Number of examples to process (None for all)
    """
    
    # Load test data
    test_data = load_dataset(test_file)
    if not test_data:
        print(f"âŒ Could not load test data from {test_file}")
        return
    
    examples = list(test_data.items())
    if sample_size:
        examples = examples[:sample_size]
        
    print(f"ğŸš€ Generating Atomic Agents predictions for {len(examples)} examples...")
    
    submission = {}
    
    for i, (uuid, example) in enumerate(tqdm(examples, desc="Atomic Agents Processing")):
        try:
            statement = example.get("Statement")
            primary_id = example.get("Primary_id")
            secondary_id = example.get("Secondary_id")
            section_id = example.get("Section_id")
            
            if not statement or not primary_id:
                submission[uuid] = {"Prediction": "Contradiction"}  # Default fallback
                continue
                
            # Get prediction from Atomic Agents system
            prediction = atomic_agents_pipeline(
                statement=statement,
                primary_id=primary_id,
                secondary_id=secondary_id,
                section_id=section_id,
                verbose=False
            )
            
            submission[uuid] = {"Prediction": prediction}
            
        except Exception as e:
            print(f"Error processing {uuid}: {e}")
            submission[uuid] = {"Prediction": "Contradiction"}  # Conservative fallback
    
    # Save submission file
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(submission, f, indent=2)
    
    print(f"âœ… Atomic Agents submission saved to {output_file}")
    return submission

# Generate submission for a small sample
atomic_submission = generate_atomic_agents_submission(
    test_file="test.json", 
    output_file="atomic_agents_submission.json",
    sample_size=10  # Adjust as needed
)

print(f"Generated predictions for {len(atomic_submission)} examples")


# ## Performance Analysis and Comparison
# 
# Let's analyze the performance of our improved Atomic Agents implementation:

# In[ ]:


import time
from collections import Counter

# Performance metrics
def analyze_performance(results, framework_name):
    """
    Analyze performance metrics for a framework.
    """
    valid_results = [r for r in results if r["predicted"] not in ["SKIPPED", "ERROR"]]
    
    if not valid_results:
        print(f"No valid results for {framework_name}")
        return
    
    # Basic metrics
    total = len(valid_results)
    correct = sum(1 for r in valid_results if r["correct"])
    accuracy = correct / total if total > 0 else 0
    
    # Confusion matrix
    true_pos = sum(1 for r in valid_results if r["expected"] == "Entailment" and r["predicted"] == "Entailment")
    true_neg = sum(1 for r in valid_results if r["expected"] == "Contradiction" and r["predicted"] == "Contradiction")
    false_pos = sum(1 for r in valid_results if r["expected"] == "Contradiction" and r["predicted"] == "Entailment")
    false_neg = sum(1 for r in valid_results if r["expected"] == "Entailment" and r["predicted"] == "Contradiction")
    
    # Calculate metrics
    precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0
    recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    
    print(f"\nğŸ“Š {framework_name} Performance Analysis:")
    print(f"{'='*50}")
    print(f"Total examples: {total}")
    print(f"Accuracy: {accuracy:.2%} ({correct}/{total})")
    print(f"Precision: {precision:.3f}")
    print(f"Recall: {recall:.3f}")
    print(f"F1-Score: {f1_score:.3f}")
    print(f"\nConfusion Matrix:")
    print(f"True Positives (Entailment): {true_pos}")
    print(f"True Negatives (Contradiction): {true_neg}")
    print(f"False Positives: {false_pos}")
    print(f"False Negatives: {false_neg}")
    
    # Label distribution
    expected_dist = Counter(r["expected"] for r in valid_results)
    predicted_dist = Counter(r["predicted"] for r in valid_results)
    
    print(f"\nLabel Distribution:")
    print(f"Expected - Entailment: {expected_dist.get('Entailment', 0)}, Contradiction: {expected_dist.get('Contradiction', 0)}")
    print(f"Predicted - Entailment: {predicted_dist.get('Entailment', 0)}, Contradiction: {predicted_dist.get('Contradiction', 0)}")
    
    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1_score": f1_score
    }

# Analyze current results
atomic_metrics = analyze_performance(atomic_agents_results, "Atomic Agents")


# ## çµè«–èˆ‡æ´å¯Ÿ
# 
# ### Atomic Agents æ¡†æ¶å„ªå‹¢ï¼š
# 1. **è¶…è¼•é‡ç´š**: æœ€å°é–‹éŠ·å’Œæ¥µå¿«å•Ÿå‹•æ™‚é–“ï¼ˆ~3Î¼sï¼‰
# 2. **æ¨¡çµ„åŒ–æ¶æ§‹**: æ˜“æ–¼çµ„åˆå’Œä¿®æ”¹ä»£ç†
# 3. **ç”Ÿç”¢å°±ç·’**: å°ˆç‚ºå¯¦éš›éƒ¨ç½²å ´æ™¯è€Œå»ºæ§‹
# 4. **è¨˜æ†¶é«”ç®¡ç†**: å…§å»ºçš„ä¸Šä¸‹æ–‡å’Œè¨˜æ†¶é«”è™•ç†
# 5. **ç°¡å–®API**: ç›´è§€çš„ä»£ç†å‰µå»ºå’Œå”èª¿
# 6. **æ¨¡å‹å½ˆæ€§**: æ”¯æ´å¤šç¨®LLMå¾Œç«¯ï¼ˆæœ¬ç¤ºä¾‹ä½¿ç”¨Google Gemini 2.5 Flashï¼‰
# 
# ### é—œéµæ”¹é€²ï¼š
# - **çµæ§‹åŒ–ç®¡é“**: ä»£ç†é–“è·è²¬æ¸…æ™°åˆ†å·¥
# - **å°ˆæ¥­è§’è‰²**: é†«ç™‚å°ˆå®¶ã€æ•¸å€¼åˆ†æå“¡ã€é‚è¼¯æª¢æŸ¥å“¡ã€èšåˆå“¡
# - **æ›´å¥½çš„å”èª¿**: ç³»çµ±åŒ–çš„è³‡è¨Šæµå‹•
# - **éŒ¯èª¤è™•ç†**: å¼·å¥çš„å›é€€æ©Ÿåˆ¶
# - **å€æ®µæå–**: é‡å°ç›¸é—œè©¦é©—å€æ®µçš„ç²¾æº–åˆ†æ
# - **Geminiæ•´åˆ**: ä½¿ç”¨Googleæœ€æ–°çš„é«˜æ•ˆèƒ½èªè¨€æ¨¡å‹
# 
# ### æ¶æ§‹å„ªå‹¢ï¼š
# - **å¿«é€ŸåŸ·è¡Œ**: é«˜ååé‡å ´æ™¯çš„æœ€å°é–‹éŠ·
# - **æ˜“æ–¼é™¤éŒ¯**: æ¸…æ™°çš„ä»£ç†é‚Šç•Œä¾¿æ–¼æ’éŒ¯
# - **å¯æ“´å±•è¨­è¨ˆ**: ç°¡å–®æ·»åŠ æ–°çš„å°ˆæ¥­ä»£ç†
# - **è¨˜æ†¶é«”æ•ˆç‡**: è¼•é‡ç´šä»£ç†å¯¦ä¾‹
# - **ç”Ÿç”¢éƒ¨ç½²**: å¯¦éš›æ‡‰ç”¨æº–å‚™å°±ç·’
# - **æ¨¡å‹ç„¡é—œ**: æ˜“æ–¼åˆ‡æ›ä¸åŒçš„LLMæä¾›å•†
# 
# ### å„ªåŒ–æ©Ÿæœƒï¼š
# 1. **æç¤ºå·¥ç¨‹**: å¾®èª¿å€‹åˆ¥ä»£ç†æç¤ºä»¥é©æ‡‰Gemini
# 2. **ä»£ç†å”èª¿**: æ”¹å–„ä»£ç†é–“è³‡è¨Šå‚³é
# 3. **éŒ¯èª¤åˆ†æ**: åˆ©ç”¨å¤±æ•—æ¡ˆä¾‹æ”¹é€²ä»£ç†æ¨ç†
# 4. **æ•ˆèƒ½èª¿æ•´**: é€Ÿåº¦èˆ‡æº–ç¢ºæ€§æ¬Šè¡¡æœ€ä½³åŒ–
# 5. **é ˜åŸŸçŸ¥è­˜**: å¢å¼·ä»£ç†çš„é†«ç™‚å°ˆæ¥­çŸ¥è­˜
# 
# ### ä½•æ™‚ä½¿ç”¨ Atomic Agentsï¼š
# - é«˜æ•ˆèƒ½ç”Ÿç”¢ç’°å¢ƒ
# - éœ€è¦å¿«é€Ÿå•Ÿå‹•å’ŒåŸ·è¡Œçš„æ‡‰ç”¨
# - é‡è¦–ç°¡æ½”æ€§å’Œæ¨¡çµ„åŒ–çš„å ´æ™¯
# - éœ€è¦è¼•é‡ç´šä»£ç†å”èª¿çš„ç³»çµ±
# - å„ªå…ˆè€ƒæ…®éƒ¨ç½²æ•ˆç‡çš„å°ˆæ¡ˆ
# - å¸Œæœ›éˆæ´»åˆ‡æ›LLMæä¾›å•†çš„ç³»çµ±
# 
# ## ğŸ“ å­¸ç¿’é‡é»ç¸½çµ
# - **åŸå­æ€§æ¦‚å¿µ**: æ¯å€‹ä»£ç†éƒ½æ˜¯ç¨ç«‹çš„æœ€å°åŠŸèƒ½å–®å…ƒ
# - **åˆ†å·¥åˆä½œ**: ä¸åŒå°ˆæ¥­é ˜åŸŸçš„ä»£ç†å”åŒå·¥ä½œ
# - **ç®¡é“è¨­è¨ˆ**: çµæ§‹åŒ–çš„è³‡æ–™æµå’Œæ±ºç­–æµç¨‹
# - **å¯¦éš›æ‡‰ç”¨**: è‡¨åºŠè©¦é©—NLPçš„å¯¦æˆ°æ¡ˆä¾‹
# - **æ¨¡å‹æ•´åˆ**: å¦‚ä½•å°‡Atomic Agentsèˆ‡Google Geminiæ•´åˆ
# 
# Atomic Agents åœ¨æ•ˆèƒ½ã€ç°¡æ½”æ€§å’Œæ¨¡çµ„åŒ–ä¹‹é–“æä¾›äº†çµ•ä½³å¹³è¡¡ï¼ŒçµåˆGoogle Gemini 2.5 Flashçš„å¼·å¤§èƒ½åŠ›ï¼Œä½¿å…¶æˆç‚ºéœ€è¦é€Ÿåº¦ã€å¯é æ€§å’Œæˆæœ¬æ•ˆç›Šçš„ç”Ÿç”¢è‡¨åºŠNLPæ‡‰ç”¨çš„ç†æƒ³é¸æ“‡ã€‚
