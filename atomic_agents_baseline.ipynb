{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/0x-yuan/clintrial-nlp/blob/main/atomic_agents_baseline.ipynb)\n",
    "\n",
    "# Atomic Agents æ¡†æ¶åŸºç·š - è‡¨åºŠè©¦é©— NLP\n",
    "\n",
    "## æ¦‚è¿°\n",
    "\n",
    "æœ¬notebookå±•ç¤ºå¦‚ä½•ä½¿ç”¨Atomic Agentsæ¡†æ¶å»ºæ§‹ä¸€å€‹è¼•é‡ç´šã€é«˜æ•ˆèƒ½çš„å¤šä»£ç†ç³»çµ±ï¼Œç”¨æ–¼è‡¨åºŠè©¦é©—è‡ªç„¶èªè¨€æ¨ç†(NLI)ã€‚Atomic Agentså°ˆç‚ºç”Ÿç”¢ç’°å¢ƒè¨­è¨ˆï¼Œå…·æœ‰æ¥µå¿«çš„å•Ÿå‹•æ™‚é–“(~3Î¼s)å’Œæ¨¡çµ„åŒ–æ¶æ§‹ã€‚\n",
    "\n",
    "## ğŸ“š å­¸ç¿’ç›®æ¨™\n",
    "å®Œæˆæœ¬æ•™å­¸å¾Œï¼Œæ‚¨å°‡å­¸æœƒï¼š\n",
    "- ç†è§£ Atomic Agents æ¡†æ¶çš„æ ¸å¿ƒæ¦‚å¿µ\n",
    "- å»ºç«‹å°ˆé–€çš„é†«ç™‚ã€æ•¸å€¼å’Œé‚è¼¯åˆ†æä»£ç†\n",
    "- å¯¦ä½œå¤šä»£ç†å”ä½œpipeline\n",
    "- è©•ä¼°å’Œæ”¹é€²ç³»çµ±æ•ˆèƒ½\n",
    "\n",
    "### ç‚ºä»€éº¼é¸æ“‡ Atomic Agentsï¼Ÿ\n",
    "- **è¶…è¼•é‡ç´š**: æœ€å°åŒ–é–‹éŠ·å’Œå¿«é€ŸåŸ·è¡Œ\n",
    "- **é«˜åº¦æ¨¡çµ„åŒ–**: æ˜“æ–¼çµ„åˆå’Œä¿®æ”¹ä»£ç†\n",
    "- **ç”Ÿç”¢å°±ç·’**: å°ˆç‚ºå¯¦éš›éƒ¨ç½²è€Œè¨­è¨ˆ\n",
    "- **ç°¡å–®API**: ç›´è§€çš„ä»£ç†å‰µå»ºå’Œå”èª¿\n",
    "- **è¨˜æ†¶é«”ç®¡ç†**: å…§å»ºçš„ä¸Šä¸‹æ–‡å’Œè¨˜æ†¶é«”è™•ç†\n",
    "\n",
    "### ğŸ—ï¸ ä»£ç†æ¶æ§‹\n",
    "éµå¾ªæ•™å­¸åœ–è¡¨ï¼Œæˆ‘å€‘å¯¦ä½œä¸€å€‹çµæ§‹åŒ–çš„pipelineï¼š\n",
    "1. **é†«ç™‚å°ˆå®¶ä»£ç†**: åˆ†æé†«å­¸è¡“èªå’Œæ¦‚å¿µ\n",
    "2. **æ•¸å€¼åˆ†æä»£ç†**: è™•ç†é‡åŒ–æ•¸æ“š\n",
    "3. **é‚è¼¯æª¢æŸ¥ä»£ç†**: é©—è­‰é‚è¼¯é—œä¿‚\n",
    "4. **èšåˆä»£ç†**: çµåˆè¦‹è§£åšå‡ºæœ€çµ‚æ±ºç­–\n",
    "5. **ç›£ç£å”èª¿**: ç®¡ç†æ•´é«”å·¥ä½œæµç¨‹\n",
    "\n",
    "> ğŸ’¡ **é‡è¦æ¦‚å¿µ**: Atomic Agents çš„\"atomic\"æŒ‡çš„æ˜¯æ¯å€‹ä»£ç†éƒ½æ˜¯ä¸€å€‹ç¨ç«‹ã€æœ€å°çš„åŠŸèƒ½å–®å…ƒï¼Œå¯ä»¥è¼•é¬†çµ„åˆæˆè¤‡é›œç³»çµ±ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1p3qmrclzei",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ”§ Colab ç’°å¢ƒè¨­ç½® - ä¸€éµå®‰è£æ‰€éœ€å¥—ä»¶\n# é€™å€‹cellæœƒéœé»˜å®‰è£æ‰€æœ‰å¿…è¦çš„Pythonå¥—ä»¶ï¼Œè®“æ‚¨å¯ä»¥åœ¨Colabä¸­ç›´æ¥é‹è¡Œæ­¤notebook\n!pip install -q atomic-agents python-dotenv pandas tqdm\n!pip install -q google-generativeai gdown\n\nprint(\"âœ… æ‰€æœ‰å¥—ä»¶å®‰è£å®Œæˆï¼å¯ä»¥é–‹å§‹ä½¿ç”¨ Atomic Agents æ¡†æ¶äº†\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fpaymxvzg",
   "metadata": {},
   "outputs": [],
   "source": "# ğŸ“¥ å¾ Google Drive ä¸‹è¼‰è¨“ç·´è³‡æ–™\n# é€™å€‹cellæœƒè‡ªå‹•ä¸‹è¼‰ä¸¦è§£å£“ç¸® clinicaltrial-nlp.zipï¼Œç¢ºä¿åœ¨Colabä¸­å¯ä»¥ç›´æ¥é‹è¡Œ\nimport os\nimport gdown\nimport zipfile\nimport shutil\n\n# Google Drive zip æª”æ¡ˆ ID\nfile_id = \"15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR\"\nzip_url = f\"https://drive.google.com/uc?id={file_id}\"\nzip_filename = \"clinicaltrial-nlp.zip\"\n\n# æª¢æŸ¥æ˜¯å¦å·²æœ‰è¨“ç·´è³‡æ–™\nif not os.path.exists(\"training_data\"):\n    print(\"ğŸ“¥ å¾ Google Drive ä¸‹è¼‰ clinicaltrial-nlp.zip...\")\n    print(\"âš ï¸ å¦‚æœä¸‹è¼‰å¤±æ•—ï¼Œè«‹ç¢ºèª:\")\n    print(\"1. Google Drive é€£çµçš„æ¬Šé™è¨­å®šç‚º 'çŸ¥é“é€£çµçš„ä½¿ç”¨è€…'\")\n    print(\"2. ç¶²è·¯é€£ç·šæ­£å¸¸\")\n    print(f\"3. æª”æ¡ˆé€£çµ: {zip_url}\")\n    \n    try:\n        # ä¸‹è¼‰ zip æª”æ¡ˆ\n        print(\"ğŸ“¥ æ­£åœ¨ä¸‹è¼‰ zip æª”æ¡ˆ...\")\n        gdown.download(zip_url, zip_filename, quiet=False)\n        \n        # è§£å£“ç¸®æª”æ¡ˆ\n        print(\"ğŸ“¦ æ­£åœ¨è§£å£“ç¸®æª”æ¡ˆ...\")\n        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n            zip_ref.extractall(\".\")\n        \n        # ç§»å‹• training_data åˆ°æ­£ç¢ºä½ç½®ï¼ˆå¦‚æœåœ¨å­è³‡æ–™å¤¾ä¸­ï¼‰\n        if os.path.exists(\"clintrial-nlp/training_data\") and not os.path.exists(\"training_data\"):\n            print(\"ğŸ“ ç§»å‹• training_data åˆ°æ­£ç¢ºä½ç½®...\")\n            shutil.move(\"clintrial-nlp/training_data\", \"training_data\")\n            # æ¸…ç†è§£å£“ç¸®çš„è³‡æ–™å¤¾\n            if os.path.exists(\"clintrial-nlp\"):\n                shutil.rmtree(\"clintrial-nlp\")\n            if os.path.exists(\"__MACOSX\"):  # æ¸…ç† macOS ç”¢ç”Ÿçš„éš±è—æª”æ¡ˆ\n                shutil.rmtree(\"__MACOSX\")\n        \n        # æ¸…ç† zip æª”æ¡ˆ\n        os.remove(zip_filename)\n        print(\"âœ… è¨“ç·´è³‡æ–™ä¸‹è¼‰ä¸¦è§£å£“ç¸®å®Œæˆï¼\")\n        \n    except Exception as e:\n        print(f\"âŒ ä¸‹è¼‰å¤±æ•—: {e}\")\n        print(\"\\nğŸ”§ æ‰‹å‹•è§£æ±ºæ–¹æ¡ˆ:\")\n        print(\"1. é»æ“Šæ­¤é€£çµä¸‹è¼‰ zip æª”æ¡ˆ:\")\n        print(\"   https://drive.google.com/file/d/15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR/view?usp=sharing\")\n        print(\"2. ä¸Šå‚³ zip æª”æ¡ˆåˆ° Colab\")\n        print(\"3. è§£å£“ç¸®å¾Œé‡æ–°åŸ·è¡Œå¾ŒçºŒçš„ cells\")\n        \n        # å‰µå»ºä¸€å€‹æç¤ºæª”æ¡ˆ\n        os.makedirs(\"training_data\", exist_ok=True)\n        with open(\"training_data/DOWNLOAD_INSTRUCTIONS.txt\", \"w\", encoding=\"utf-8\") as f:\n            f.write(\"è«‹æ‰‹å‹•ä¸‹è¼‰ä¸¦è§£å£“ç¸® clinicaltrial-nlp.zip:\\n\")\n            f.write(\"https://drive.google.com/file/d/15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR/view?usp=sharing\\n\")\n        \n        print(\"\\nğŸ“ å·²å‰µå»ºä¸‹è¼‰æŒ‡ç¤ºæª”æ¡ˆæ–¼ training_data/DOWNLOAD_INSTRUCTIONS.txt\")\nelse:\n    print(\"âœ… è¨“ç·´è³‡æ–™å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰\")\n\n# æª¢æŸ¥ä¸‹è¼‰çš„è³‡æ–™çµæ§‹\nif os.path.exists(\"training_data\"):\n    contents = os.listdir(\"training_data\")\n    print(f\"ğŸ“‚ è³‡æ–™å¤¾å…§å®¹: {contents}\")\n    if os.path.exists(\"training_data/CT json\"):\n        ct_files = len([f for f in os.listdir(\"training_data/CT json\") if f.endswith('.json')])\n        print(f\"ğŸ“„ æ‰¾åˆ° {ct_files} å€‹è‡¨åºŠè©¦é©—JSONæª”æ¡ˆ\")\n    else:\n        print(\"âš ï¸ æ‰¾ä¸åˆ° 'CT json' å­è³‡æ–™å¤¾ï¼Œè«‹æª¢æŸ¥ä¸‹è¼‰æ˜¯å¦å®Œæ•´\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l9m7hu5aaui",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª æº–å‚™æ¸¬è©¦è³‡æ–™é›†\n",
    "# å¦‚æœæ²’æœ‰ test.jsonï¼Œå¾è¨“ç·´è³‡æ–™å‰µå»ºä¸€å€‹æ¸¬è©¦é›†\n",
    "import json\n",
    "\n",
    "def create_test_data_if_needed():\n",
    "    \"\"\"å¦‚æœä¸å­˜åœ¨ test.jsonï¼Œå¾ train.json å‰µå»ºä¸€å€‹å°çš„æ¸¬è©¦é›†\"\"\"\n",
    "    if not os.path.exists(\"test.json\"):\n",
    "        try:\n",
    "            # è¼‰å…¥è¨“ç·´è³‡æ–™\n",
    "            with open(\"training_data/train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                train_data = json.load(f)\n",
    "            \n",
    "            # å–å‰100å€‹æ¨£æœ¬ä½œç‚ºæ¸¬è©¦è³‡æ–™\n",
    "            test_data = dict(list(train_data.items())[:100])\n",
    "            \n",
    "            # å„²å­˜æ¸¬è©¦è³‡æ–™\n",
    "            with open(\"test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(test_data, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(f\"âœ… å·²å‰µå»ºæ¸¬è©¦è³‡æ–™é›†ï¼ŒåŒ…å« {len(test_data)} å€‹æ¨£æœ¬\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å‰µå»ºæ¸¬è©¦è³‡æ–™å¤±æ•—: {e}\")\n",
    "    else:\n",
    "        print(\"âœ… test.json å·²å­˜åœ¨\")\n",
    "\n",
    "# åŸ·è¡Œæ¸¬è©¦è³‡æ–™æº–å‚™\n",
    "create_test_data_if_needed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## ç’°å¢ƒè¨­ç½®å’Œå®‰è£\n",
    "\n",
    "é¦–å…ˆï¼Œè®“æˆ‘å€‘è¨­ç½®ç’°å¢ƒä¸¦åŒ¯å…¥å¿…è¦çš„å‡½å¼åº«ï¼š\n",
    "\n",
    "> ğŸ“ **èªªæ˜**: åœ¨é€™å€‹æ­¥é©Ÿä¸­ï¼Œæˆ‘å€‘æœƒè¼‰å…¥ç’°å¢ƒè®Šæ•¸ä¸¦ç¢ºèªæ‰€æœ‰å¿…è¦çš„å¥—ä»¶éƒ½å·²æ­£ç¢ºå®‰è£ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ç’°å¢ƒè®Šæ•¸\n",
    "# é€™å€‹æ­¥é©Ÿæœƒå¾ .env æª”æ¡ˆè¼‰å…¥ API é‡‘é‘°ç­‰æ•æ„Ÿè³‡è¨Š\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(\"âœ… ç’°å¢ƒè®Šæ•¸è¼‰å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependencies",
   "metadata": {},
   "outputs": [],
   "source": "# åŒ¯å…¥å¿…è¦çš„å‡½å¼åº«\n# é€™äº›å‡½å¼åº«æä¾›äº†è³‡æ–™è™•ç†ã€AIæ¨¡å‹å’ŒAtomic Agentsæ¡†æ¶çš„åŠŸèƒ½\nimport json\nimport pandas as pd\nfrom tqdm import tqdm  # é€²åº¦æ¢é¡¯ç¤º\nimport google.generativeai as genai  # Google Gemini API\nfrom typing import Dict, List, Any, Optional\nimport warnings\nwarnings.filterwarnings('ignore')  # éš±è—è­¦å‘Šè¨Šæ¯\n\n# Atomic Agents æ ¸å¿ƒçµ„ä»¶\nfrom atomic_agents.lib.components.agent_memory import AgentMemory  # ä»£ç†è¨˜æ†¶é«”ç®¡ç†\nfrom atomic_agents.agents.base_agent import BaseAgent, BaseAgentConfig, BaseAgentInputSchema  # åŸºç¤ä»£ç†é¡åˆ¥\nfrom atomic_agents.lib.components.system_prompt_generator import SystemPromptGenerator  # ç³»çµ±æç¤ºç”Ÿæˆå™¨\n\nprint(\"âœ… æ‰€æœ‰å‡½å¼åº«åŒ¯å…¥æˆåŠŸ\")"
  },
  {
   "cell_type": "markdown",
   "id": "data_utils",
   "metadata": {},
   "source": [
    "## è³‡æ–™è¼‰å…¥å’Œå·¥å…·å‡½å¼\n",
    "\n",
    "è®“æˆ‘å€‘å»ºç«‹ç”¨æ–¼è¼‰å…¥å’Œè™•ç†è‡¨åºŠè©¦é©—è³‡æ–™çš„å·¥å…·å‡½å¼ï¼š\n",
    "\n",
    "> ğŸ”§ **åŠŸèƒ½èªªæ˜**: é€™äº›å‡½å¼è² è²¬å¾JSONæª”æ¡ˆä¸­è¼‰å…¥è‡¨åºŠè©¦é©—è³‡æ–™ï¼Œä¸¦å°‡å…¶è½‰æ›ç‚ºé©åˆAIä»£ç†åˆ†æçš„æ ¼å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clinical_trial(trial_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"è¼‰å…¥è‡¨åºŠè©¦é©—è³‡æ–™å¾JSONæª”æ¡ˆã€‚\n",
    "    \n",
    "    Args:\n",
    "        trial_id: è‡¨åºŠè©¦é©—çš„NCTè­˜åˆ¥ç¢¼\n",
    "        \n",
    "    Returns:\n",
    "        åŒ…å«è©¦é©—è³‡æ–™çš„å­—å…¸æˆ–éŒ¯èª¤è³‡è¨Š\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ç¢ºä¿ä½¿ç”¨ä¸‹è¼‰çš„è³‡æ–™å¤¾è·¯å¾‘\n",
    "        file_path = os.path.join(\"training_data\", \"CT json\", f\"{trial_id}.json\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        return {\"error\": f\"æ‰¾ä¸åˆ°è‡¨åºŠè©¦é©— {trial_id}\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"è¼‰å…¥ {trial_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\"}\n",
    "\n",
    "def load_dataset(filepath: str) -> Dict[str, Any]:\n",
    "    \"\"\"è¼‰å…¥è¨“ç·´æˆ–æ¸¬è©¦è³‡æ–™é›†ã€‚\n",
    "    \n",
    "    Args:\n",
    "        filepath: JSONè³‡æ–™é›†æª”æ¡ˆçš„è·¯å¾‘ï¼ˆæœƒè‡ªå‹•æª¢æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ  training_data/ å‰ç¶´ï¼‰\n",
    "        \n",
    "    Returns:\n",
    "        åŒ…å«è³‡æ–™é›†çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # å¦‚æœè·¯å¾‘ä¸æ˜¯çµ•å°è·¯å¾‘ä¸”ä¸åŒ…å« training_dataï¼Œè‡ªå‹•æ·»åŠ å‰ç¶´\n",
    "        if not os.path.isabs(filepath) and not filepath.startswith(\"training_data/\"):\n",
    "            # å°æ–¼æ¨™æº–æª”æ¡ˆåï¼Œä½¿ç”¨ training_data å‰ç¶´\n",
    "            if filepath in [\"train.json\", \"dev.json\"]:\n",
    "                filepath = os.path.join(\"training_data\", filepath)\n",
    "        \n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"è¼‰å…¥è³‡æ–™é›†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        return {}\n",
    "\n",
    "def extract_relevant_sections(trial_data: Dict[str, Any], section_id: str) -> str:\n",
    "    \"\"\"æ ¹æ“šsection_idå¾è©¦é©—è³‡æ–™ä¸­æå–ç›¸é—œéƒ¨åˆ†ã€‚\n",
    "    \n",
    "    Args:\n",
    "        trial_data: è‡¨åºŠè©¦é©—è³‡æ–™å­—å…¸\n",
    "        section_id: ç›®æ¨™å€æ®µ (Eligibility, Intervention, Results, Adverse Events)\n",
    "        \n",
    "    Returns:\n",
    "        åŒ…å«ç›¸é—œå€æ®µè³‡æ–™çš„æ ¼å¼åŒ–å­—ä¸²\n",
    "    \"\"\"\n",
    "    if \"error\" in trial_data:\n",
    "        return f\"éŒ¯èª¤: {trial_data['error']}\"\n",
    "    \n",
    "    sections = {\n",
    "        \"Eligibility\": trial_data.get(\"Eligibility\", []),\n",
    "        \"Intervention\": trial_data.get(\"Intervention\", []),\n",
    "        \"Results\": trial_data.get(\"Results\", []),\n",
    "        \"Adverse Events\": trial_data.get(\"Adverse_Events\", [])\n",
    "    }\n",
    "    \n",
    "    # å¦‚æœè«‹æ±‚ç‰¹å®šå€æ®µï¼Œå‰‡åƒ…è¿”å›è©²å€æ®µ\n",
    "    if section_id in sections:\n",
    "        section_data = sections[section_id]\n",
    "        if isinstance(section_data, list):\n",
    "            return \"\\n\".join(str(item) for item in section_data)\n",
    "        return str(section_data)\n",
    "    \n",
    "    # å¦å‰‡è¿”å›æ‰€æœ‰å€æ®µ\n",
    "    result = []\n",
    "    for section_name, section_data in sections.items():\n",
    "        if section_data:\n",
    "            result.append(f\"{section_name}:\")\n",
    "            if isinstance(section_data, list):\n",
    "                result.extend([f\"  {item}\" for item in section_data])\n",
    "            else:\n",
    "                result.append(f\"  {section_data}\")\n",
    "    \n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "# æ¸¬è©¦å·¥å…·å‡½å¼\n",
    "sample_trial = load_clinical_trial(\"NCT00066573\")\n",
    "print(f\"âœ… è³‡æ–™å·¥å…·å‡½å¼æº–å‚™å°±ç·’ã€‚ç¯„ä¾‹è©¦é©—: {sample_trial.get('Clinical Trial ID', 'éŒ¯èª¤')}\")\n",
    "\n",
    "# ğŸ“‹ å‡½å¼èªªæ˜ï¼š\n",
    "# - load_clinical_trial(): è¼‰å…¥å–®ä¸€è‡¨åºŠè©¦é©—çš„å®Œæ•´è³‡æ–™\n",
    "# - load_dataset(): è¼‰å…¥åŒ…å«å¤šå€‹è©¦é©—çš„è¨“ç·´/æ¸¬è©¦è³‡æ–™é›†ï¼ˆè‡ªå‹•è™•ç†è·¯å¾‘ï¼‰\n",
    "# - extract_relevant_sections(): æå–è©¦é©—ä¸­ç‰¹å®šå€æ®µçš„è³‡æ–™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_setup",
   "metadata": {},
   "source": "## æ¨¡å‹é…ç½®\n\né…ç½®Google Geminiæ¨¡å‹ï¼š\n\n> ğŸ¤– **æŠ€è¡“èªªæ˜**: æˆ‘å€‘ä½¿ç”¨Google Gemini 2.5 Flashæ¨¡å‹ï¼Œé€™æ˜¯ä¸€å€‹é«˜æ•ˆèƒ½ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„å¤§å‹èªè¨€æ¨¡å‹ï¼Œç‰¹åˆ¥é©åˆå¤šä»£ç†ç³»çµ±ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_config",
   "metadata": {},
   "outputs": [],
   "source": "# é…ç½® Google Gemini æ¨¡å‹\n# è¨­ç½® API é‡‘é‘°å’Œæ¨¡å‹åƒæ•¸\ngenai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n\n# å‰µå»º Gemini æ¨¡å‹å¯¦ä¾‹\nmodel = genai.GenerativeModel(\n    model_name=\"gemini-2.5-flash\",\n    generation_config=genai.types.GenerationConfig(\n        temperature=0.1,  # ä½æº«åº¦ç¢ºä¿ä¸€è‡´çš„çµæœ\n        max_output_tokens=4096,\n        top_p=1,\n        top_k=1\n    )\n)\n\n# æ¨¡å‹é…ç½®\nMODEL_NAME = \"gemini-2.5-flash\"\n\nprint(f\"âœ… Google Geminiæ¨¡å‹é…ç½®å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {MODEL_NAME}\")\n\n# ğŸ’¡ æ¨¡å‹èªªæ˜ï¼šGemini 2.5 Flashæ˜¯Googleæœ€æ–°çš„é«˜æ•ˆèƒ½æ¨¡å‹ï¼Œæä¾›å„ªç§€çš„æ¨ç†èƒ½åŠ›å’Œå¿«é€Ÿå›æ‡‰"
  },
  {
   "cell_type": "markdown",
   "id": "agent_definitions",
   "metadata": {},
   "source": [
    "## ä»£ç†å®šç¾©\n",
    "\n",
    "ç¾åœ¨è®“æˆ‘å€‘ä½¿ç”¨Atomic Agentsæ¡†æ¶å®šç¾©æ¯å€‹å°ˆé–€çš„ä»£ç†ã€‚æ¯å€‹ä»£ç†éƒ½æœ‰ç‰¹å®šçš„è§’è‰²å’Œå°ˆæ¥­é ˜åŸŸï¼š\n",
    "\n",
    "> ğŸ¯ **è¨­è¨ˆåŸå‰‡**: æ¯å€‹ä»£ç†éƒ½å°ˆæ³¨æ–¼ç‰¹å®šé ˜åŸŸçš„åˆ†æï¼Œé€éåˆ†å·¥åˆä½œä¾†æé«˜æ•´é«”åˆ†æå“è³ªã€‚é€™æ˜¯å¤šä»£ç†ç³»çµ±çš„æ ¸å¿ƒå„ªå‹¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical_expert",
   "metadata": {},
   "outputs": [],
   "source": "# 1. é†«ç™‚å°ˆå®¶ä»£ç†\n# é€™å€‹ä»£ç†å°ˆé–€è² è²¬å¾é†«å­¸è§’åº¦åˆ†æé™³è¿°çš„æº–ç¢ºæ€§\n\n# å‰µå»ºé©ç”¨æ–¼Geminiçš„åˆ†æå‡½æ•¸\ndef analyze_with_gemini(prompt: str, context: str) -> str:\n    \"\"\"ä½¿ç”¨Geminiæ¨¡å‹é€²è¡Œåˆ†æ\"\"\"\n    full_prompt = f\"{prompt}\\n\\nContext:\\n{context}\"\n    try:\n        response = model.generate_content(full_prompt)\n        return response.text\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\ndef medical_expert_analysis(context: str) -> str:\n    \"\"\"é†«ç™‚å°ˆå®¶ä»£ç†åˆ†æå‡½æ•¸\"\"\"\n    prompt = \"\"\"You are a Medical Expert Agent specializing in clinical trial analysis.\nYou have deep knowledge of medical terminology, clinical concepts, and trial procedures.\nYour role is to analyze statements from a medical perspective and identify relevant clinical insights.\n\nSteps:\n1. Analyze the medical terminology and concepts in the statement\n2. Identify key clinical elements and their significance\n3. Review the clinical trial data for medical accuracy\n4. Assess whether the medical claims align with the trial evidence\n5. Provide medical reasoning and clinical context\n\nProvide a clear medical analysis focusing on:\n- Medical terminology accuracy\n- Clinical relevance and significance\n- Alignment with medical evidence in the trial\n- Any medical concerns or considerations\n\nEnd with: MEDICAL_ASSESSMENT: [SUPPORTS/CONTRADICTS/UNCLEAR] based on medical evidence\"\"\"\n    \n    return analyze_with_gemini(prompt, context)\n\nprint(\"âœ… é†«ç™‚å°ˆå®¶ä»£ç†å»ºç«‹å®Œæˆ\")\n\n# ğŸ©º ä»£ç†èªªæ˜ï¼šé†«ç™‚å°ˆå®¶ä»£ç†å°ˆé–€åˆ†æé†«å­¸è¡“èªçš„æº–ç¢ºæ€§å’Œè‡¨åºŠç›¸é—œæ€§"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical_analyzer",
   "metadata": {},
   "outputs": [],
   "source": "# 2. æ•¸å€¼åˆ†æä»£ç†\ndef numerical_analyzer_analysis(context: str) -> str:\n    \"\"\"æ•¸å€¼åˆ†æä»£ç†åˆ†æå‡½æ•¸\"\"\"\n    prompt = \"\"\"You are a Numerical Analyzer Agent specializing in quantitative analysis of clinical trials.\nYou excel at processing numbers, statistics, percentages, and numerical relationships.\nYour role is to verify numerical claims and perform statistical analysis.\n\nSteps:\n1. Extract all numerical values, percentages, and statistics from the statement\n2. Identify corresponding numbers in the clinical trial data\n3. Perform calculations to verify numerical relationships\n4. Check for statistical significance and clinical meaningfulness\n5. Identify any numerical inconsistencies or errors\n\nProvide a detailed numerical analysis including:\n- All numerical values extracted from the statement\n- Corresponding values found in trial data\n- Calculations performed to verify claims\n- Assessment of numerical accuracy\n\nEnd with: NUMERICAL_ASSESSMENT: [ACCURATE/INACCURATE/PARTIALLY_ACCURATE] with confidence level\"\"\"\n    \n    return analyze_with_gemini(prompt, context)\n\nprint(\"âœ… æ•¸å€¼åˆ†æä»£ç†å»ºç«‹å®Œæˆ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logic_checker",
   "metadata": {},
   "outputs": [],
   "source": "# 3. é‚è¼¯æª¢æŸ¥ä»£ç†\ndef logic_checker_analysis(context: str) -> str:\n    \"\"\"é‚è¼¯æª¢æŸ¥ä»£ç†åˆ†æå‡½æ•¸\"\"\"\n    prompt = \"\"\"You are a Logic Checker Agent responsible for validating logical reasoning and consistency.\nYou specialize in identifying logical relationships, contradictions, and reasoning patterns.\nYour role is to ensure logical soundness and coherence in claims and evidence.\n\nSteps:\n1. Analyze the logical structure of the statement\n2. Identify cause-and-effect relationships and implications\n3. Check for internal consistency and coherence\n4. Evaluate the validity of inferences and conclusions\n5. Detect any logical fallacies or contradictions\n\nProvide a logical analysis focusing on:\n- Logical structure and reasoning patterns\n- Consistency between claims and evidence\n- Validity of inferences and implications\n- Any logical issues or contradictions found\n\nEnd with: LOGICAL_ASSESSMENT: [VALID/INVALID/QUESTIONABLE] with reasoning\"\"\"\n    \n    return analyze_with_gemini(prompt, context)\n\nprint(\"âœ… é‚è¼¯æª¢æŸ¥ä»£ç†å»ºç«‹å®Œæˆ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregator",
   "metadata": {},
   "outputs": [],
   "source": "# 4. èšåˆä»£ç†\ndef aggregator_analysis(medical_analysis: str, numerical_analysis: str, logic_analysis: str, statement: str) -> str:\n    \"\"\"èšåˆä»£ç†åˆ†æå‡½æ•¸\"\"\"\n    prompt = f\"\"\"You are the Aggregator Agent responsible for making final entailment decisions.\nYou synthesize analyses from the Medical Expert, Numerical Analyzer, and Logic Checker.\nYour role is to weigh different evidence types and make the final classification decision.\n\nORIGINAL STATEMENT: \"{statement}\"\n\nMEDICAL EXPERT ANALYSIS:\n{medical_analysis}\n\nNUMERICAL ANALYZER ANALYSIS:\n{numerical_analysis}\n\nLOGIC CHECKER ANALYSIS:\n{logic_analysis}\n\nSteps:\n1. Review the medical assessment for clinical accuracy\n2. Consider the numerical analysis for quantitative validity\n3. Evaluate the logical assessment for reasoning soundness\n4. Weigh all evidence types appropriately\n5. Make the final Entailment vs Contradiction decision\n\nBased on all specialist analyses, determine if the statement is:\n- ENTAILMENT: Statement is directly supported by the trial data\n- CONTRADICTION: Statement is refuted by the trial data\n\nProvide brief reasoning and then output exactly one of:\nFINAL_DECISION: Entailment\nFINAL_DECISION: Contradiction\"\"\"\n    \n    try:\n        response = model.generate_content(prompt)\n        return response.text\n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\nprint(\"âœ… èšåˆä»£ç†å»ºç«‹å®Œæˆ\")"
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": [
    "## å¤šä»£ç†åˆ†æç®¡é“\n",
    "\n",
    "ç¾åœ¨è®“æˆ‘å€‘å‰µå»ºéµå¾ªæ•™å­¸åœ–è¡¨æ¶æ§‹ã€å”èª¿æ‰€æœ‰ä»£ç†çš„çµæ§‹åŒ–ç®¡é“ï¼š\n",
    "\n",
    "> âš™ï¸ **å·¥ä½œæµç¨‹èªªæ˜**: é€™å€‹ç®¡é“å°‡æŒ‰é †åºåŸ·è¡Œæ¯å€‹å°ˆæ¥­ä»£ç†ï¼Œæœ€çµ‚ç”±èšåˆä»£ç†åšå‡ºæ±ºç­–ã€‚æ¯å€‹æ­¥é©Ÿéƒ½æœƒç”¢ç”Ÿå°ˆæ¥­çš„åˆ†æçµæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_pipeline",
   "metadata": {},
   "outputs": [],
   "source": "def atomic_agents_pipeline(statement: str, primary_id: str, secondary_id: Optional[str] = None, \n                          section_id: Optional[str] = None, verbose: bool = False) -> str:\n    \"\"\"\n    Run the complete Atomic Agents analysis pipeline using Google Gemini.\n    \n    Args:\n        statement: The natural language statement to analyze\n        primary_id: Primary clinical trial ID\n        secondary_id: Secondary trial ID for comparison statements\n        section_id: Relevant section of the trial\n        verbose: Whether to print intermediate results\n        \n    Returns:\n        Final decision: 'Entailment' or 'Contradiction'\n    \"\"\"\n    \n    try:\n        # Step 1: Load clinical trial data\n        primary_data = load_clinical_trial(primary_id)\n        secondary_data = None\n        if secondary_id:\n            secondary_data = load_clinical_trial(secondary_id)\n        \n        # Step 2: Extract relevant sections\n        primary_sections = extract_relevant_sections(primary_data, section_id or \"All\")\n        secondary_sections = None\n        if secondary_data:\n            secondary_sections = extract_relevant_sections(secondary_data, section_id or \"All\")\n        \n        # Step 3: Prepare input context for agents\n        input_context = f\"\"\"\nSTATEMENT TO ANALYZE: \"{statement}\"\n\nPRIMARY TRIAL ({primary_id}):\n{primary_sections}\n\n{f'SECONDARY TRIAL ({secondary_id}):\\n{secondary_sections}' if secondary_sections else ''}\n\nTASK: Analyze this statement against the clinical trial evidence.\n        \"\"\".strip()\n        \n        if verbose:\n            print(f\"ğŸ“„ Analyzing: {statement[:100]}...\")\n            print(f\"ğŸ¥ Primary Trial: {primary_id}\")\n            if secondary_id:\n                print(f\"ğŸ¥ Secondary Trial: {secondary_id}\")\n        \n        # Step 4: Medical Expert Analysis\n        medical_analysis = medical_expert_analysis(input_context)\n        \n        if verbose:\n            print(f\"ğŸ©º Medical Expert: {medical_analysis.split('MEDICAL_ASSESSMENT:')[-1].strip() if 'MEDICAL_ASSESSMENT:' in medical_analysis else 'Analysis complete'}\")\n        \n        # Step 5: Numerical Analyzer Analysis\n        numerical_analysis = numerical_analyzer_analysis(input_context)\n        \n        if verbose:\n            print(f\"ğŸ”¢ Numerical Analyzer: {numerical_analysis.split('NUMERICAL_ASSESSMENT:')[-1].strip() if 'NUMERICAL_ASSESSMENT:' in numerical_analysis else 'Analysis complete'}\")\n        \n        # Step 6: Logic Checker Analysis\n        logic_analysis = logic_checker_analysis(input_context)\n        \n        if verbose:\n            print(f\"ğŸ§  Logic Checker: {logic_analysis.split('LOGICAL_ASSESSMENT:')[-1].strip() if 'LOGICAL_ASSESSMENT:' in logic_analysis else 'Analysis complete'}\")\n        \n        # Step 7: Aggregator Decision\n        final_analysis = aggregator_analysis(medical_analysis, numerical_analysis, logic_analysis, statement)\n        \n        # Step 8: Extract final decision\n        if \"FINAL_DECISION: Entailment\" in final_analysis:\n            decision = \"Entailment\"\n        elif \"FINAL_DECISION: Contradiction\" in final_analysis:\n            decision = \"Contradiction\"\n        else:\n            # Fallback parsing\n            if \"entailment\" in final_analysis.lower() and \"contradiction\" not in final_analysis.lower():\n                decision = \"Entailment\"\n            else:\n                decision = \"Contradiction\"\n        \n        if verbose:\n            print(f\"âš–ï¸ Final Decision: {decision}\")\n            print(\"-\" * 50)\n        \n        return decision\n        \n    except Exception as e:\n        if verbose:\n            print(f\"âŒ Error in pipeline: {e}\")\n        return \"Contradiction\"  # Conservative fallback\n\nprint(\"âœ… Atomic Agents pipeline ready (using Google Gemini)\")"
  },
  {
   "cell_type": "markdown",
   "id": "test_example",
   "metadata": {},
   "source": [
    "## æ¸¬è©¦ç¯„ä¾‹\n",
    "\n",
    "è®“æˆ‘å€‘æ¸¬è©¦æ”¹é€²çš„Atomic Agentsç³»çµ±ï¼š\n",
    "\n",
    "> ğŸ§ª **æ¸¬è©¦èªªæ˜**: é€™å€‹æ¸¬è©¦å°‡å±•ç¤ºå®Œæ•´çš„å¤šä»£ç†åˆ†ææµç¨‹ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°æ¯å€‹ä»£ç†å¦‚ä½•å”ä½œè™•ç†è‡¨åºŠè©¦é©—é™³è¿°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample statement\n",
    "test_statement = \"there is a 13.2% difference between the results from the two the primary trial cohorts\"\n",
    "test_primary_id = \"NCT00066573\"\n",
    "\n",
    "print(f\"Testing Atomic Agents with statement:\")\n",
    "print(f\"'{test_statement}'\")\n",
    "print(f\"Primary trial: {test_primary_id}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Run the analysis with verbose output\n",
    "result = atomic_agents_pipeline(\n",
    "    statement=test_statement,\n",
    "    primary_id=test_primary_id,\n",
    "    section_id=\"Results\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ¯ ATOMIC AGENTS RESULT: {result}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation",
   "metadata": {},
   "source": [
    "## åœ¨è¨“ç·´è³‡æ–™ä¸Šçš„è©•ä¼°\n",
    "\n",
    "è®“æˆ‘å€‘åœ¨è¨“ç·´è³‡æ–™ä¸Šè©•ä¼°æ”¹é€²çš„Atomic Agentsç³»çµ±ï¼š\n",
    "\n",
    "> ğŸ“Š **è©•ä¼°èªªæ˜**: é€™å€‹éƒ¨åˆ†å°‡æ¸¬è©¦æˆ‘å€‘çš„å¤šä»£ç†ç³»çµ±åœ¨å¯¦éš›è³‡æ–™ä¸Šçš„è¡¨ç¾ï¼Œä¸¦è¨ˆç®—æº–ç¢ºç‡ç­‰é—œéµæŒ‡æ¨™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_data = load_dataset(\"training_data/train.json\")\n",
    "print(f\"Loaded {len(train_data)} training examples\")\n",
    "\n",
    "# Evaluate on a sample (adjust sample_size as needed)\n",
    "sample_size = 30\n",
    "examples = list(train_data.items())[:sample_size]\n",
    "\n",
    "print(f\"\\nEvaluating Atomic Agents on {len(examples)} examples...\")\n",
    "\n",
    "results = []\n",
    "correct = 0\n",
    "\n",
    "for i, (uuid, example) in enumerate(tqdm(examples, desc=\"Atomic Agents\")):\n",
    "    try:\n",
    "        statement = example.get(\"Statement\")\n",
    "        primary_id = example.get(\"Primary_id\")\n",
    "        secondary_id = example.get(\"Secondary_id\")\n",
    "        section_id = example.get(\"Section_id\")\n",
    "        expected = example.get(\"Label\")\n",
    "        \n",
    "        if not statement or not primary_id:\n",
    "            results.append({\n",
    "                \"uuid\": uuid,\n",
    "                \"expected\": expected,\n",
    "                \"predicted\": \"SKIPPED\",\n",
    "                \"correct\": False\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Get prediction from Atomic Agents pipeline\n",
    "        predicted = atomic_agents_pipeline(\n",
    "            statement=statement,\n",
    "            primary_id=primary_id,\n",
    "            secondary_id=secondary_id,\n",
    "            section_id=section_id,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Check if correct\n",
    "        is_correct = (predicted.strip() == expected.strip())\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            \n",
    "        results.append({\n",
    "            \"uuid\": uuid,\n",
    "            \"statement\": statement[:100] + \"...\" if len(statement) > 100 else statement,\n",
    "            \"expected\": expected,\n",
    "            \"predicted\": predicted,\n",
    "            \"correct\": is_correct\n",
    "        })\n",
    "        \n",
    "        status = \"âœ…\" if is_correct else \"âŒ\"\n",
    "        print(f\"Example {i+1:2d}: {expected:12} -> {predicted:12} {status}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example {i+1}: {e}\")\n",
    "        results.append({\n",
    "            \"uuid\": uuid,\n",
    "            \"expected\": expected,\n",
    "            \"predicted\": \"ERROR\",\n",
    "            \"correct\": False\n",
    "        })\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / len(examples) if examples else 0\n",
    "print(f\"\\nğŸ“Š Atomic Agents Results:\")\n",
    "print(f\"Accuracy: {accuracy:.2%} ({correct}/{len(examples)})\")\n",
    "\n",
    "# Store results for later comparison\n",
    "atomic_agents_results = results.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error_analysis",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Let's analyze the errors to understand areas for improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze_errors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze incorrect predictions\n",
    "incorrect_results = [r for r in results if not r[\"correct\"] and r[\"predicted\"] not in [\"SKIPPED\", \"ERROR\"]]\n",
    "\n",
    "print(f\"\\nğŸ” Error Analysis ({len(incorrect_results)} incorrect predictions):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Group errors by type\n",
    "entailment_to_contradiction = [r for r in incorrect_results if r[\"expected\"] == \"Entailment\" and r[\"predicted\"] == \"Contradiction\"]\n",
    "contradiction_to_entailment = [r for r in incorrect_results if r[\"expected\"] == \"Contradiction\" and r[\"predicted\"] == \"Entailment\"]\n",
    "\n",
    "print(f\"Entailment -> Contradiction errors: {len(entailment_to_contradiction)}\")\n",
    "print(f\"Contradiction -> Entailment errors: {len(contradiction_to_entailment)}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSample errors:\")\n",
    "for i, result in enumerate(incorrect_results[:3]):\n",
    "    print(f\"\\nError #{i+1}:\")\n",
    "    print(f\"Statement: {result['statement']}\")\n",
    "    print(f\"Expected: {result['expected']} | Predicted: {result['predicted']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "submission",
   "metadata": {},
   "source": [
    "## Generate Submission File\n",
    "\n",
    "Let's generate a submission file using our Atomic Agents system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_atomic_agents_submission(test_file=\"test.json\", output_file=\"atomic_agents_submission.json\", sample_size=None):\n",
    "    \"\"\"\n",
    "    Generate submission file using Atomic Agents system.\n",
    "    \n",
    "    Args:\n",
    "        test_file: Path to test data\n",
    "        output_file: Output submission file\n",
    "        sample_size: Number of examples to process (None for all)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = load_dataset(test_file)\n",
    "    if not test_data:\n",
    "        print(f\"âŒ Could not load test data from {test_file}\")\n",
    "        return\n",
    "    \n",
    "    examples = list(test_data.items())\n",
    "    if sample_size:\n",
    "        examples = examples[:sample_size]\n",
    "        \n",
    "    print(f\"ğŸš€ Generating Atomic Agents predictions for {len(examples)} examples...\")\n",
    "    \n",
    "    submission = {}\n",
    "    \n",
    "    for i, (uuid, example) in enumerate(tqdm(examples, desc=\"Atomic Agents Processing\")):\n",
    "        try:\n",
    "            statement = example.get(\"Statement\")\n",
    "            primary_id = example.get(\"Primary_id\")\n",
    "            secondary_id = example.get(\"Secondary_id\")\n",
    "            section_id = example.get(\"Section_id\")\n",
    "            \n",
    "            if not statement or not primary_id:\n",
    "                submission[uuid] = {\"Prediction\": \"Contradiction\"}  # Default fallback\n",
    "                continue\n",
    "                \n",
    "            # Get prediction from Atomic Agents system\n",
    "            prediction = atomic_agents_pipeline(\n",
    "                statement=statement,\n",
    "                primary_id=primary_id,\n",
    "                secondary_id=secondary_id,\n",
    "                section_id=section_id,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            submission[uuid] = {\"Prediction\": prediction}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {uuid}: {e}\")\n",
    "            submission[uuid] = {\"Prediction\": \"Contradiction\"}  # Conservative fallback\n",
    "    \n",
    "    # Save submission file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(submission, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Atomic Agents submission saved to {output_file}\")\n",
    "    return submission\n",
    "\n",
    "# Generate submission for a small sample\n",
    "atomic_submission = generate_atomic_agents_submission(\n",
    "    test_file=\"test.json\", \n",
    "    output_file=\"atomic_agents_submission.json\",\n",
    "    sample_size=10  # Adjust as needed\n",
    ")\n",
    "\n",
    "print(f\"Generated predictions for {len(atomic_submission)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance_analysis",
   "metadata": {},
   "source": [
    "## Performance Analysis and Comparison\n",
    "\n",
    "Let's analyze the performance of our improved Atomic Agents implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Performance metrics\n",
    "def analyze_performance(results, framework_name):\n",
    "    \"\"\"\n",
    "    Analyze performance metrics for a framework.\n",
    "    \"\"\"\n",
    "    valid_results = [r for r in results if r[\"predicted\"] not in [\"SKIPPED\", \"ERROR\"]]\n",
    "    \n",
    "    if not valid_results:\n",
    "        print(f\"No valid results for {framework_name}\")\n",
    "        return\n",
    "    \n",
    "    # Basic metrics\n",
    "    total = len(valid_results)\n",
    "    correct = sum(1 for r in valid_results if r[\"correct\"])\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    # Confusion matrix\n",
    "    true_pos = sum(1 for r in valid_results if r[\"expected\"] == \"Entailment\" and r[\"predicted\"] == \"Entailment\")\n",
    "    true_neg = sum(1 for r in valid_results if r[\"expected\"] == \"Contradiction\" and r[\"predicted\"] == \"Contradiction\")\n",
    "    false_pos = sum(1 for r in valid_results if r[\"expected\"] == \"Contradiction\" and r[\"predicted\"] == \"Entailment\")\n",
    "    false_neg = sum(1 for r in valid_results if r[\"expected\"] == \"Entailment\" and r[\"predicted\"] == \"Contradiction\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
    "    recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nğŸ“Š {framework_name} Performance Analysis:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total examples: {total}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1-Score: {f1_score:.3f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Positives (Entailment): {true_pos}\")\n",
    "    print(f\"True Negatives (Contradiction): {true_neg}\")\n",
    "    print(f\"False Positives: {false_pos}\")\n",
    "    print(f\"False Negatives: {false_neg}\")\n",
    "    \n",
    "    # Label distribution\n",
    "    expected_dist = Counter(r[\"expected\"] for r in valid_results)\n",
    "    predicted_dist = Counter(r[\"predicted\"] for r in valid_results)\n",
    "    \n",
    "    print(f\"\\nLabel Distribution:\")\n",
    "    print(f\"Expected - Entailment: {expected_dist.get('Entailment', 0)}, Contradiction: {expected_dist.get('Contradiction', 0)}\")\n",
    "    print(f\"Predicted - Entailment: {predicted_dist.get('Entailment', 0)}, Contradiction: {predicted_dist.get('Contradiction', 0)}\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score\n",
    "    }\n",
    "\n",
    "# Analyze current results\n",
    "atomic_metrics = analyze_performance(atomic_agents_results, \"Atomic Agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": "## çµè«–èˆ‡æ´å¯Ÿ\n\n### Atomic Agents æ¡†æ¶å„ªå‹¢ï¼š\n1. **è¶…è¼•é‡ç´š**: æœ€å°é–‹éŠ·å’Œæ¥µå¿«å•Ÿå‹•æ™‚é–“ï¼ˆ~3Î¼sï¼‰\n2. **æ¨¡çµ„åŒ–æ¶æ§‹**: æ˜“æ–¼çµ„åˆå’Œä¿®æ”¹ä»£ç†\n3. **ç”Ÿç”¢å°±ç·’**: å°ˆç‚ºå¯¦éš›éƒ¨ç½²å ´æ™¯è€Œå»ºæ§‹\n4. **è¨˜æ†¶é«”ç®¡ç†**: å…§å»ºçš„ä¸Šä¸‹æ–‡å’Œè¨˜æ†¶é«”è™•ç†\n5. **ç°¡å–®API**: ç›´è§€çš„ä»£ç†å‰µå»ºå’Œå”èª¿\n6. **æ¨¡å‹å½ˆæ€§**: æ”¯æ´å¤šç¨®LLMå¾Œç«¯ï¼ˆæœ¬ç¤ºä¾‹ä½¿ç”¨Google Gemini 2.5 Flashï¼‰\n\n### é—œéµæ”¹é€²ï¼š\n- **çµæ§‹åŒ–ç®¡é“**: ä»£ç†é–“è·è²¬æ¸…æ™°åˆ†å·¥\n- **å°ˆæ¥­è§’è‰²**: é†«ç™‚å°ˆå®¶ã€æ•¸å€¼åˆ†æå“¡ã€é‚è¼¯æª¢æŸ¥å“¡ã€èšåˆå“¡\n- **æ›´å¥½çš„å”èª¿**: ç³»çµ±åŒ–çš„è³‡è¨Šæµå‹•\n- **éŒ¯èª¤è™•ç†**: å¼·å¥çš„å›é€€æ©Ÿåˆ¶\n- **å€æ®µæå–**: é‡å°ç›¸é—œè©¦é©—å€æ®µçš„ç²¾æº–åˆ†æ\n- **Geminiæ•´åˆ**: ä½¿ç”¨Googleæœ€æ–°çš„é«˜æ•ˆèƒ½èªè¨€æ¨¡å‹\n\n### æ¶æ§‹å„ªå‹¢ï¼š\n- **å¿«é€ŸåŸ·è¡Œ**: é«˜ååé‡å ´æ™¯çš„æœ€å°é–‹éŠ·\n- **æ˜“æ–¼é™¤éŒ¯**: æ¸…æ™°çš„ä»£ç†é‚Šç•Œä¾¿æ–¼æ’éŒ¯\n- **å¯æ“´å±•è¨­è¨ˆ**: ç°¡å–®æ·»åŠ æ–°çš„å°ˆæ¥­ä»£ç†\n- **è¨˜æ†¶é«”æ•ˆç‡**: è¼•é‡ç´šä»£ç†å¯¦ä¾‹\n- **ç”Ÿç”¢éƒ¨ç½²**: å¯¦éš›æ‡‰ç”¨æº–å‚™å°±ç·’\n- **æ¨¡å‹ç„¡é—œ**: æ˜“æ–¼åˆ‡æ›ä¸åŒçš„LLMæä¾›å•†\n\n### å„ªåŒ–æ©Ÿæœƒï¼š\n1. **æç¤ºå·¥ç¨‹**: å¾®èª¿å€‹åˆ¥ä»£ç†æç¤ºä»¥é©æ‡‰Gemini\n2. **ä»£ç†å”èª¿**: æ”¹å–„ä»£ç†é–“è³‡è¨Šå‚³é\n3. **éŒ¯èª¤åˆ†æ**: åˆ©ç”¨å¤±æ•—æ¡ˆä¾‹æ”¹é€²ä»£ç†æ¨ç†\n4. **æ•ˆèƒ½èª¿æ•´**: é€Ÿåº¦èˆ‡æº–ç¢ºæ€§æ¬Šè¡¡æœ€ä½³åŒ–\n5. **é ˜åŸŸçŸ¥è­˜**: å¢å¼·ä»£ç†çš„é†«ç™‚å°ˆæ¥­çŸ¥è­˜\n\n### ä½•æ™‚ä½¿ç”¨ Atomic Agentsï¼š\n- é«˜æ•ˆèƒ½ç”Ÿç”¢ç’°å¢ƒ\n- éœ€è¦å¿«é€Ÿå•Ÿå‹•å’ŒåŸ·è¡Œçš„æ‡‰ç”¨\n- é‡è¦–ç°¡æ½”æ€§å’Œæ¨¡çµ„åŒ–çš„å ´æ™¯\n- éœ€è¦è¼•é‡ç´šä»£ç†å”èª¿çš„ç³»çµ±\n- å„ªå…ˆè€ƒæ…®éƒ¨ç½²æ•ˆç‡çš„å°ˆæ¡ˆ\n- å¸Œæœ›éˆæ´»åˆ‡æ›LLMæä¾›å•†çš„ç³»çµ±\n\n## ğŸ“ å­¸ç¿’é‡é»ç¸½çµ\n- **åŸå­æ€§æ¦‚å¿µ**: æ¯å€‹ä»£ç†éƒ½æ˜¯ç¨ç«‹çš„æœ€å°åŠŸèƒ½å–®å…ƒ\n- **åˆ†å·¥åˆä½œ**: ä¸åŒå°ˆæ¥­é ˜åŸŸçš„ä»£ç†å”åŒå·¥ä½œ\n- **ç®¡é“è¨­è¨ˆ**: çµæ§‹åŒ–çš„è³‡æ–™æµå’Œæ±ºç­–æµç¨‹\n- **å¯¦éš›æ‡‰ç”¨**: è‡¨åºŠè©¦é©—NLPçš„å¯¦æˆ°æ¡ˆä¾‹\n- **æ¨¡å‹æ•´åˆ**: å¦‚ä½•å°‡Atomic Agentsèˆ‡Google Geminiæ•´åˆ\n\nAtomic Agents åœ¨æ•ˆèƒ½ã€ç°¡æ½”æ€§å’Œæ¨¡çµ„åŒ–ä¹‹é–“æä¾›äº†çµ•ä½³å¹³è¡¡ï¼ŒçµåˆGoogle Gemini 2.5 Flashçš„å¼·å¤§èƒ½åŠ›ï¼Œä½¿å…¶æˆç‚ºéœ€è¦é€Ÿåº¦ã€å¯é æ€§å’Œæˆæœ¬æ•ˆç›Šçš„ç”Ÿç”¢è‡¨åºŠNLPæ‡‰ç”¨çš„ç†æƒ³é¸æ“‡ã€‚"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}