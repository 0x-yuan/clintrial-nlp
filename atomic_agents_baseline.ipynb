{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/0x-yuan/clintrial-nlp/blob/main/atomic_agents_baseline.ipynb)\n",
    "\n",
    "# Atomic Agents æ¡†æ¶åŸºç·š - è‡¨åºŠè©¦é©— NLP\n",
    "\n",
    "## æ¦‚è¿°\n",
    "\n",
    "æœ¬notebookå±•ç¤ºå¦‚ä½•ä½¿ç”¨ç°¡å–®çš„å¤šä»£ç†æ€è€ƒæ¨¡å¼é€²è¡Œè‡¨åºŠè©¦é©—è‡ªç„¶èªè¨€æ¨ç†(NLI)ã€‚æˆ‘å€‘æ¨¡æ“¬ã€ŒåŸå­ä»£ç†ã€æ¦‚å¿µï¼Œå…¶ä¸­æ¯å€‹ä»£ç†éƒ½æ˜¯ä¸€å€‹å°ˆé–€çš„æ€è€ƒåŠŸèƒ½ï¼Œæœ€çµ‚å”ä½œåšå‡ºæ±ºç­–ã€‚\n",
    "\n",
    "## ğŸ“š å­¸ç¿’ç›®æ¨™\n",
    "- ç†è§£å¤šä»£ç†æ€è€ƒçš„åŸºæœ¬æ¦‚å¿µ\n",
    "- å»ºç«‹å°ˆé–€çš„åˆ†æä»£ç†æ€è€ƒæ¨¡å¼\n",
    "- å¯¦ä½œç°¡å–®ä½†æœ‰æ•ˆçš„å”ä½œæ¨ç†\n",
    "- è©•ä¼°ç³»çµ±æ•ˆèƒ½\n",
    "\n",
    "### ğŸ—ï¸ ä»£ç†æ€è€ƒæ¶æ§‹\n",
    "æˆ‘å€‘å¯¦ä½œ4å€‹å°ˆé–€çš„æ€è€ƒä»£ç†ï¼š\n",
    "1. **é†«ç™‚å°ˆå®¶æ€è€ƒ**: åˆ†æé†«å­¸æ¦‚å¿µå’Œè‡¨åºŠæ„ç¾©\n",
    "2. **æ•¸å€¼åˆ†ææ€è€ƒ**: è™•ç†çµ±è¨ˆæ•¸æ“šå’Œè¨ˆç®—\n",
    "3. **é‚è¼¯æª¢æŸ¥æ€è€ƒ**: é©—è­‰æ¨ç†é‚è¼¯\n",
    "4. **æ±ºç­–ç¶œåˆæ€è€ƒ**: æ•´åˆæ‰€æœ‰åˆ†æåšå‡ºæœ€çµ‚åˆ¤æ–·\n",
    "\n",
    "> ğŸ’¡ **æ ¸å¿ƒæ¦‚å¿µ**: \"åŸå­\"æŒ‡æ¯å€‹ä»£ç†å°ˆæ³¨æ–¼å–®ä¸€ã€æ˜ç¢ºçš„æ€è€ƒä»»å‹™ï¼Œæœ€çµ‚çµ„åˆæˆå®Œæ•´çš„åˆ†æç³»çµ±ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ ç’°å¢ƒè¨­ç½® - ä¸€éµå®‰è£æ‰€éœ€å¥—ä»¶\n",
    "!pip install -q google-generativeai python-dotenv pandas tqdm gdown\n",
    "\n",
    "print(\"âœ… æ‰€æœ‰å¥—ä»¶å®‰è£å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¥ å¾ Google Drive ä¸‹è¼‰è¨“ç·´è³‡æ–™\n",
    "import os\n",
    "import gdown\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Google Drive zip æª”æ¡ˆ ID\n",
    "file_id = \"15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR\"\n",
    "zip_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "zip_filename = \"clinicaltrial-nlp.zip\"\n",
    "\n",
    "if not os.path.exists(\"training_data\"):\n",
    "    print(\"ğŸ“¥ å¾ Google Drive ä¸‹è¼‰ clinicaltrial-nlp.zip...\")\n",
    "    try:\n",
    "        gdown.download(zip_url, zip_filename, quiet=False)\n",
    "        \n",
    "        print(\"ğŸ“¦ æ­£åœ¨è§£å£“ç¸®æª”æ¡ˆ...\")\n",
    "        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall(\".\")\n",
    "        \n",
    "        if os.path.exists(\"clintrial-nlp/training_data\"):\n",
    "            shutil.move(\"clintrial-nlp/training_data\", \"training_data\")\n",
    "            if os.path.exists(\"clintrial-nlp\"):\n",
    "                shutil.rmtree(\"clintrial-nlp\")\n",
    "        \n",
    "        os.remove(zip_filename)\n",
    "        print(\"âœ… è¨“ç·´è³‡æ–™ä¸‹è¼‰ä¸¦è§£å£“ç¸®å®Œæˆï¼\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¸‹è¼‰å¤±æ•—: {e}\")\n",
    "        print(\"è«‹æ‰‹å‹•ä¸‹è¼‰: https://drive.google.com/file/d/15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR/view\")\n",
    "else:\n",
    "    print(\"âœ… è¨“ç·´è³‡æ–™å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰\")\n",
    "\n",
    "# æª¢æŸ¥ä¸‹è¼‰çš„è³‡æ–™\n",
    "if os.path.exists(\"training_data/CT json\"):\n",
    "    ct_files = len([f for f in os.listdir(\"training_data/CT json\") if f.endswith('.json')])\n",
    "    print(f\"ğŸ“„ æ‰¾åˆ° {ct_files} å€‹è‡¨åºŠè©¦é©—JSONæª”æ¡ˆ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª æº–å‚™æ¸¬è©¦è³‡æ–™é›†\n",
    "import json\n",
    "\n",
    "def create_test_data_if_needed():\n",
    "    if not os.path.exists(\"test.json\"):\n",
    "        try:\n",
    "            with open(\"training_data/train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                train_data = json.load(f)\n",
    "            test_data = dict(list(train_data.items())[:100])\n",
    "            with open(\"test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(test_data, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"âœ… å·²å‰µå»ºæ¸¬è©¦è³‡æ–™é›†ï¼ŒåŒ…å« {len(test_data)} å€‹æ¨£æœ¬\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ å‰µå»ºæ¸¬è©¦è³‡æ–™å¤±æ•—: {e}\")\n",
    "    else:\n",
    "        print(\"âœ… test.json å·²å­˜åœ¨\")\n",
    "\n",
    "create_test_data_if_needed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥ç’°å¢ƒè®Šæ•¸å’Œå¿…è¦å‡½å¼åº«\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv()\n",
    "print(\"âœ… ç’°å¢ƒè®Šæ•¸è¼‰å…¥å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨¡å‹é…ç½®\n",
    "\n",
    "é…ç½®Google Geminiæ¨¡å‹é€²è¡Œæ¨ç†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½® Google Gemini æ¨¡å‹\n",
    "from google.colab import userdata\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\") or userdata.get(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"âš ï¸ è«‹è¨­å®š GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸\")\n",
    "    print(\"å¯ä»¥åœ¨ Colab å·¦å´é¢æ¿çš„ 'Secrets' ä¸­è¨­å®š\")\n",
    "    raise ValueError(\"ç¼ºå°‘ API é‡‘é‘°\")\n",
    "else:\n",
    "    print(f\"âœ… æ‰¾åˆ° API é‡‘é‘°: {api_key[:8]}...{api_key[-4:]}\")\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# æ¸¬è©¦ API é€£æ¥\n",
    "try:\n",
    "    test_model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "    test_response = test_model.generate_content(\"Hello, respond with 'API test successful'\")\n",
    "    print(f\"âœ… API é€£æ¥æ¸¬è©¦æˆåŠŸ: {test_response.text[:50]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ API é€£æ¥æ¸¬è©¦å¤±æ•—: {e}\")\n",
    "    raise\n",
    "\n",
    "# å‰µå»º Gemini æ¨¡å‹å¯¦ä¾‹\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-2.5-flash\",\n",
    "    generation_config=genai.types.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        max_output_tokens=4096,\n",
    "        top_p=1,\n",
    "        top_k=1\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"âœ… Google Geminiæ¨¡å‹é…ç½®å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è³‡æ–™å·¥å…·å‡½å¼\n",
    "\n",
    "å»ºç«‹ç”¨æ–¼è¼‰å…¥å’Œè™•ç†è‡¨åºŠè©¦é©—è³‡æ–™çš„å·¥å…·å‡½å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clinical_trial(trial_id: str) -> dict:\n",
    "    \"\"\"è¼‰å…¥è‡¨åºŠè©¦é©—è³‡æ–™\"\"\"\n",
    "    try:\n",
    "        file_path = os.path.join(\"training_data\", \"CT json\", f\"{trial_id}.json\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        return {\"error\": f\"æ‰¾ä¸åˆ°è‡¨åºŠè©¦é©— {trial_id}\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"è¼‰å…¥ {trial_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\"}\n",
    "\n",
    "def extract_trial_section(trial_data: dict, section_id: str = None) -> str:\n",
    "    \"\"\"æå–è©¦é©—ç›¸é—œå€æ®µ\"\"\"\n",
    "    if \"error\" in trial_data:\n",
    "        return f\"éŒ¯èª¤: {trial_data['error']}\"\n",
    "    \n",
    "    sections = {\n",
    "        \"Eligibility\": trial_data.get(\"Eligibility\", []),\n",
    "        \"Intervention\": trial_data.get(\"Intervention\", []),\n",
    "        \"Results\": trial_data.get(\"Results\", []),\n",
    "        \"Adverse Events\": trial_data.get(\"Adverse_Events\", [])\n",
    "    }\n",
    "    \n",
    "    if section_id and section_id in sections:\n",
    "        section_data = sections[section_id]\n",
    "        if isinstance(section_data, list):\n",
    "            return \"\\n\".join(str(item) for item in section_data)\n",
    "        return str(section_data)\n",
    "    \n",
    "    # è¿”å›æ‰€æœ‰å€æ®µ\n",
    "    result = []\n",
    "    for section_name, section_data in sections.items():\n",
    "        if section_data:\n",
    "            result.append(f\"{section_name}:\")\n",
    "            if isinstance(section_data, list):\n",
    "                for item in section_data[:3]:  # é™åˆ¶é•·åº¦\n",
    "                    result.append(f\"  {item}\")\n",
    "                if len(section_data) > 3:\n",
    "                    result.append(f\"  ... ({len(section_data)-3} more items)\")\n",
    "            else:\n",
    "                result.append(f\"  {section_data}\")\n",
    "    \n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "# æ¸¬è©¦å·¥å…·å‡½å¼\n",
    "sample_trial = load_clinical_trial(\"NCT00066573\")\n",
    "print(f\"âœ… è³‡æ–™å·¥å…·å‡½å¼æº–å‚™å°±ç·’ã€‚ç¯„ä¾‹è©¦é©—: {sample_trial.get('Clinical Trial ID', 'éŒ¯èª¤')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŸå­ä»£ç†æ€è€ƒæ¨¡å¼\n",
    "\n",
    "å®šç¾©å››å€‹å°ˆé–€çš„æ€è€ƒä»£ç†ï¼Œæ¯å€‹éƒ½æœ‰ç‰¹å®šçš„åˆ†æè§’è‰²ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medical_expert_thinking(statement: str, trial_evidence: str) -> str:\n",
    "    \"\"\"é†«ç™‚å°ˆå®¶æ€è€ƒä»£ç†\"\"\"\n",
    "    prompt = f\"\"\"ä½ æ˜¯ä¸€ä½é†«ç™‚å°ˆå®¶ï¼Œå°ˆç²¾è‡¨åºŠè©¦é©—åˆ†æã€‚ä½ çš„ä»»å‹™æ˜¯å¾é†«å­¸è§’åº¦åˆ†æä»¥ä¸‹é™³è¿°çš„æº–ç¢ºæ€§ã€‚\n",
    "\n",
    "åˆ†æé‡é»ï¼š\n",
    "- é†«å­¸è¡“èªçš„æº–ç¢ºæ€§\n",
    "- è‡¨åºŠç›¸é—œæ€§å’Œæ„ç¾©\n",
    "- é†«å­¸æ¦‚å¿µçš„åˆç†æ€§\n",
    "- èˆ‡è©¦é©—è­‰æ“šçš„é†«å­¸ä¸€è‡´æ€§\n",
    "\n",
    "é™³è¿°: \"{statement}\"\n",
    "\n",
    "è©¦é©—è­‰æ“š:\n",
    "{trial_evidence}\n",
    "\n",
    "è«‹æä¾›ä½ çš„é†«å­¸åˆ†æï¼Œæœ€å¾Œä»¥ã€Œé†«å­¸è©•ä¼°: [æ”¯æŒ/åé§/ä¸æ˜ç¢º]ã€çµå°¾ã€‚\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"é†«å­¸åˆ†æéŒ¯èª¤: {str(e)}\"\n",
    "\n",
    "def numerical_analyst_thinking(statement: str, trial_evidence: str) -> str:\n",
    "    \"\"\"æ•¸å€¼åˆ†ææ€è€ƒä»£ç†\"\"\"\n",
    "    prompt = f\"\"\"ä½ æ˜¯ä¸€ä½æ•¸å€¼åˆ†æå°ˆå®¶ï¼Œå°ˆç²¾çµ±è¨ˆæ•¸æ“šå’Œè¨ˆç®—é©—è­‰ã€‚ä½ çš„ä»»å‹™æ˜¯åˆ†æé™³è¿°ä¸­çš„æ•¸å€¼æº–ç¢ºæ€§ã€‚\n",
    "\n",
    "åˆ†æé‡é»ï¼š\n",
    "- æå–æ‰€æœ‰æ•¸å€¼ã€ç™¾åˆ†æ¯”ã€çµ±è¨ˆæ•¸æ“š\n",
    "- é©—è­‰è¨ˆç®—å’Œæ•¸å€¼é—œä¿‚\n",
    "- æª¢æŸ¥çµ±è¨ˆçš„åˆç†æ€§\n",
    "- èˆ‡è©¦é©—æ•¸æ“šçš„æ•¸å€¼ä¸€è‡´æ€§\n",
    "\n",
    "é™³è¿°: \"{statement}\"\n",
    "\n",
    "è©¦é©—è­‰æ“š:\n",
    "{trial_evidence}\n",
    "\n",
    "è«‹æä¾›ä½ çš„æ•¸å€¼åˆ†æï¼Œæœ€å¾Œä»¥ã€Œæ•¸å€¼è©•ä¼°: [æº–ç¢º/ä¸æº–ç¢º/éƒ¨åˆ†æº–ç¢º]ã€çµå°¾ã€‚\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"æ•¸å€¼åˆ†æéŒ¯èª¤: {str(e)}\"\n",
    "\n",
    "def logic_checker_thinking(statement: str, trial_evidence: str) -> str:\n",
    "    \"\"\"é‚è¼¯æª¢æŸ¥æ€è€ƒä»£ç†\"\"\"\n",
    "    prompt = f\"\"\"ä½ æ˜¯ä¸€ä½é‚è¼¯åˆ†æå°ˆå®¶ï¼Œå°ˆç²¾æ¨ç†é‚è¼¯å’Œä¸€è‡´æ€§æª¢æŸ¥ã€‚ä½ çš„ä»»å‹™æ˜¯é©—è­‰é™³è¿°çš„é‚è¼¯åˆç†æ€§ã€‚\n",
    "\n",
    "åˆ†æé‡é»ï¼š\n",
    "- é‚è¼¯çµæ§‹å’Œæ¨ç†éˆ\n",
    "- å› æœé—œä¿‚çš„åˆç†æ€§\n",
    "- å…§åœ¨é‚è¼¯ä¸€è‡´æ€§\n",
    "- æ¨è«–çš„æœ‰æ•ˆæ€§\n",
    "\n",
    "é™³è¿°: \"{statement}\"\n",
    "\n",
    "è©¦é©—è­‰æ“š:\n",
    "{trial_evidence}\n",
    "\n",
    "è«‹æä¾›ä½ çš„é‚è¼¯åˆ†æï¼Œæœ€å¾Œä»¥ã€Œé‚è¼¯è©•ä¼°: [åˆç†/ä¸åˆç†/æœ‰ç–‘æ…®]ã€çµå°¾ã€‚\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"é‚è¼¯åˆ†æéŒ¯èª¤: {str(e)}\"\n",
    "\n",
    "def decision_synthesizer_thinking(statement: str, medical_analysis: str, numerical_analysis: str, logic_analysis: str) -> str:\n",
    "    \"\"\"æ±ºç­–ç¶œåˆæ€è€ƒä»£ç†\"\"\"\n",
    "    prompt = f\"\"\"ä½ æ˜¯æ±ºç­–ç¶œåˆå°ˆå®¶ï¼Œè² è²¬æ•´åˆæ‰€æœ‰å°ˆå®¶åˆ†æä¸¦åšå‡ºæœ€çµ‚åˆ¤æ–·ã€‚\n",
    "\n",
    "ä»»å‹™: åˆ¤æ–·é™³è¿°æ˜¯ã€Œè˜Šå«ã€(Entailment)é‚„æ˜¯ã€ŒçŸ›ç›¾ã€(Contradiction)\n",
    "- è˜Šå«: é™³è¿°è¢«è©¦é©—è­‰æ“šç›´æ¥æ”¯æŒ\n",
    "- çŸ›ç›¾: é™³è¿°è¢«è©¦é©—è­‰æ“šåé§\n",
    "\n",
    "åŸå§‹é™³è¿°: \"{statement}\"\n",
    "\n",
    "é†«å­¸å°ˆå®¶åˆ†æ:\n",
    "{medical_analysis}\n",
    "\n",
    "æ•¸å€¼åˆ†æå°ˆå®¶åˆ†æ:\n",
    "{numerical_analysis}\n",
    "\n",
    "é‚è¼¯æª¢æŸ¥å°ˆå®¶åˆ†æ:\n",
    "{logic_analysis}\n",
    "\n",
    "è«‹ç¶œåˆä»¥ä¸Šåˆ†æï¼Œæä¾›ç°¡è¦æ¨ç†ï¼Œç„¶å¾Œä»¥ã€Œæœ€çµ‚æ±ºç­–: [Entailment/Contradiction]ã€çµå°¾ã€‚\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"æ±ºç­–ç¶œåˆéŒ¯èª¤: {str(e)}\"\n",
    "\n",
    "print(\"âœ… å››å€‹åŸå­æ€è€ƒä»£ç†å®šç¾©å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŸå­ä»£ç†åˆ†æç®¡é“\n",
    "\n",
    "å‰µå»ºå”èª¿æ‰€æœ‰æ€è€ƒä»£ç†çš„åˆ†æç®¡é“ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atomic_agents_pipeline(statement: str, primary_id: str, secondary_id: str = None, \n",
    "                          section_id: str = None, verbose: bool = False) -> str:\n",
    "    \"\"\"é‹è¡Œå®Œæ•´çš„åŸå­ä»£ç†åˆ†æç®¡é“\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # è¼‰å…¥è©¦é©—è³‡æ–™\n",
    "        primary_data = load_clinical_trial(primary_id)\n",
    "        primary_evidence = extract_trial_section(primary_data, section_id)\n",
    "        \n",
    "        trial_context = f\"ä¸»è¦è©¦é©— ({primary_id}):\\n{primary_evidence}\"\n",
    "        \n",
    "        if secondary_id:\n",
    "            secondary_data = load_clinical_trial(secondary_id)\n",
    "            secondary_evidence = extract_trial_section(secondary_data, section_id)\n",
    "            trial_context += f\"\\n\\næ¬¡è¦è©¦é©— ({secondary_id}):\\n{secondary_evidence}\"\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"ğŸ“„ åˆ†æé™³è¿°: {statement[:100]}...\")\n",
    "            print(f\"ğŸ¥ ä¸»è¦è©¦é©—: {primary_id}\")\n",
    "            if secondary_id:\n",
    "                print(f\"ğŸ¥ æ¬¡è¦è©¦é©—: {secondary_id}\")\n",
    "        \n",
    "        # æ­¥é©Ÿ1: é†«å­¸å°ˆå®¶æ€è€ƒ\n",
    "        medical_analysis = medical_expert_thinking(statement, trial_context)\n",
    "        if verbose:\n",
    "            print(\"ğŸ©º é†«å­¸å°ˆå®¶: å®Œæˆåˆ†æ\")\n",
    "        \n",
    "        # æ­¥é©Ÿ2: æ•¸å€¼åˆ†ææ€è€ƒ\n",
    "        numerical_analysis = numerical_analyst_thinking(statement, trial_context)\n",
    "        if verbose:\n",
    "            print(\"ğŸ”¢ æ•¸å€¼åˆ†æ: å®Œæˆåˆ†æ\")\n",
    "        \n",
    "        # æ­¥é©Ÿ3: é‚è¼¯æª¢æŸ¥æ€è€ƒ\n",
    "        logic_analysis = logic_checker_thinking(statement, trial_context)\n",
    "        if verbose:\n",
    "            print(\"ğŸ§  é‚è¼¯æª¢æŸ¥: å®Œæˆåˆ†æ\")\n",
    "        \n",
    "        # æ­¥é©Ÿ4: æ±ºç­–ç¶œåˆ\n",
    "        final_analysis = decision_synthesizer_thinking(\n",
    "            statement, medical_analysis, numerical_analysis, logic_analysis\n",
    "        )\n",
    "        \n",
    "        # æå–æœ€çµ‚æ±ºç­–\n",
    "        if \"æœ€çµ‚æ±ºç­–: Entailment\" in final_analysis:\n",
    "            decision = \"Entailment\"\n",
    "        elif \"æœ€çµ‚æ±ºç­–: Contradiction\" in final_analysis:\n",
    "            decision = \"Contradiction\"\n",
    "        else:\n",
    "            # å‚™ç”¨è§£æ\n",
    "            if \"entailment\" in final_analysis.lower() and \"contradiction\" not in final_analysis.lower():\n",
    "                decision = \"Entailment\"\n",
    "            else:\n",
    "                decision = \"Contradiction\"\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"âš–ï¸ æœ€çµ‚æ±ºç­–: {decision}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        return decision\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"âŒ ç®¡é“éŒ¯èª¤: {e}\")\n",
    "        return \"Contradiction\"  # ä¿å®ˆçš„å‚™ç”¨æ–¹æ¡ˆ\n",
    "\n",
    "print(\"âœ… åŸå­ä»£ç†åˆ†æç®¡é“æº–å‚™å°±ç·’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¸¬è©¦ç¯„ä¾‹\n",
    "\n",
    "æ¸¬è©¦æˆ‘å€‘çš„åŸå­ä»£ç†ç³»çµ±ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦ç¯„ä¾‹\n",
    "test_statement = \"there is a 13.2% difference between the results from the two the primary trial cohorts\"\n",
    "test_primary_id = \"NCT00066573\"\n",
    "\n",
    "print(f\"æ¸¬è©¦åŸå­ä»£ç†ç³»çµ±:\")\n",
    "print(f\"é™³è¿°: '{test_statement}'\")\n",
    "print(f\"ä¸»è¦è©¦é©—: {test_primary_id}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# åŸ·è¡Œåˆ†æ\n",
    "start_time = time.time()\n",
    "result = atomic_agents_pipeline(\n",
    "    statement=test_statement,\n",
    "    primary_id=test_primary_id,\n",
    "    section_id=\"Results\",\n",
    "    verbose=True\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nğŸ¯ åŸå­ä»£ç†çµæœ: {result}\")\n",
    "print(f\"â±ï¸ åŸ·è¡Œæ™‚é–“: {end_time - start_time:.2f} ç§’\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åœ¨è¨“ç·´è³‡æ–™ä¸Šè©•ä¼°\n",
    "\n",
    "åœ¨è¨“ç·´è³‡æ–™æ¨£æœ¬ä¸Šè©•ä¼°æˆ‘å€‘çš„ç³»çµ±ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¼‰å…¥è¨“ç·´è³‡æ–™\n",
    "with open(\"training_data/train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "print(f\"è¼‰å…¥ {len(train_data)} å€‹è¨“ç·´ç¯„ä¾‹\")\n",
    "\n",
    "# åœ¨æ¨£æœ¬ä¸Šè©•ä¼°\n",
    "sample_size = 20\n",
    "examples = list(train_data.items())[:sample_size]\n",
    "\n",
    "print(f\"\\nåœ¨ {len(examples)} å€‹ç¯„ä¾‹ä¸Šè©•ä¼°åŸå­ä»£ç†ç³»çµ±...\")\n",
    "\n",
    "results = []\n",
    "correct = 0\n",
    "total_time = 0\n",
    "\n",
    "for i, (uuid, example) in enumerate(tqdm(examples, desc=\"åŸå­ä»£ç†è™•ç†\")):\n",
    "    try:\n",
    "        statement = example.get(\"Statement\")\n",
    "        primary_id = example.get(\"Primary_id\")\n",
    "        secondary_id = example.get(\"Secondary_id\")\n",
    "        section_id = example.get(\"Section_id\")\n",
    "        expected = example.get(\"Label\")\n",
    "        \n",
    "        if not statement or not primary_id:\n",
    "            results.append({\n",
    "                \"uuid\": uuid,\n",
    "                \"expected\": expected,\n",
    "                \"predicted\": \"SKIPPED\",\n",
    "                \"correct\": False,\n",
    "                \"time\": 0\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # ç²å–é æ¸¬\n",
    "        start_time = time.time()\n",
    "        predicted = atomic_agents_pipeline(\n",
    "            statement=statement,\n",
    "            primary_id=primary_id,\n",
    "            secondary_id=secondary_id,\n",
    "            section_id=section_id,\n",
    "            verbose=False\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        execution_time = end_time - start_time\n",
    "        total_time += execution_time\n",
    "        \n",
    "        # æª¢æŸ¥æ­£ç¢ºæ€§\n",
    "        is_correct = (predicted.strip() == expected.strip())\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            \n",
    "        results.append({\n",
    "            \"uuid\": uuid,\n",
    "            \"statement\": statement[:80] + \"...\" if len(statement) > 80 else statement,\n",
    "            \"expected\": expected,\n",
    "            \"predicted\": predicted,\n",
    "            \"correct\": is_correct,\n",
    "            \"time\": execution_time\n",
    "        })\n",
    "        \n",
    "        status = \"âœ…\" if is_correct else \"âŒ\"\n",
    "        print(f\"ç¯„ä¾‹ {i+1:2d}: {expected:12} -> {predicted:12} {status} ({execution_time:.1f}s)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"è™•ç†ç¯„ä¾‹ {i+1} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        results.append({\n",
    "            \"uuid\": uuid,\n",
    "            \"expected\": expected,\n",
    "            \"predicted\": \"ERROR\",\n",
    "            \"correct\": False,\n",
    "            \"time\": 0\n",
    "        })\n",
    "\n",
    "# è¨ˆç®—æº–ç¢ºç‡\n",
    "accuracy = correct / len(examples) if examples else 0\n",
    "avg_time = total_time / len(examples) if examples else 0\n",
    "\n",
    "print(f\"\\nğŸ“Š åŸå­ä»£ç†ç³»çµ±çµæœ:\")\n",
    "print(f\"æº–ç¢ºç‡: {accuracy:.2%} ({correct}/{len(examples)})\")\n",
    "print(f\"å¹³å‡åŸ·è¡Œæ™‚é–“: {avg_time:.2f} ç§’/ä¾‹\")\n",
    "print(f\"ç¸½åŸ·è¡Œæ™‚é–“: {total_time:.2f} ç§’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç”¢ç”Ÿæäº¤æª”æ¡ˆ\n",
    "\n",
    "ä½¿ç”¨æˆ‘å€‘çš„åŸå­ä»£ç†ç³»çµ±ç”¢ç”Ÿé æ¸¬çµæœï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_atomic_submission(test_file=\"test.json\", output_file=\"atomic_agents_submission.json\", sample_size=None):\n",
    "    \"\"\"ä½¿ç”¨åŸå­ä»£ç†ç³»çµ±ç”¢ç”Ÿæäº¤æª”æ¡ˆ\"\"\"\n",
    "    \n",
    "    # è¼‰å…¥æ¸¬è©¦è³‡æ–™\n",
    "    try:\n",
    "        with open(test_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            test_data = json.load(f)\n",
    "    except:\n",
    "        print(f\"âŒ ç„¡æ³•è¼‰å…¥æ¸¬è©¦è³‡æ–™ {test_file}\")\n",
    "        return\n",
    "    \n",
    "    examples = list(test_data.items())\n",
    "    if sample_size:\n",
    "        examples = examples[:sample_size]\n",
    "        \n",
    "    print(f\"ğŸš€ ç‚º {len(examples)} å€‹ç¯„ä¾‹ç”¢ç”ŸåŸå­ä»£ç†é æ¸¬...\")\n",
    "    \n",
    "    submission = {}\n",
    "    \n",
    "    for i, (uuid, example) in enumerate(tqdm(examples, desc=\"åŸå­ä»£ç†è™•ç†\")):\n",
    "        try:\n",
    "            statement = example.get(\"Statement\")\n",
    "            primary_id = example.get(\"Primary_id\")\n",
    "            secondary_id = example.get(\"Secondary_id\")\n",
    "            section_id = example.get(\"Section_id\")\n",
    "            \n",
    "            if not statement or not primary_id:\n",
    "                submission[uuid] = {\"Prediction\": \"Contradiction\"}\n",
    "                continue\n",
    "                \n",
    "            # ç²å–é æ¸¬\n",
    "            prediction = atomic_agents_pipeline(\n",
    "                statement=statement,\n",
    "                primary_id=primary_id,\n",
    "                secondary_id=secondary_id,\n",
    "                section_id=section_id,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            submission[uuid] = {\"Prediction\": prediction}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"è™•ç† {uuid} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "            submission[uuid] = {\"Prediction\": \"Contradiction\"}\n",
    "    \n",
    "    # å„²å­˜æäº¤æª”æ¡ˆ\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(submission, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… åŸå­ä»£ç†æäº¤æª”æ¡ˆå·²å„²å­˜è‡³ {output_file}\")\n",
    "    return submission\n",
    "\n",
    "# ç”¢ç”Ÿå°æ¨£æœ¬æäº¤\n",
    "atomic_submission = generate_atomic_submission(\n",
    "    test_file=\"test.json\", \n",
    "    output_file=\"atomic_agents_submission.json\",\n",
    "    sample_size=10\n",
    ")\n",
    "\n",
    "print(f\"ç‚º {len(atomic_submission)} å€‹ç¯„ä¾‹ç”¢ç”Ÿäº†é æ¸¬\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## çµè«–\n",
    "\n",
    "### åŸå­ä»£ç†ç³»çµ±å„ªå‹¢ï¼š\n",
    "1. **ç°¡æ½”æ€§**: ç„¡è¤‡é›œæ¡†æ¶ï¼Œç´”ç²¹ä½¿ç”¨Gemini API\n",
    "2. **å°ˆæ¥­åˆ†å·¥**: å››å€‹å°ˆé–€çš„æ€è€ƒä»£ç†å„å¸å…¶è·\n",
    "3. **é€æ˜æ€§**: æ¯å€‹æ€è€ƒæ­¥é©Ÿéƒ½æ¸…æ™°å¯è¦‹\n",
    "4. **é«˜æ•ˆæ€§**: ç›´æ¥çš„APIèª¿ç”¨ï¼Œç„¡é¡å¤–é–‹éŠ·\n",
    "5. **å¯é æ€§**: ç°¡å–®æ¶æ§‹ï¼Œè¼ƒå°‘å‡ºéŒ¯é»\n",
    "\n",
    "### æ€è€ƒä»£ç†æ¶æ§‹ï¼š\n",
    "- **é†«ç™‚å°ˆå®¶æ€è€ƒ**: å°ˆæ³¨æ–¼é†«å­¸æº–ç¢ºæ€§å’Œè‡¨åºŠæ„ç¾©\n",
    "- **æ•¸å€¼åˆ†ææ€è€ƒ**: é©—è­‰çµ±è¨ˆæ•¸æ“šå’Œè¨ˆç®—\n",
    "- **é‚è¼¯æª¢æŸ¥æ€è€ƒ**: ç¢ºä¿æ¨ç†é‚è¼¯çš„åˆç†æ€§\n",
    "- **æ±ºç­–ç¶œåˆæ€è€ƒ**: æ•´åˆæ‰€æœ‰åˆ†æåšå‡ºæœ€çµ‚åˆ¤æ–·\n",
    "\n",
    "### é©ç”¨å ´æ™¯ï¼š\n",
    "- éœ€è¦å¿«é€Ÿéƒ¨ç½²çš„è‡¨åºŠNLPåˆ†æ\n",
    "- è³‡æºæœ‰é™ä½†è¦æ±‚æº–ç¢ºæ€§çš„æ‡‰ç”¨\n",
    "- æ•™å­¸å’Œæ¼”ç¤ºç”¨é€”\n",
    "- ä½œç‚ºæ›´è¤‡é›œç³»çµ±çš„åŸºæº–\n",
    "\n",
    "é€™å€‹åŸå­ä»£ç†ç³»çµ±å±•ç¤ºäº†å¦‚ä½•ç”¨æœ€ç°¡å–®çš„æ–¹å¼å¯¦ç¾æœ‰æ•ˆçš„å¤šä»£ç†æ€è€ƒï¼Œé©åˆä½œç‚ºè‡¨åºŠè©¦é©—NLPåˆ†æçš„å¯¦ç”¨åŸºæº–ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
