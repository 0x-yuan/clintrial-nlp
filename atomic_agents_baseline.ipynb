{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/luyuan/clintrial-nlp/blob/main/atomic_agents_baseline.ipynb)\n\n# Atomic Agents æ¡†æ¶åŸºç·š - è‡¨åºŠè©¦é©— NLP\n\n## æ¦‚è¿°\n\næœ¬notebookå±•ç¤ºå¦‚ä½•ä½¿ç”¨Atomic Agentsæ¡†æ¶å»ºæ§‹ä¸€å€‹è¼•é‡ç´šã€é«˜æ•ˆèƒ½çš„å¤šä»£ç†ç³»çµ±ï¼Œç”¨æ–¼è‡¨åºŠè©¦é©—è‡ªç„¶èªè¨€æ¨ç†(NLI)ã€‚Atomic Agentså°ˆç‚ºç”Ÿç”¢ç’°å¢ƒè¨­è¨ˆï¼Œå…·æœ‰æ¥µå¿«çš„å•Ÿå‹•æ™‚é–“(~3Î¼s)å’Œæ¨¡çµ„åŒ–æ¶æ§‹ã€‚\n\n## ğŸ“š å­¸ç¿’ç›®æ¨™\nå®Œæˆæœ¬æ•™å­¸å¾Œï¼Œæ‚¨å°‡å­¸æœƒï¼š\n- ç†è§£ Atomic Agents æ¡†æ¶çš„æ ¸å¿ƒæ¦‚å¿µ\n- å»ºç«‹å°ˆé–€çš„é†«ç™‚ã€æ•¸å€¼å’Œé‚è¼¯åˆ†æä»£ç†\n- å¯¦ä½œå¤šä»£ç†å”ä½œpipeline\n- è©•ä¼°å’Œæ”¹é€²ç³»çµ±æ•ˆèƒ½\n\n### ç‚ºä»€éº¼é¸æ“‡ Atomic Agentsï¼Ÿ\n- **è¶…è¼•é‡ç´š**: æœ€å°åŒ–é–‹éŠ·å’Œå¿«é€ŸåŸ·è¡Œ\n- **é«˜åº¦æ¨¡çµ„åŒ–**: æ˜“æ–¼çµ„åˆå’Œä¿®æ”¹ä»£ç†\n- **ç”Ÿç”¢å°±ç·’**: å°ˆç‚ºå¯¦éš›éƒ¨ç½²è€Œè¨­è¨ˆ\n- **ç°¡å–®API**: ç›´è§€çš„ä»£ç†å‰µå»ºå’Œå”èª¿\n- **è¨˜æ†¶é«”ç®¡ç†**: å…§å»ºçš„ä¸Šä¸‹æ–‡å’Œè¨˜æ†¶é«”è™•ç†\n\n### ğŸ—ï¸ ä»£ç†æ¶æ§‹\néµå¾ªæ•™å­¸åœ–è¡¨ï¼Œæˆ‘å€‘å¯¦ä½œä¸€å€‹çµæ§‹åŒ–çš„pipelineï¼š\n1. **é†«ç™‚å°ˆå®¶ä»£ç†**: åˆ†æé†«å­¸è¡“èªå’Œæ¦‚å¿µ\n2. **æ•¸å€¼åˆ†æä»£ç†**: è™•ç†é‡åŒ–æ•¸æ“š\n3. **é‚è¼¯æª¢æŸ¥ä»£ç†**: é©—è­‰é‚è¼¯é—œä¿‚\n4. **èšåˆä»£ç†**: çµåˆè¦‹è§£åšå‡ºæœ€çµ‚æ±ºç­–\n5. **ç›£ç£å”èª¿**: ç®¡ç†æ•´é«”å·¥ä½œæµç¨‹\n\n> ğŸ’¡ **é‡è¦æ¦‚å¿µ**: Atomic Agents çš„\"atomic\"æŒ‡çš„æ˜¯æ¯å€‹ä»£ç†éƒ½æ˜¯ä¸€å€‹ç¨ç«‹ã€æœ€å°çš„åŠŸèƒ½å–®å…ƒï¼Œå¯ä»¥è¼•é¬†çµ„åˆæˆè¤‡é›œç³»çµ±ã€‚"
  },
  {
   "cell_type": "code",
   "id": "1p3qmrclzei",
   "source": "# ğŸ”§ Colab ç’°å¢ƒè¨­ç½® - ä¸€éµå®‰è£æ‰€éœ€å¥—ä»¶\n# é€™å€‹cellæœƒéœé»˜å®‰è£æ‰€æœ‰å¿…è¦çš„Pythonå¥—ä»¶ï¼Œè®“æ‚¨å¯ä»¥åœ¨Colabä¸­ç›´æ¥é‹è¡Œæ­¤notebook\n!pip install -q atomic-agents instructor openai python-dotenv pandas tqdm\n!pip install -q google-generativeai gdown\n\nprint(\"âœ… æ‰€æœ‰å¥—ä»¶å®‰è£å®Œæˆï¼å¯ä»¥é–‹å§‹ä½¿ç”¨ Atomic Agents æ¡†æ¶äº†\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3fpaymxvzg",
   "source": "# ğŸ“¥ å¾ Google Drive ä¸‹è¼‰è¨“ç·´è³‡æ–™\n# é€™å€‹cellæœƒè‡ªå‹•ä¸‹è¼‰æ‰€éœ€çš„è‡¨åºŠè©¦é©—è³‡æ–™ï¼Œç¢ºä¿åœ¨Colabä¸­å¯ä»¥ç›´æ¥é‹è¡Œ\nimport os\nimport gdown\n\n# Google Drive è³‡æ–™å¤¾ ID\nfolder_id = \"1IcRoCbH0Oxtia8N47dy9fTWl4dfq3RS3\"\n\n# æª¢æŸ¥æ˜¯å¦å·²ä¸‹è¼‰è¨“ç·´è³‡æ–™\nif not os.path.exists(\"training_data\"):\n    print(\"ğŸ“¥ å¾ Google Drive ä¸‹è¼‰è¨“ç·´è³‡æ–™...\")\n    try:\n        # ä¸‹è¼‰æ•´å€‹è³‡æ–™å¤¾\n        gdown.download_folder(id=folder_id, output=\".\", quiet=False, use_cookies=False)\n        print(\"âœ… è¨“ç·´è³‡æ–™ä¸‹è¼‰å®Œæˆï¼\")\n    except Exception as e:\n        print(f\"âŒ ä¸‹è¼‰å¤±æ•—: {e}\")\n        print(\"è«‹ç¢ºèª Google Drive é€£çµçš„æ¬Šé™è¨­å®š\")\nelse:\n    print(\"âœ… è¨“ç·´è³‡æ–™å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰\")\n\n# æª¢æŸ¥ä¸‹è¼‰çš„è³‡æ–™çµæ§‹\nif os.path.exists(\"training_data\"):\n    print(f\"ğŸ“‚ è³‡æ–™å¤¾å…§å®¹: {os.listdir('training_data')}\")\n    if os.path.exists(\"training_data/CT json\"):\n        ct_files = len(os.listdir(\"training_data/CT json\"))\n        print(f\"ğŸ“„ æ‰¾åˆ° {ct_files} å€‹è‡¨åºŠè©¦é©—JSONæª”æ¡ˆ\")\nelse:\n    print(\"âš ï¸ ç„¡æ³•æ‰¾åˆ° training_data è³‡æ–™å¤¾\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": "## ç’°å¢ƒè¨­ç½®å’Œå®‰è£\n\né¦–å…ˆï¼Œè®“æˆ‘å€‘è¨­ç½®ç’°å¢ƒä¸¦åŒ¯å…¥å¿…è¦çš„å‡½å¼åº«ï¼š\n\n> ğŸ“ **èªªæ˜**: åœ¨é€™å€‹æ­¥é©Ÿä¸­ï¼Œæˆ‘å€‘æœƒè¼‰å…¥ç’°å¢ƒè®Šæ•¸ä¸¦ç¢ºèªæ‰€æœ‰å¿…è¦çš„å¥—ä»¶éƒ½å·²æ­£ç¢ºå®‰è£ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "# è¼‰å…¥ç’°å¢ƒè®Šæ•¸\n# é€™å€‹æ­¥é©Ÿæœƒå¾ .env æª”æ¡ˆè¼‰å…¥ API é‡‘é‘°ç­‰æ•æ„Ÿè³‡è¨Š\nfrom dotenv import load_dotenv\nimport os\n\nload_dotenv()\nprint(\"âœ… ç’°å¢ƒè®Šæ•¸è¼‰å…¥å®Œæˆ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependencies",
   "metadata": {},
   "outputs": [],
   "source": "# åŒ¯å…¥å¿…è¦çš„å‡½å¼åº«\n# é€™äº›å‡½å¼åº«æä¾›äº†è³‡æ–™è™•ç†ã€AIæ¨¡å‹å’ŒAtomic Agentsæ¡†æ¶çš„åŠŸèƒ½\nimport json\nimport pandas as pd\nfrom tqdm import tqdm  # é€²åº¦æ¢é¡¯ç¤º\nimport instructor  # çµæ§‹åŒ–è¼¸å‡º\nimport openai\nfrom typing import Dict, List, Any, Optional\nimport warnings\nwarnings.filterwarnings('ignore')  # éš±è—è­¦å‘Šè¨Šæ¯\n\n# Atomic Agents æ ¸å¿ƒçµ„ä»¶\nfrom atomic_agents.lib.components.agent_memory import AgentMemory  # ä»£ç†è¨˜æ†¶é«”ç®¡ç†\nfrom atomic_agents.agents.base_agent import BaseAgent, BaseAgentConfig, BaseAgentInputSchema  # åŸºç¤ä»£ç†é¡åˆ¥\nfrom atomic_agents.lib.components.system_prompt_generator import SystemPromptGenerator  # ç³»çµ±æç¤ºç”Ÿæˆå™¨\n\nprint(\"âœ… æ‰€æœ‰å‡½å¼åº«åŒ¯å…¥æˆåŠŸ\")"
  },
  {
   "cell_type": "markdown",
   "id": "data_utils",
   "metadata": {},
   "source": "## è³‡æ–™è¼‰å…¥å’Œå·¥å…·å‡½å¼\n\nè®“æˆ‘å€‘å»ºç«‹ç”¨æ–¼è¼‰å…¥å’Œè™•ç†è‡¨åºŠè©¦é©—è³‡æ–™çš„å·¥å…·å‡½å¼ï¼š\n\n> ğŸ”§ **åŠŸèƒ½èªªæ˜**: é€™äº›å‡½å¼è² è²¬å¾JSONæª”æ¡ˆä¸­è¼‰å…¥è‡¨åºŠè©¦é©—è³‡æ–™ï¼Œä¸¦å°‡å…¶è½‰æ›ç‚ºé©åˆAIä»£ç†åˆ†æçš„æ ¼å¼ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utils",
   "metadata": {},
   "outputs": [],
   "source": "def load_clinical_trial(trial_id: str) -> Dict[str, Any]:\n    \"\"\"è¼‰å…¥è‡¨åºŠè©¦é©—è³‡æ–™å¾JSONæª”æ¡ˆã€‚\n    \n    Args:\n        trial_id: è‡¨åºŠè©¦é©—çš„NCTè­˜åˆ¥ç¢¼\n        \n    Returns:\n        åŒ…å«è©¦é©—è³‡æ–™çš„å­—å…¸æˆ–éŒ¯èª¤è³‡è¨Š\n    \"\"\"\n    try:\n        file_path = os.path.join(\"training_data\", \"CT json\", f\"{trial_id}.json\")\n        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        return data\n    except FileNotFoundError:\n        return {\"error\": f\"æ‰¾ä¸åˆ°è‡¨åºŠè©¦é©— {trial_id}\"}\n    except Exception as e:\n        return {\"error\": f\"è¼‰å…¥ {trial_id} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}\"}\n\ndef load_dataset(filepath: str) -> Dict[str, Any]:\n    \"\"\"è¼‰å…¥è¨“ç·´æˆ–æ¸¬è©¦è³‡æ–™é›†ã€‚\n    \n    Args:\n        filepath: JSONè³‡æ–™é›†æª”æ¡ˆçš„è·¯å¾‘\n        \n    Returns:\n        åŒ…å«è³‡æ–™é›†çš„å­—å…¸\n    \"\"\"\n    try:\n        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n    except Exception as e:\n        print(f\"è¼‰å…¥è³‡æ–™é›†æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")\n        return {}\n\ndef extract_relevant_sections(trial_data: Dict[str, Any], section_id: str) -> str:\n    \"\"\"æ ¹æ“šsection_idå¾è©¦é©—è³‡æ–™ä¸­æå–ç›¸é—œéƒ¨åˆ†ã€‚\n    \n    Args:\n        trial_data: è‡¨åºŠè©¦é©—è³‡æ–™å­—å…¸\n        section_id: ç›®æ¨™å€æ®µ (Eligibility, Intervention, Results, Adverse Events)\n        \n    Returns:\n        åŒ…å«ç›¸é—œå€æ®µè³‡æ–™çš„æ ¼å¼åŒ–å­—ä¸²\n    \"\"\"\n    if \"error\" in trial_data:\n        return f\"éŒ¯èª¤: {trial_data['error']}\"\n    \n    sections = {\n        \"Eligibility\": trial_data.get(\"Eligibility\", []),\n        \"Intervention\": trial_data.get(\"Intervention\", []),\n        \"Results\": trial_data.get(\"Results\", []),\n        \"Adverse Events\": trial_data.get(\"Adverse_Events\", [])\n    }\n    \n    # å¦‚æœè«‹æ±‚ç‰¹å®šå€æ®µï¼Œå‰‡åƒ…è¿”å›è©²å€æ®µ\n    if section_id in sections:\n        section_data = sections[section_id]\n        if isinstance(section_data, list):\n            return \"\\n\".join(str(item) for item in section_data)\n        return str(section_data)\n    \n    # å¦å‰‡è¿”å›æ‰€æœ‰å€æ®µ\n    result = []\n    for section_name, section_data in sections.items():\n        if section_data:\n            result.append(f\"{section_name}:\")\n            if isinstance(section_data, list):\n                result.extend([f\"  {item}\" for item in section_data])\n            else:\n                result.append(f\"  {section_data}\")\n    \n    return \"\\n\".join(result)\n\n# æ¸¬è©¦å·¥å…·å‡½å¼\nsample_trial = load_clinical_trial(\"NCT00066573\")\nprint(f\"âœ… è³‡æ–™å·¥å…·å‡½å¼æº–å‚™å°±ç·’ã€‚ç¯„ä¾‹è©¦é©—: {sample_trial.get('Clinical Trial ID', 'éŒ¯èª¤')}\")\n\n# ğŸ“‹ å‡½å¼èªªæ˜ï¼š\n# - load_clinical_trial(): è¼‰å…¥å–®ä¸€è‡¨åºŠè©¦é©—çš„å®Œæ•´è³‡æ–™\n# - load_dataset(): è¼‰å…¥åŒ…å«å¤šå€‹è©¦é©—çš„è¨“ç·´/æ¸¬è©¦è³‡æ–™é›†  \n# - extract_relevant_sections(): æå–è©¦é©—ä¸­ç‰¹å®šå€æ®µçš„è³‡æ–™"
  },
  {
   "cell_type": "markdown",
   "id": "model_setup",
   "metadata": {},
   "source": "## æ¨¡å‹é…ç½®\n\nè¨­ç½®OpenAIå®¢æˆ¶ç«¯é…åˆinstructorä¾†ç”¢ç”Ÿçµæ§‹åŒ–è¼¸å‡ºï¼š\n\n> ğŸ¤– **æŠ€è¡“èªªæ˜**: Instructorå¥—ä»¶å¯ä»¥ç¢ºä¿AIæ¨¡å‹ç”¢ç”Ÿç¬¦åˆç‰¹å®šæ ¼å¼çš„çµæ§‹åŒ–å›æ‡‰ï¼Œæé«˜ç³»çµ±çš„å¯é æ€§ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_config",
   "metadata": {},
   "outputs": [],
   "source": "# åˆå§‹åŒ–OpenAIå®¢æˆ¶ç«¯é…åˆinstructor\n# instructoråŒ…è£å™¨ç¢ºä¿AIå›æ‡‰éµå¾ªçµæ§‹åŒ–æ ¼å¼\nclient = instructor.from_openai(openai.OpenAI(\n    api_key=os.getenv(\"OPENAI_API_KEY\")  # å¾ç’°å¢ƒè®Šæ•¸å–å¾—APIé‡‘é‘°\n))\n\n# æ¨¡å‹é…ç½®\nMODEL_NAME = \"gpt-4o-mini\"  # ä½¿ç”¨GPT-4o-miniæ¨¡å‹ï¼ˆæˆæœ¬æ•ˆç›Šè¼ƒé«˜ï¼‰\n\nprint(f\"âœ… OpenAIå®¢æˆ¶ç«¯é…ç½®å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {MODEL_NAME}\")\n\n# ğŸ’° æˆæœ¬èªªæ˜ï¼šGPT-4o-miniæ˜¯OpenAIè¼ƒæ–°çš„æ¨¡å‹ï¼Œæ¯”GPT-4ä¾¿å®œä½†ä¿æŒé«˜å“è³ª"
  },
  {
   "cell_type": "markdown",
   "id": "agent_definitions",
   "metadata": {},
   "source": "## ä»£ç†å®šç¾©\n\nç¾åœ¨è®“æˆ‘å€‘ä½¿ç”¨Atomic Agentsæ¡†æ¶å®šç¾©æ¯å€‹å°ˆé–€çš„ä»£ç†ã€‚æ¯å€‹ä»£ç†éƒ½æœ‰ç‰¹å®šçš„è§’è‰²å’Œå°ˆæ¥­é ˜åŸŸï¼š\n\n> ğŸ¯ **è¨­è¨ˆåŸå‰‡**: æ¯å€‹ä»£ç†éƒ½å°ˆæ³¨æ–¼ç‰¹å®šé ˜åŸŸçš„åˆ†æï¼Œé€éåˆ†å·¥åˆä½œä¾†æé«˜æ•´é«”åˆ†æå“è³ªã€‚é€™æ˜¯å¤šä»£ç†ç³»çµ±çš„æ ¸å¿ƒå„ªå‹¢ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical_expert",
   "metadata": {},
   "outputs": [],
   "source": "# 1. é†«ç™‚å°ˆå®¶ä»£ç†\n# é€™å€‹ä»£ç†å°ˆé–€è² è²¬å¾é†«å­¸è§’åº¦åˆ†æé™³è¿°çš„æº–ç¢ºæ€§\nmedical_expert_prompt = SystemPromptGenerator(\n    background=[\n        \"You are a Medical Expert Agent specializing in clinical trial analysis.\",\n        \"You have deep knowledge of medical terminology, clinical concepts, and trial procedures.\",\n        \"Your role is to analyze statements from a medical perspective and identify relevant clinical insights.\"\n    ],\n    steps=[\n        \"1. Analyze the medical terminology and concepts in the statement\",\n        \"2. Identify key clinical elements and their significance\", \n        \"3. Review the clinical trial data for medical accuracy\",\n        \"4. Assess whether the medical claims align with the trial evidence\",\n        \"5. Provide medical reasoning and clinical context\"\n    ],\n    output_instructions=[\n        \"Provide a clear medical analysis focusing on:\",\n        \"- Medical terminology accuracy\",\n        \"- Clinical relevance and significance\", \n        \"- Alignment with medical evidence in the trial\",\n        \"- Any medical concerns or considerations\",\n        \"End with: MEDICAL_ASSESSMENT: [SUPPORTS/CONTRADICTS/UNCLEAR] based on medical evidence\"\n    ]\n)\n\n# ç‚ºé†«ç™‚å°ˆå®¶ä»£ç†å»ºç«‹è¨˜æ†¶é«”çµ„ä»¶\nmedical_expert_memory = AgentMemory()\n\n# å»ºç«‹é†«ç™‚å°ˆå®¶ä»£ç†å¯¦ä¾‹\nmedical_expert_agent = BaseAgent(\n    config=BaseAgentConfig(\n        client=client,\n        model=MODEL_NAME,\n        system_prompt_generator=medical_expert_prompt,\n        memory=medical_expert_memory\n    )\n)\n\nprint(\"âœ… é†«ç™‚å°ˆå®¶ä»£ç†å»ºç«‹å®Œæˆ\")\n\n# ğŸ©º ä»£ç†èªªæ˜ï¼šé†«ç™‚å°ˆå®¶ä»£ç†å°ˆé–€åˆ†æé†«å­¸è¡“èªçš„æº–ç¢ºæ€§å’Œè‡¨åºŠç›¸é—œæ€§"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical_analyzer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Numerical Analyzer Agent\n",
    "numerical_analyzer_prompt = SystemPromptGenerator(\n",
    "    background=[\n",
    "        \"You are a Numerical Analyzer Agent specializing in quantitative analysis of clinical trials.\",\n",
    "        \"You excel at processing numbers, statistics, percentages, and numerical relationships.\",\n",
    "        \"Your role is to verify numerical claims and perform statistical analysis.\"\n",
    "    ],\n",
    "    steps=[\n",
    "        \"1. Extract all numerical values, percentages, and statistics from the statement\",\n",
    "        \"2. Identify corresponding numbers in the clinical trial data\",\n",
    "        \"3. Perform calculations to verify numerical relationships\",\n",
    "        \"4. Check for statistical significance and clinical meaningfulness\",\n",
    "        \"5. Identify any numerical inconsistencies or errors\"\n",
    "    ],\n",
    "    output_instructions=[\n",
    "        \"Provide a detailed numerical analysis including:\",\n",
    "        \"- All numerical values extracted from the statement\",\n",
    "        \"- Corresponding values found in trial data\",\n",
    "        \"- Calculations performed to verify claims\",\n",
    "        \"- Assessment of numerical accuracy\",\n",
    "        \"End with: NUMERICAL_ASSESSMENT: [ACCURATE/INACCURATE/PARTIALLY_ACCURATE] with confidence level\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "numerical_analyzer_memory = AgentMemory()\n",
    "\n",
    "numerical_analyzer_agent = BaseAgent(\n",
    "    config=BaseAgentConfig(\n",
    "        client=client,\n",
    "        model=MODEL_NAME,\n",
    "        system_prompt_generator=numerical_analyzer_prompt,\n",
    "        memory=numerical_analyzer_memory\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"âœ… Numerical Analyzer Agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logic_checker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Logic Checker Agent\n",
    "logic_checker_prompt = SystemPromptGenerator(\n",
    "    background=[\n",
    "        \"You are a Logic Checker Agent responsible for validating logical reasoning and consistency.\",\n",
    "        \"You specialize in identifying logical relationships, contradictions, and reasoning patterns.\",\n",
    "        \"Your role is to ensure logical soundness and coherence in claims and evidence.\"\n",
    "    ],\n",
    "    steps=[\n",
    "        \"1. Analyze the logical structure of the statement\",\n",
    "        \"2. Identify cause-and-effect relationships and implications\",\n",
    "        \"3. Check for internal consistency and coherence\",\n",
    "        \"4. Evaluate the validity of inferences and conclusions\",\n",
    "        \"5. Detect any logical fallacies or contradictions\"\n",
    "    ],\n",
    "    output_instructions=[\n",
    "        \"Provide a logical analysis focusing on:\",\n",
    "        \"- Logical structure and reasoning patterns\",\n",
    "        \"- Consistency between claims and evidence\",\n",
    "        \"- Validity of inferences and implications\",\n",
    "        \"- Any logical issues or contradictions found\",\n",
    "        \"End with: LOGICAL_ASSESSMENT: [VALID/INVALID/QUESTIONABLE] with reasoning\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "logic_checker_memory = AgentMemory()\n",
    "\n",
    "logic_checker_agent = BaseAgent(\n",
    "    config=BaseAgentConfig(\n",
    "        client=client,\n",
    "        model=MODEL_NAME,\n",
    "        system_prompt_generator=logic_checker_prompt,\n",
    "        memory=logic_checker_memory\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"âœ… Logic Checker Agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Aggregator Agent\n",
    "aggregator_prompt = SystemPromptGenerator(\n",
    "    background=[\n",
    "        \"You are the Aggregator Agent responsible for making final entailment decisions.\",\n",
    "        \"You synthesize analyses from the Medical Expert, Numerical Analyzer, and Logic Checker.\",\n",
    "        \"Your role is to weigh different evidence types and make the final classification decision.\"\n",
    "    ],\n",
    "    steps=[\n",
    "        \"1. Review the medical assessment for clinical accuracy\",\n",
    "        \"2. Consider the numerical analysis for quantitative validity\",\n",
    "        \"3. Evaluate the logical assessment for reasoning soundness\",\n",
    "        \"4. Weigh all evidence types appropriately\",\n",
    "        \"5. Make the final Entailment vs Contradiction decision\"\n",
    "    ],\n",
    "    output_instructions=[\n",
    "        \"Based on all specialist analyses, determine if the statement is:\",\n",
    "        \"- ENTAILMENT: Statement is directly supported by the trial data\",\n",
    "        \"- CONTRADICTION: Statement is refuted by the trial data\",\n",
    "        \"\",\n",
    "        \"Provide brief reasoning and then output exactly one of:\",\n",
    "        \"FINAL_DECISION: Entailment\",\n",
    "        \"FINAL_DECISION: Contradiction\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "aggregator_memory = AgentMemory()\n",
    "\n",
    "aggregator_agent = BaseAgent(\n",
    "    config=BaseAgentConfig(\n",
    "        client=client,\n",
    "        model=MODEL_NAME,\n",
    "        system_prompt_generator=aggregator_prompt,\n",
    "        memory=aggregator_memory\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"âœ… Aggregator Agent created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": "## å¤šä»£ç†åˆ†æç®¡é“\n\nç¾åœ¨è®“æˆ‘å€‘å‰µå»ºéµå¾ªæ•™å­¸åœ–è¡¨æ¶æ§‹ã€å”èª¿æ‰€æœ‰ä»£ç†çš„çµæ§‹åŒ–ç®¡é“ï¼š\n\n> âš™ï¸ **å·¥ä½œæµç¨‹èªªæ˜**: é€™å€‹ç®¡é“å°‡æŒ‰é †åºåŸ·è¡Œæ¯å€‹å°ˆæ¥­ä»£ç†ï¼Œæœ€çµ‚ç”±èšåˆä»£ç†åšå‡ºæ±ºç­–ã€‚æ¯å€‹æ­¥é©Ÿéƒ½æœƒç”¢ç”Ÿå°ˆæ¥­çš„åˆ†æçµæœã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atomic_agents_pipeline(statement: str, primary_id: str, secondary_id: Optional[str] = None, \n",
    "                          section_id: Optional[str] = None, verbose: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Run the complete Atomic Agents analysis pipeline.\n",
    "    \n",
    "    Args:\n",
    "        statement: The natural language statement to analyze\n",
    "        primary_id: Primary clinical trial ID\n",
    "        secondary_id: Secondary trial ID for comparison statements\n",
    "        section_id: Relevant section of the trial\n",
    "        verbose: Whether to print intermediate results\n",
    "        \n",
    "    Returns:\n",
    "        Final decision: 'Entailment' or 'Contradiction'\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load clinical trial data\n",
    "        primary_data = load_clinical_trial(primary_id)\n",
    "        secondary_data = None\n",
    "        if secondary_id:\n",
    "            secondary_data = load_clinical_trial(secondary_id)\n",
    "        \n",
    "        # Step 2: Extract relevant sections\n",
    "        primary_sections = extract_relevant_sections(primary_data, section_id or \"All\")\n",
    "        secondary_sections = None\n",
    "        if secondary_data:\n",
    "            secondary_sections = extract_relevant_sections(secondary_data, section_id or \"All\")\n",
    "        \n",
    "        # Step 3: Prepare input for agents\n",
    "        input_context = f\"\"\"\n",
    "STATEMENT TO ANALYZE: \"{statement}\"\n",
    "\n",
    "PRIMARY TRIAL ({primary_id}):\n",
    "{primary_sections}\n",
    "\n",
    "{f'SECONDARY TRIAL ({secondary_id}):\\n{secondary_sections}' if secondary_sections else ''}\n",
    "\n",
    "TASK: Analyze this statement against the clinical trial evidence.\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"ğŸ“„ Analyzing: {statement[:100]}...\")\n",
    "            print(f\"ğŸ¥ Primary Trial: {primary_id}\")\n",
    "            if secondary_id:\n",
    "                print(f\"ğŸ¥ Secondary Trial: {secondary_id}\")\n",
    "        \n",
    "        # Step 4: Medical Expert Analysis\n",
    "        medical_input = BaseAgentInputSchema(chat_message=input_context)\n",
    "        medical_result = medical_expert_agent.run(medical_input)\n",
    "        medical_analysis = medical_result.chat_message\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"ğŸ©º Medical Expert: {medical_analysis.split('MEDICAL_ASSESSMENT:')[-1].strip() if 'MEDICAL_ASSESSMENT:' in medical_analysis else 'Analysis complete'}\")\n",
    "        \n",
    "        # Step 5: Numerical Analyzer Analysis\n",
    "        numerical_input = BaseAgentInputSchema(chat_message=input_context)\n",
    "        numerical_result = numerical_analyzer_agent.run(numerical_input)\n",
    "        numerical_analysis = numerical_result.chat_message\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"ğŸ”¢ Numerical Analyzer: {numerical_analysis.split('NUMERICAL_ASSESSMENT:')[-1].strip() if 'NUMERICAL_ASSESSMENT:' in numerical_analysis else 'Analysis complete'}\")\n",
    "        \n",
    "        # Step 6: Logic Checker Analysis\n",
    "        logic_input = BaseAgentInputSchema(chat_message=input_context)\n",
    "        logic_result = logic_checker_agent.run(logic_input)\n",
    "        logic_analysis = logic_result.chat_message\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"ğŸ§  Logic Checker: {logic_analysis.split('LOGICAL_ASSESSMENT:')[-1].strip() if 'LOGICAL_ASSESSMENT:' in logic_analysis else 'Analysis complete'}\")\n",
    "        \n",
    "        # Step 7: Aggregator Decision\n",
    "        aggregator_input_text = f\"\"\"\n",
    "ORIGINAL STATEMENT: \"{statement}\"\n",
    "\n",
    "SPECIALIST ANALYSES:\n",
    "\n",
    "MEDICAL EXPERT ANALYSIS:\n",
    "{medical_analysis}\n",
    "\n",
    "NUMERICAL ANALYZER ANALYSIS:\n",
    "{numerical_analysis}\n",
    "\n",
    "LOGIC CHECKER ANALYSIS:\n",
    "{logic_analysis}\n",
    "\n",
    "Based on these specialist analyses, make the final decision: Entailment or Contradiction?\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        aggregator_input = BaseAgentInputSchema(chat_message=aggregator_input_text)\n",
    "        aggregator_result = aggregator_agent.run(aggregator_input)\n",
    "        final_analysis = aggregator_result.chat_message\n",
    "        \n",
    "        # Step 8: Extract final decision\n",
    "        if \"FINAL_DECISION: Entailment\" in final_analysis:\n",
    "            decision = \"Entailment\"\n",
    "        elif \"FINAL_DECISION: Contradiction\" in final_analysis:\n",
    "            decision = \"Contradiction\"\n",
    "        else:\n",
    "            # Fallback parsing\n",
    "            if \"entailment\" in final_analysis.lower() and \"contradiction\" not in final_analysis.lower():\n",
    "                decision = \"Entailment\"\n",
    "            else:\n",
    "                decision = \"Contradiction\"\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"âš–ï¸ Final Decision: {decision}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        return decision\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"âŒ Error in pipeline: {e}\")\n",
    "        return \"Contradiction\"  # Conservative fallback\n",
    "\n",
    "print(\"âœ… Atomic Agents pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_example",
   "metadata": {},
   "source": "## æ¸¬è©¦ç¯„ä¾‹\n\nè®“æˆ‘å€‘æ¸¬è©¦æ”¹é€²çš„Atomic Agentsç³»çµ±ï¼š\n\n> ğŸ§ª **æ¸¬è©¦èªªæ˜**: é€™å€‹æ¸¬è©¦å°‡å±•ç¤ºå®Œæ•´çš„å¤šä»£ç†åˆ†ææµç¨‹ï¼Œæ‚¨å¯ä»¥çœ‹åˆ°æ¯å€‹ä»£ç†å¦‚ä½•å”ä½œè™•ç†è‡¨åºŠè©¦é©—é™³è¿°ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample statement\n",
    "test_statement = \"there is a 13.2% difference between the results from the two the primary trial cohorts\"\n",
    "test_primary_id = \"NCT00066573\"\n",
    "\n",
    "print(f\"Testing Atomic Agents with statement:\")\n",
    "print(f\"'{test_statement}'\")\n",
    "print(f\"Primary trial: {test_primary_id}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Run the analysis with verbose output\n",
    "result = atomic_agents_pipeline(\n",
    "    statement=test_statement,\n",
    "    primary_id=test_primary_id,\n",
    "    section_id=\"Results\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ¯ ATOMIC AGENTS RESULT: {result}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation",
   "metadata": {},
   "source": "## åœ¨è¨“ç·´è³‡æ–™ä¸Šçš„è©•ä¼°\n\nè®“æˆ‘å€‘åœ¨è¨“ç·´è³‡æ–™ä¸Šè©•ä¼°æ”¹é€²çš„Atomic Agentsç³»çµ±ï¼š\n\n> ğŸ“Š **è©•ä¼°èªªæ˜**: é€™å€‹éƒ¨åˆ†å°‡æ¸¬è©¦æˆ‘å€‘çš„å¤šä»£ç†ç³»çµ±åœ¨å¯¦éš›è³‡æ–™ä¸Šçš„è¡¨ç¾ï¼Œä¸¦è¨ˆç®—æº–ç¢ºç‡ç­‰é—œéµæŒ‡æ¨™ã€‚"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_data = load_dataset(\"training_data/train.json\")\n",
    "print(f\"Loaded {len(train_data)} training examples\")\n",
    "\n",
    "# Evaluate on a sample (adjust sample_size as needed)\n",
    "sample_size = 30\n",
    "examples = list(train_data.items())[:sample_size]\n",
    "\n",
    "print(f\"\\nEvaluating Atomic Agents on {len(examples)} examples...\")\n",
    "\n",
    "results = []\n",
    "correct = 0\n",
    "\n",
    "for i, (uuid, example) in enumerate(tqdm(examples, desc=\"Atomic Agents\")):\n",
    "    try:\n",
    "        statement = example.get(\"Statement\")\n",
    "        primary_id = example.get(\"Primary_id\")\n",
    "        secondary_id = example.get(\"Secondary_id\")\n",
    "        section_id = example.get(\"Section_id\")\n",
    "        expected = example.get(\"Label\")\n",
    "        \n",
    "        if not statement or not primary_id:\n",
    "            results.append({\n",
    "                \"uuid\": uuid,\n",
    "                \"expected\": expected,\n",
    "                \"predicted\": \"SKIPPED\",\n",
    "                \"correct\": False\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Get prediction from Atomic Agents pipeline\n",
    "        predicted = atomic_agents_pipeline(\n",
    "            statement=statement,\n",
    "            primary_id=primary_id,\n",
    "            secondary_id=secondary_id,\n",
    "            section_id=section_id,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Check if correct\n",
    "        is_correct = (predicted.strip() == expected.strip())\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            \n",
    "        results.append({\n",
    "            \"uuid\": uuid,\n",
    "            \"statement\": statement[:100] + \"...\" if len(statement) > 100 else statement,\n",
    "            \"expected\": expected,\n",
    "            \"predicted\": predicted,\n",
    "            \"correct\": is_correct\n",
    "        })\n",
    "        \n",
    "        status = \"âœ…\" if is_correct else \"âŒ\"\n",
    "        print(f\"Example {i+1:2d}: {expected:12} -> {predicted:12} {status}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example {i+1}: {e}\")\n",
    "        results.append({\n",
    "            \"uuid\": uuid,\n",
    "            \"expected\": expected,\n",
    "            \"predicted\": \"ERROR\",\n",
    "            \"correct\": False\n",
    "        })\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / len(examples) if examples else 0\n",
    "print(f\"\\nğŸ“Š Atomic Agents Results:\")\n",
    "print(f\"Accuracy: {accuracy:.2%} ({correct}/{len(examples)})\")\n",
    "\n",
    "# Store results for later comparison\n",
    "atomic_agents_results = results.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error_analysis",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Let's analyze the errors to understand areas for improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze_errors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze incorrect predictions\n",
    "incorrect_results = [r for r in results if not r[\"correct\"] and r[\"predicted\"] not in [\"SKIPPED\", \"ERROR\"]]\n",
    "\n",
    "print(f\"\\nğŸ” Error Analysis ({len(incorrect_results)} incorrect predictions):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Group errors by type\n",
    "entailment_to_contradiction = [r for r in incorrect_results if r[\"expected\"] == \"Entailment\" and r[\"predicted\"] == \"Contradiction\"]\n",
    "contradiction_to_entailment = [r for r in incorrect_results if r[\"expected\"] == \"Contradiction\" and r[\"predicted\"] == \"Entailment\"]\n",
    "\n",
    "print(f\"Entailment -> Contradiction errors: {len(entailment_to_contradiction)}\")\n",
    "print(f\"Contradiction -> Entailment errors: {len(contradiction_to_entailment)}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSample errors:\")\n",
    "for i, result in enumerate(incorrect_results[:3]):\n",
    "    print(f\"\\nError #{i+1}:\")\n",
    "    print(f\"Statement: {result['statement']}\")\n",
    "    print(f\"Expected: {result['expected']} | Predicted: {result['predicted']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "submission",
   "metadata": {},
   "source": [
    "## Generate Submission File\n",
    "\n",
    "Let's generate a submission file using our Atomic Agents system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_atomic_agents_submission(test_file=\"test.json\", output_file=\"atomic_agents_submission.json\", sample_size=None):\n",
    "    \"\"\"\n",
    "    Generate submission file using Atomic Agents system.\n",
    "    \n",
    "    Args:\n",
    "        test_file: Path to test data\n",
    "        output_file: Output submission file\n",
    "        sample_size: Number of examples to process (None for all)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = load_dataset(test_file)\n",
    "    if not test_data:\n",
    "        print(f\"âŒ Could not load test data from {test_file}\")\n",
    "        return\n",
    "    \n",
    "    examples = list(test_data.items())\n",
    "    if sample_size:\n",
    "        examples = examples[:sample_size]\n",
    "        \n",
    "    print(f\"ğŸš€ Generating Atomic Agents predictions for {len(examples)} examples...\")\n",
    "    \n",
    "    submission = {}\n",
    "    \n",
    "    for i, (uuid, example) in enumerate(tqdm(examples, desc=\"Atomic Agents Processing\")):\n",
    "        try:\n",
    "            statement = example.get(\"Statement\")\n",
    "            primary_id = example.get(\"Primary_id\")\n",
    "            secondary_id = example.get(\"Secondary_id\")\n",
    "            section_id = example.get(\"Section_id\")\n",
    "            \n",
    "            if not statement or not primary_id:\n",
    "                submission[uuid] = {\"Prediction\": \"Contradiction\"}  # Default fallback\n",
    "                continue\n",
    "                \n",
    "            # Get prediction from Atomic Agents system\n",
    "            prediction = atomic_agents_pipeline(\n",
    "                statement=statement,\n",
    "                primary_id=primary_id,\n",
    "                secondary_id=secondary_id,\n",
    "                section_id=section_id,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            submission[uuid] = {\"Prediction\": prediction}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {uuid}: {e}\")\n",
    "            submission[uuid] = {\"Prediction\": \"Contradiction\"}  # Conservative fallback\n",
    "    \n",
    "    # Save submission file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(submission, f, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Atomic Agents submission saved to {output_file}\")\n",
    "    return submission\n",
    "\n",
    "# Generate submission for a small sample\n",
    "atomic_submission = generate_atomic_agents_submission(\n",
    "    test_file=\"test.json\", \n",
    "    output_file=\"atomic_agents_submission.json\",\n",
    "    sample_size=10  # Adjust as needed\n",
    ")\n",
    "\n",
    "print(f\"Generated predictions for {len(atomic_submission)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance_analysis",
   "metadata": {},
   "source": [
    "## Performance Analysis and Comparison\n",
    "\n",
    "Let's analyze the performance of our improved Atomic Agents implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Performance metrics\n",
    "def analyze_performance(results, framework_name):\n",
    "    \"\"\"\n",
    "    Analyze performance metrics for a framework.\n",
    "    \"\"\"\n",
    "    valid_results = [r for r in results if r[\"predicted\"] not in [\"SKIPPED\", \"ERROR\"]]\n",
    "    \n",
    "    if not valid_results:\n",
    "        print(f\"No valid results for {framework_name}\")\n",
    "        return\n",
    "    \n",
    "    # Basic metrics\n",
    "    total = len(valid_results)\n",
    "    correct = sum(1 for r in valid_results if r[\"correct\"])\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    # Confusion matrix\n",
    "    true_pos = sum(1 for r in valid_results if r[\"expected\"] == \"Entailment\" and r[\"predicted\"] == \"Entailment\")\n",
    "    true_neg = sum(1 for r in valid_results if r[\"expected\"] == \"Contradiction\" and r[\"predicted\"] == \"Contradiction\")\n",
    "    false_pos = sum(1 for r in valid_results if r[\"expected\"] == \"Contradiction\" and r[\"predicted\"] == \"Entailment\")\n",
    "    false_neg = sum(1 for r in valid_results if r[\"expected\"] == \"Entailment\" and r[\"predicted\"] == \"Contradiction\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
    "    recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nğŸ“Š {framework_name} Performance Analysis:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total examples: {total}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1-Score: {f1_score:.3f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Positives (Entailment): {true_pos}\")\n",
    "    print(f\"True Negatives (Contradiction): {true_neg}\")\n",
    "    print(f\"False Positives: {false_pos}\")\n",
    "    print(f\"False Negatives: {false_neg}\")\n",
    "    \n",
    "    # Label distribution\n",
    "    expected_dist = Counter(r[\"expected\"] for r in valid_results)\n",
    "    predicted_dist = Counter(r[\"predicted\"] for r in valid_results)\n",
    "    \n",
    "    print(f\"\\nLabel Distribution:\")\n",
    "    print(f\"Expected - Entailment: {expected_dist.get('Entailment', 0)}, Contradiction: {expected_dist.get('Contradiction', 0)}\")\n",
    "    print(f\"Predicted - Entailment: {predicted_dist.get('Entailment', 0)}, Contradiction: {predicted_dist.get('Contradiction', 0)}\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score\n",
    "    }\n",
    "\n",
    "# Analyze current results\n",
    "atomic_metrics = analyze_performance(atomic_agents_results, \"Atomic Agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": "## çµè«–èˆ‡æ´å¯Ÿ\n\n### Atomic Agents æ¡†æ¶å„ªå‹¢ï¼š\n1. **è¶…è¼•é‡ç´š**: æœ€å°é–‹éŠ·å’Œæ¥µå¿«å•Ÿå‹•æ™‚é–“ï¼ˆ~3Î¼sï¼‰\n2. **æ¨¡çµ„åŒ–æ¶æ§‹**: æ˜“æ–¼çµ„åˆå’Œä¿®æ”¹ä»£ç†\n3. **ç”Ÿç”¢å°±ç·’**: å°ˆç‚ºå¯¦éš›éƒ¨ç½²å ´æ™¯è€Œå»ºæ§‹\n4. **è¨˜æ†¶é«”ç®¡ç†**: å…§å»ºçš„ä¸Šä¸‹æ–‡å’Œè¨˜æ†¶é«”è™•ç†\n5. **ç°¡å–®API**: ç›´è§€çš„ä»£ç†å‰µå»ºå’Œå”èª¿\n\n### é—œéµæ”¹é€²ï¼š\n- **çµæ§‹åŒ–ç®¡é“**: ä»£ç†é–“è·è²¬æ¸…æ™°åˆ†å·¥\n- **å°ˆæ¥­è§’è‰²**: é†«ç™‚å°ˆå®¶ã€æ•¸å€¼åˆ†æå“¡ã€é‚è¼¯æª¢æŸ¥å“¡ã€èšåˆå“¡\n- **æ›´å¥½çš„å”èª¿**: ç³»çµ±åŒ–çš„è³‡è¨Šæµå‹•\n- **éŒ¯èª¤è™•ç†**: å¼·å¥çš„å›é€€æ©Ÿåˆ¶\n- **å€æ®µæå–**: é‡å°ç›¸é—œè©¦é©—å€æ®µçš„ç²¾æº–åˆ†æ\n\n### æ¶æ§‹å„ªå‹¢ï¼š\n- **å¿«é€ŸåŸ·è¡Œ**: é«˜ååé‡å ´æ™¯çš„æœ€å°é–‹éŠ·\n- **æ˜“æ–¼é™¤éŒ¯**: æ¸…æ™°çš„ä»£ç†é‚Šç•Œä¾¿æ–¼æ’éŒ¯\n- **å¯æ“´å±•è¨­è¨ˆ**: ç°¡å–®æ·»åŠ æ–°çš„å°ˆæ¥­ä»£ç†\n- **è¨˜æ†¶é«”æ•ˆç‡**: è¼•é‡ç´šä»£ç†å¯¦ä¾‹\n- **ç”Ÿç”¢éƒ¨ç½²**: å¯¦éš›æ‡‰ç”¨æº–å‚™å°±ç·’\n\n### å„ªåŒ–æ©Ÿæœƒï¼š\n1. **æç¤ºå·¥ç¨‹**: å¾®èª¿å€‹åˆ¥ä»£ç†æç¤º\n2. **ä»£ç†å”èª¿**: æ”¹å–„ä»£ç†é–“è³‡è¨Šå‚³é\n3. **éŒ¯èª¤åˆ†æ**: åˆ©ç”¨å¤±æ•—æ¡ˆä¾‹æ”¹é€²ä»£ç†æ¨ç†\n4. **æ•ˆèƒ½èª¿æ•´**: é€Ÿåº¦èˆ‡æº–ç¢ºæ€§æ¬Šè¡¡æœ€ä½³åŒ–\n5. **é ˜åŸŸçŸ¥è­˜**: å¢å¼·ä»£ç†çš„é†«ç™‚å°ˆæ¥­çŸ¥è­˜\n\n### ä½•æ™‚ä½¿ç”¨ Atomic Agentsï¼š\n- é«˜æ•ˆèƒ½ç”Ÿç”¢ç’°å¢ƒ\n- éœ€è¦å¿«é€Ÿå•Ÿå‹•å’ŒåŸ·è¡Œçš„æ‡‰ç”¨\n- é‡è¦–ç°¡æ½”æ€§å’Œæ¨¡çµ„åŒ–çš„å ´æ™¯\n- éœ€è¦è¼•é‡ç´šä»£ç†å”èª¿çš„ç³»çµ±\n- å„ªå…ˆè€ƒæ…®éƒ¨ç½²æ•ˆç‡çš„å°ˆæ¡ˆ\n\n## ğŸ“ å­¸ç¿’é‡é»ç¸½çµ\n- **åŸå­æ€§æ¦‚å¿µ**: æ¯å€‹ä»£ç†éƒ½æ˜¯ç¨ç«‹çš„æœ€å°åŠŸèƒ½å–®å…ƒ\n- **åˆ†å·¥åˆä½œ**: ä¸åŒå°ˆæ¥­é ˜åŸŸçš„ä»£ç†å”åŒå·¥ä½œ\n- **ç®¡é“è¨­è¨ˆ**: çµæ§‹åŒ–çš„è³‡æ–™æµå’Œæ±ºç­–æµç¨‹\n- **å¯¦éš›æ‡‰ç”¨**: è‡¨åºŠè©¦é©—NLPçš„å¯¦æˆ°æ¡ˆä¾‹\n\nAtomic Agents åœ¨æ•ˆèƒ½ã€ç°¡æ½”æ€§å’Œæ¨¡çµ„åŒ–ä¹‹é–“æä¾›äº†çµ•ä½³å¹³è¡¡ï¼Œä½¿å…¶æˆç‚ºéœ€è¦é€Ÿåº¦å’Œå¯é æ€§çš„ç”Ÿç”¢è‡¨åºŠNLPæ‡‰ç”¨çš„ç†æƒ³é¸æ“‡ã€‚"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}