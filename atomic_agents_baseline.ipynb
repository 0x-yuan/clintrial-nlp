{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Atomic Agents Framework Baseline for Clinical Trial NLP\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to use the Atomic Agents framework to build a lightweight, high-performance multi-agent system for clinical trial natural language inference (NLI). Atomic Agents is designed for production use with extremely fast startup times (~3μs) and modular architecture.\n",
    "\n",
    "### Why Atomic Agents?\n",
    "- **Ultra-lightweight**: Minimal overhead and fast execution\n",
    "- **High modularity**: Easy to compose and modify agents\n",
    "- **Production-ready**: Built for real-world deployment\n",
    "- **Simple API**: Straightforward agent creation and coordination\n",
    "- **Memory management**: Built-in context and memory handling\n",
    "\n",
    "### Agent Architecture\n",
    "Following the teaching diagram, we implement a structured pipeline:\n",
    "1. **Medical Expert Agent**: Analyzes medical terminology and concepts\n",
    "2. **Numerical Analyzer Agent**: Processes quantitative data\n",
    "3. **Logic Checker Agent**: Validates logical relationships\n",
    "4. **Aggregator Agent**: Combines insights for final decision\n",
    "5. **Supervisor Coordination**: Manages the overall workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's set up our environment and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "print(\"✅ Environment loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependencies",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import instructor\n",
    "import openai\n",
    "from typing import Dict, List, Any, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Atomic Agents imports\n",
    "from atomic_agents.lib.components.agent_memory import AgentMemory\n",
    "from atomic_agents.agents.base_agent import BaseAgent, BaseAgentConfig, BaseAgentInputSchema\n",
    "from atomic_agents.lib.components.system_prompt_generator import SystemPromptGenerator\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_utils",
   "metadata": {},
   "source": [
    "## Data Loading and Utilities\n",
    "\n",
    "Let's create utility functions for loading and processing clinical trial data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utils",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clinical_trial(trial_id: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load clinical trial data from JSON file.\n",
    "    \n",
    "    Args:\n",
    "        trial_id: The NCT identifier for the clinical trial\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing trial data or error information\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_path = os.path.join(\"training_data\", \"CT json\", f\"{trial_id}.json\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        return {\"error\": f\"Clinical trial {trial_id} not found\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error loading {trial_id}: {str(e)}\"}\n",
    "\n",
    "def load_dataset(filepath: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load training or test dataset.\n",
    "    \n",
    "    Args:\n",
    "        filepath: Path to the JSON dataset file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing the dataset\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return {}\n",
    "\n",
    "def extract_relevant_sections(trial_data: Dict[str, Any], section_id: str) -> str:\n",
    "    \"\"\"Extract relevant sections from trial data based on section_id.\n",
    "    \n",
    "    Args:\n",
    "        trial_data: Clinical trial data dictionary\n",
    "        section_id: Target section (Eligibility, Intervention, Results, Adverse Events)\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string containing relevant section data\n",
    "    \"\"\"\n",
    "    if \"error\" in trial_data:\n",
    "        return f\"Error: {trial_data['error']}\"\n",
    "    \n",
    "    sections = {\n",
    "        \"Eligibility\": trial_data.get(\"Eligibility\", []),\n",
    "        \"Intervention\": trial_data.get(\"Intervention\", []),\n",
    "        \"Results\": trial_data.get(\"Results\", []),\n",
    "        \"Adverse Events\": trial_data.get(\"Adverse_Events\", [])\n",
    "    }\n",
    "    \n",
    "    # If specific section requested, return only that section\n",
    "    if section_id in sections:\n",
    "        section_data = sections[section_id]\n",
    "        if isinstance(section_data, list):\n",
    "            return \"\\n\".join(str(item) for item in section_data)\n",
    "        return str(section_data)\n",
    "    \n",
    "    # Otherwise, return all sections\n",
    "    result = []\n",
    "    for section_name, section_data in sections.items():\n",
    "        if section_data:\n",
    "            result.append(f\"{section_name}:\")\n",
    "            if isinstance(section_data, list):\n",
    "                result.extend([f\"  {item}\" for item in section_data])\n",
    "            else:\n",
    "                result.append(f\"  {section_data}\")\n",
    "    \n",
    "    return \"\\n\".join(result)\n",
    "\n",
    "# Test utilities\n",
    "sample_trial = load_clinical_trial(\"NCT00066573\")\n",
    "print(f\"✅ Data utilities ready. Sample trial: {sample_trial.get('Clinical Trial ID', 'Error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_setup",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "Set up the OpenAI client with instructor for structured outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client with instructor\n",
    "client = instructor.from_openai(openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "))\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "print(f\"✅ OpenAI client configured with model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agent_definitions",
   "metadata": {},
   "source": [
    "## Agent Definitions\n",
    "\n",
    "Now let's define each specialized agent using Atomic Agents framework. Each agent has a specific role and expertise domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical_expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Medical Expert Agent\n",
    "medical_expert_prompt = SystemPromptGenerator(\n",
    "    background=[\n",
    "        \"You are a Medical Expert Agent specializing in clinical trial analysis.\",\n",
    "        \"You have deep knowledge of medical terminology, clinical concepts, and trial procedures.\",\n",
    "        \"Your role is to analyze statements from a medical perspective and identify relevant clinical insights.\"\n",
    "    ],\n",
    "    steps=[\n",
    "        \"1. Analyze the medical terminology and concepts in the statement\",\n",
    "        \"2. Identify key clinical elements and their significance\",\n",
    "        \"3. Review the clinical trial data for medical accuracy\",\n",
    "        \"4. Assess whether the medical claims align with the trial evidence\",\n",
    "        \"5. Provide medical reasoning and clinical context\"\n",
    "    ],\n",
    "    output_instructions=[\n",
    "        \"Provide a clear medical analysis focusing on:\",\n",
    "        \"- Medical terminology accuracy\",\n",
    "        \"- Clinical relevance and significance\",\n",
    "        \"- Alignment with medical evidence in the trial\",\n",
    "        \"- Any medical concerns or considerations\",\n",
    "        \"End with: MEDICAL_ASSESSMENT: [SUPPORTS/CONTRADICTS/UNCLEAR] based on medical evidence\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "medical_expert_memory = AgentMemory()\n",
    "\n",
    "medical_expert_agent = BaseAgent(\n",
    "    config=BaseAgentConfig(\n",
    "        client=client,\n",
    "        model=MODEL_NAME,\n",
    "        system_prompt_generator=medical_expert_prompt,\n",
    "        memory=medical_expert_memory\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"✅ Medical Expert Agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical_analyzer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Numerical Analyzer Agent\n",
    "numerical_analyzer_prompt = SystemPromptGenerator(\n",
    "    background=[\n",
    "        \"You are a Numerical Analyzer Agent specializing in quantitative analysis of clinical trials.\",\n",
    "        \"You excel at processing numbers, statistics, percentages, and numerical relationships.\",\n",
    "        \"Your role is to verify numerical claims and perform statistical analysis.\"\n",
    "    ],\n",
    "    steps=[\n",
    "        \"1. Extract all numerical values, percentages, and statistics from the statement\",\n",
    "        \"2. Identify corresponding numbers in the clinical trial data\",\n",
    "        \"3. Perform calculations to verify numerical relationships\",\n",
    "        \"4. Check for statistical significance and clinical meaningfulness\",\n",
    "        \"5. Identify any numerical inconsistencies or errors\"\n",
    "    ],\n",
    "    output_instructions=[\n",
    "        \"Provide a detailed numerical analysis including:\",\n",
    "        \"- All numerical values extracted from the statement\",\n",
    "        \"- Corresponding values found in trial data\",\n",
    "        \"- Calculations performed to verify claims\",\n",
    "        \"- Assessment of numerical accuracy\",\n",
    "        \"End with: NUMERICAL_ASSESSMENT: [ACCURATE/INACCURATE/PARTIALLY_ACCURATE] with confidence level\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "numerical_analyzer_memory = AgentMemory()\n",
    "\n",
    "numerical_analyzer_agent = BaseAgent(\n",
    "    config=BaseAgentConfig(\n",
    "        client=client,\n",
    "        model=MODEL_NAME,\n",
    "        system_prompt_generator=numerical_analyzer_prompt,\n",
    "        memory=numerical_analyzer_memory\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"✅ Numerical Analyzer Agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logic_checker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Logic Checker Agent\n",
    "logic_checker_prompt = SystemPromptGenerator(\n",
    "    background=[\n",
    "        \"You are a Logic Checker Agent responsible for validating logical reasoning and consistency.\",\n",
    "        \"You specialize in identifying logical relationships, contradictions, and reasoning patterns.\",\n",
    "        \"Your role is to ensure logical soundness and coherence in claims and evidence.\"\n",
    "    ],\n",
    "    steps=[\n",
    "        \"1. Analyze the logical structure of the statement\",\n",
    "        \"2. Identify cause-and-effect relationships and implications\",\n",
    "        \"3. Check for internal consistency and coherence\",\n",
    "        \"4. Evaluate the validity of inferences and conclusions\",\n",
    "        \"5. Detect any logical fallacies or contradictions\"\n",
    "    ],\n",
    "    output_instructions=[\n",
    "        \"Provide a logical analysis focusing on:\",\n",
    "        \"- Logical structure and reasoning patterns\",\n",
    "        \"- Consistency between claims and evidence\",\n",
    "        \"- Validity of inferences and implications\",\n",
    "        \"- Any logical issues or contradictions found\",\n",
    "        \"End with: LOGICAL_ASSESSMENT: [VALID/INVALID/QUESTIONABLE] with reasoning\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "logic_checker_memory = AgentMemory()\n",
    "\n",
    "logic_checker_agent = BaseAgent(\n",
    "    config=BaseAgentConfig(\n",
    "        client=client,\n",
    "        model=MODEL_NAME,\n",
    "        system_prompt_generator=logic_checker_prompt,\n",
    "        memory=logic_checker_memory\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"✅ Logic Checker Agent created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Aggregator Agent\n",
    "aggregator_prompt = SystemPromptGenerator(\n",
    "    background=[\n",
    "        \"You are the Aggregator Agent responsible for making final entailment decisions.\",\n",
    "        \"You synthesize analyses from the Medical Expert, Numerical Analyzer, and Logic Checker.\",\n",
    "        \"Your role is to weigh different evidence types and make the final classification decision.\"\n",
    "    ],\n",
    "    steps=[\n",
    "        \"1. Review the medical assessment for clinical accuracy\",\n",
    "        \"2. Consider the numerical analysis for quantitative validity\",\n",
    "        \"3. Evaluate the logical assessment for reasoning soundness\",\n",
    "        \"4. Weigh all evidence types appropriately\",\n",
    "        \"5. Make the final Entailment vs Contradiction decision\"\n",
    "    ],\n",
    "    output_instructions=[\n",
    "        \"Based on all specialist analyses, determine if the statement is:\",\n",
    "        \"- ENTAILMENT: Statement is directly supported by the trial data\",\n",
    "        \"- CONTRADICTION: Statement is refuted by the trial data\",\n",
    "        \"\",\n",
    "        \"Provide brief reasoning and then output exactly one of:\",\n",
    "        \"FINAL_DECISION: Entailment\",\n",
    "        \"FINAL_DECISION: Contradiction\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "aggregator_memory = AgentMemory()\n",
    "\n",
    "aggregator_agent = BaseAgent(\n",
    "    config=BaseAgentConfig(\n",
    "        client=client,\n",
    "        model=MODEL_NAME,\n",
    "        system_prompt_generator=aggregator_prompt,\n",
    "        memory=aggregator_memory\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"✅ Aggregator Agent created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline",
   "metadata": {},
   "source": [
    "## Multi-Agent Analysis Pipeline\n",
    "\n",
    "Now let's create our structured pipeline that coordinates all agents following the teaching diagram architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analysis_pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atomic_agents_pipeline(statement: str, primary_id: str, secondary_id: Optional[str] = None, \n",
    "                          section_id: Optional[str] = None, verbose: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Run the complete Atomic Agents analysis pipeline.\n",
    "    \n",
    "    Args:\n",
    "        statement: The natural language statement to analyze\n",
    "        primary_id: Primary clinical trial ID\n",
    "        secondary_id: Secondary trial ID for comparison statements\n",
    "        section_id: Relevant section of the trial\n",
    "        verbose: Whether to print intermediate results\n",
    "        \n",
    "    Returns:\n",
    "        Final decision: 'Entailment' or 'Contradiction'\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load clinical trial data\n",
    "        primary_data = load_clinical_trial(primary_id)\n",
    "        secondary_data = None\n",
    "        if secondary_id:\n",
    "            secondary_data = load_clinical_trial(secondary_id)\n",
    "        \n",
    "        # Step 2: Extract relevant sections\n",
    "        primary_sections = extract_relevant_sections(primary_data, section_id or \"All\")\n",
    "        secondary_sections = None\n",
    "        if secondary_data:\n",
    "            secondary_sections = extract_relevant_sections(secondary_data, section_id or \"All\")\n",
    "        \n",
    "        # Step 3: Prepare input for agents\n",
    "        input_context = f\"\"\"\n",
    "STATEMENT TO ANALYZE: \"{statement}\"\n",
    "\n",
    "PRIMARY TRIAL ({primary_id}):\n",
    "{primary_sections}\n",
    "\n",
    "{f'SECONDARY TRIAL ({secondary_id}):\\n{secondary_sections}' if secondary_sections else ''}\n",
    "\n",
    "TASK: Analyze this statement against the clinical trial evidence.\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"📄 Analyzing: {statement[:100]}...\")\n",
    "            print(f\"🏥 Primary Trial: {primary_id}\")\n",
    "            if secondary_id:\n",
    "                print(f\"🏥 Secondary Trial: {secondary_id}\")\n",
    "        \n",
    "        # Step 4: Medical Expert Analysis\n",
    "        medical_input = BaseAgentInputSchema(chat_message=input_context)\n",
    "        medical_result = medical_expert_agent.run(medical_input)\n",
    "        medical_analysis = medical_result.chat_message\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"🩺 Medical Expert: {medical_analysis.split('MEDICAL_ASSESSMENT:')[-1].strip() if 'MEDICAL_ASSESSMENT:' in medical_analysis else 'Analysis complete'}\")\n",
    "        \n",
    "        # Step 5: Numerical Analyzer Analysis\n",
    "        numerical_input = BaseAgentInputSchema(chat_message=input_context)\n",
    "        numerical_result = numerical_analyzer_agent.run(numerical_input)\n",
    "        numerical_analysis = numerical_result.chat_message\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"🔢 Numerical Analyzer: {numerical_analysis.split('NUMERICAL_ASSESSMENT:')[-1].strip() if 'NUMERICAL_ASSESSMENT:' in numerical_analysis else 'Analysis complete'}\")\n",
    "        \n",
    "        # Step 6: Logic Checker Analysis\n",
    "        logic_input = BaseAgentInputSchema(chat_message=input_context)\n",
    "        logic_result = logic_checker_agent.run(logic_input)\n",
    "        logic_analysis = logic_result.chat_message\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"🧠 Logic Checker: {logic_analysis.split('LOGICAL_ASSESSMENT:')[-1].strip() if 'LOGICAL_ASSESSMENT:' in logic_analysis else 'Analysis complete'}\")\n",
    "        \n",
    "        # Step 7: Aggregator Decision\n",
    "        aggregator_input_text = f\"\"\"\n",
    "ORIGINAL STATEMENT: \"{statement}\"\n",
    "\n",
    "SPECIALIST ANALYSES:\n",
    "\n",
    "MEDICAL EXPERT ANALYSIS:\n",
    "{medical_analysis}\n",
    "\n",
    "NUMERICAL ANALYZER ANALYSIS:\n",
    "{numerical_analysis}\n",
    "\n",
    "LOGIC CHECKER ANALYSIS:\n",
    "{logic_analysis}\n",
    "\n",
    "Based on these specialist analyses, make the final decision: Entailment or Contradiction?\n",
    "        \"\"\".strip()\n",
    "        \n",
    "        aggregator_input = BaseAgentInputSchema(chat_message=aggregator_input_text)\n",
    "        aggregator_result = aggregator_agent.run(aggregator_input)\n",
    "        final_analysis = aggregator_result.chat_message\n",
    "        \n",
    "        # Step 8: Extract final decision\n",
    "        if \"FINAL_DECISION: Entailment\" in final_analysis:\n",
    "            decision = \"Entailment\"\n",
    "        elif \"FINAL_DECISION: Contradiction\" in final_analysis:\n",
    "            decision = \"Contradiction\"\n",
    "        else:\n",
    "            # Fallback parsing\n",
    "            if \"entailment\" in final_analysis.lower() and \"contradiction\" not in final_analysis.lower():\n",
    "                decision = \"Entailment\"\n",
    "            else:\n",
    "                decision = \"Contradiction\"\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"⚖️ Final Decision: {decision}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        return decision\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"❌ Error in pipeline: {e}\")\n",
    "        return \"Contradiction\"  # Conservative fallback\n",
    "\n",
    "print(\"✅ Atomic Agents pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test_example",
   "metadata": {},
   "source": [
    "## Test Example\n",
    "\n",
    "Let's test our improved Atomic Agents system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample statement\n",
    "test_statement = \"there is a 13.2% difference between the results from the two the primary trial cohorts\"\n",
    "test_primary_id = \"NCT00066573\"\n",
    "\n",
    "print(f\"Testing Atomic Agents with statement:\")\n",
    "print(f\"'{test_statement}'\")\n",
    "print(f\"Primary trial: {test_primary_id}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Run the analysis with verbose output\n",
    "result = atomic_agents_pipeline(\n",
    "    statement=test_statement,\n",
    "    primary_id=test_primary_id,\n",
    "    section_id=\"Results\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n🎯 ATOMIC AGENTS RESULT: {result}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation",
   "metadata": {},
   "source": [
    "## Evaluation on Training Data\n",
    "\n",
    "Let's evaluate our improved Atomic Agents system on training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train_evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_data = load_dataset(\"training_data/train.json\")\n",
    "print(f\"Loaded {len(train_data)} training examples\")\n",
    "\n",
    "# Evaluate on a sample (adjust sample_size as needed)\n",
    "sample_size = 30\n",
    "examples = list(train_data.items())[:sample_size]\n",
    "\n",
    "print(f\"\\nEvaluating Atomic Agents on {len(examples)} examples...\")\n",
    "\n",
    "results = []\n",
    "correct = 0\n",
    "\n",
    "for i, (uuid, example) in enumerate(tqdm(examples, desc=\"Atomic Agents\")):\n",
    "    try:\n",
    "        statement = example.get(\"Statement\")\n",
    "        primary_id = example.get(\"Primary_id\")\n",
    "        secondary_id = example.get(\"Secondary_id\")\n",
    "        section_id = example.get(\"Section_id\")\n",
    "        expected = example.get(\"Label\")\n",
    "        \n",
    "        if not statement or not primary_id:\n",
    "            results.append({\n",
    "                \"uuid\": uuid,\n",
    "                \"expected\": expected,\n",
    "                \"predicted\": \"SKIPPED\",\n",
    "                \"correct\": False\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        # Get prediction from Atomic Agents pipeline\n",
    "        predicted = atomic_agents_pipeline(\n",
    "            statement=statement,\n",
    "            primary_id=primary_id,\n",
    "            secondary_id=secondary_id,\n",
    "            section_id=section_id,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Check if correct\n",
    "        is_correct = (predicted.strip() == expected.strip())\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "            \n",
    "        results.append({\n",
    "            \"uuid\": uuid,\n",
    "            \"statement\": statement[:100] + \"...\" if len(statement) > 100 else statement,\n",
    "            \"expected\": expected,\n",
    "            \"predicted\": predicted,\n",
    "            \"correct\": is_correct\n",
    "        })\n",
    "        \n",
    "        status = \"✅\" if is_correct else \"❌\"\n",
    "        print(f\"Example {i+1:2d}: {expected:12} -> {predicted:12} {status}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example {i+1}: {e}\")\n",
    "        results.append({\n",
    "            \"uuid\": uuid,\n",
    "            \"expected\": expected,\n",
    "            \"predicted\": \"ERROR\",\n",
    "            \"correct\": False\n",
    "        })\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / len(examples) if examples else 0\n",
    "print(f\"\\n📊 Atomic Agents Results:\")\n",
    "print(f\"Accuracy: {accuracy:.2%} ({correct}/{len(examples)})\")\n",
    "\n",
    "# Store results for later comparison\n",
    "atomic_agents_results = results.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "error_analysis",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Let's analyze the errors to understand areas for improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze_errors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze incorrect predictions\n",
    "incorrect_results = [r for r in results if not r[\"correct\"] and r[\"predicted\"] not in [\"SKIPPED\", \"ERROR\"]]\n",
    "\n",
    "print(f\"\\n🔍 Error Analysis ({len(incorrect_results)} incorrect predictions):\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Group errors by type\n",
    "entailment_to_contradiction = [r for r in incorrect_results if r[\"expected\"] == \"Entailment\" and r[\"predicted\"] == \"Contradiction\"]\n",
    "contradiction_to_entailment = [r for r in incorrect_results if r[\"expected\"] == \"Contradiction\" and r[\"predicted\"] == \"Entailment\"]\n",
    "\n",
    "print(f\"Entailment -> Contradiction errors: {len(entailment_to_contradiction)}\")\n",
    "print(f\"Contradiction -> Entailment errors: {len(contradiction_to_entailment)}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSample errors:\")\n",
    "for i, result in enumerate(incorrect_results[:3]):\n",
    "    print(f\"\\nError #{i+1}:\")\n",
    "    print(f\"Statement: {result['statement']}\")\n",
    "    print(f\"Expected: {result['expected']} | Predicted: {result['predicted']}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "submission",
   "metadata": {},
   "source": [
    "## Generate Submission File\n",
    "\n",
    "Let's generate a submission file using our Atomic Agents system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_atomic_agents_submission(test_file=\"test.json\", output_file=\"atomic_agents_submission.json\", sample_size=None):\n",
    "    \"\"\"\n",
    "    Generate submission file using Atomic Agents system.\n",
    "    \n",
    "    Args:\n",
    "        test_file: Path to test data\n",
    "        output_file: Output submission file\n",
    "        sample_size: Number of examples to process (None for all)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = load_dataset(test_file)\n",
    "    if not test_data:\n",
    "        print(f\"❌ Could not load test data from {test_file}\")\n",
    "        return\n",
    "    \n",
    "    examples = list(test_data.items())\n",
    "    if sample_size:\n",
    "        examples = examples[:sample_size]\n",
    "        \n",
    "    print(f\"🚀 Generating Atomic Agents predictions for {len(examples)} examples...\")\n",
    "    \n",
    "    submission = {}\n",
    "    \n",
    "    for i, (uuid, example) in enumerate(tqdm(examples, desc=\"Atomic Agents Processing\")):\n",
    "        try:\n",
    "            statement = example.get(\"Statement\")\n",
    "            primary_id = example.get(\"Primary_id\")\n",
    "            secondary_id = example.get(\"Secondary_id\")\n",
    "            section_id = example.get(\"Section_id\")\n",
    "            \n",
    "            if not statement or not primary_id:\n",
    "                submission[uuid] = {\"Prediction\": \"Contradiction\"}  # Default fallback\n",
    "                continue\n",
    "                \n",
    "            # Get prediction from Atomic Agents system\n",
    "            prediction = atomic_agents_pipeline(\n",
    "                statement=statement,\n",
    "                primary_id=primary_id,\n",
    "                secondary_id=secondary_id,\n",
    "                section_id=section_id,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            submission[uuid] = {\"Prediction\": prediction}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {uuid}: {e}\")\n",
    "            submission[uuid] = {\"Prediction\": \"Contradiction\"}  # Conservative fallback\n",
    "    \n",
    "    # Save submission file\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(submission, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Atomic Agents submission saved to {output_file}\")\n",
    "    return submission\n",
    "\n",
    "# Generate submission for a small sample\n",
    "atomic_submission = generate_atomic_agents_submission(\n",
    "    test_file=\"test.json\", \n",
    "    output_file=\"atomic_agents_submission.json\",\n",
    "    sample_size=10  # Adjust as needed\n",
    ")\n",
    "\n",
    "print(f\"Generated predictions for {len(atomic_submission)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance_analysis",
   "metadata": {},
   "source": [
    "## Performance Analysis and Comparison\n",
    "\n",
    "Let's analyze the performance of our improved Atomic Agents implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance_metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Performance metrics\n",
    "def analyze_performance(results, framework_name):\n",
    "    \"\"\"\n",
    "    Analyze performance metrics for a framework.\n",
    "    \"\"\"\n",
    "    valid_results = [r for r in results if r[\"predicted\"] not in [\"SKIPPED\", \"ERROR\"]]\n",
    "    \n",
    "    if not valid_results:\n",
    "        print(f\"No valid results for {framework_name}\")\n",
    "        return\n",
    "    \n",
    "    # Basic metrics\n",
    "    total = len(valid_results)\n",
    "    correct = sum(1 for r in valid_results if r[\"correct\"])\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    \n",
    "    # Confusion matrix\n",
    "    true_pos = sum(1 for r in valid_results if r[\"expected\"] == \"Entailment\" and r[\"predicted\"] == \"Entailment\")\n",
    "    true_neg = sum(1 for r in valid_results if r[\"expected\"] == \"Contradiction\" and r[\"predicted\"] == \"Contradiction\")\n",
    "    false_pos = sum(1 for r in valid_results if r[\"expected\"] == \"Contradiction\" and r[\"predicted\"] == \"Entailment\")\n",
    "    false_neg = sum(1 for r in valid_results if r[\"expected\"] == \"Entailment\" and r[\"predicted\"] == \"Contradiction\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
    "    recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n📊 {framework_name} Performance Analysis:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total examples: {total}\")\n",
    "    print(f\"Accuracy: {accuracy:.2%} ({correct}/{total})\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1-Score: {f1_score:.3f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Positives (Entailment): {true_pos}\")\n",
    "    print(f\"True Negatives (Contradiction): {true_neg}\")\n",
    "    print(f\"False Positives: {false_pos}\")\n",
    "    print(f\"False Negatives: {false_neg}\")\n",
    "    \n",
    "    # Label distribution\n",
    "    expected_dist = Counter(r[\"expected\"] for r in valid_results)\n",
    "    predicted_dist = Counter(r[\"predicted\"] for r in valid_results)\n",
    "    \n",
    "    print(f\"\\nLabel Distribution:\")\n",
    "    print(f\"Expected - Entailment: {expected_dist.get('Entailment', 0)}, Contradiction: {expected_dist.get('Contradiction', 0)}\")\n",
    "    print(f\"Predicted - Entailment: {predicted_dist.get('Entailment', 0)}, Contradiction: {predicted_dist.get('Contradiction', 0)}\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1_score\n",
    "    }\n",
    "\n",
    "# Analyze current results\n",
    "atomic_metrics = analyze_performance(atomic_agents_results, \"Atomic Agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## Conclusion and Insights\n",
    "\n",
    "### Atomic Agents Framework Strengths:\n",
    "1. **Ultra-lightweight**: Minimal overhead and extremely fast startup (~3μs)\n",
    "2. **Modular architecture**: Easy to compose and modify agents\n",
    "3. **Production-ready**: Built for real-world deployment scenarios\n",
    "4. **Memory management**: Built-in context and memory handling\n",
    "5. **Simple API**: Straightforward agent creation and coordination\n",
    "\n",
    "### Key Improvements Made:\n",
    "- **Structured pipeline**: Clear separation of concerns between agents\n",
    "- **Specialized roles**: Medical Expert, Numerical Analyzer, Logic Checker, Aggregator\n",
    "- **Better coordination**: Systematic information flow between agents\n",
    "- **Error handling**: Robust fallback mechanisms\n",
    "- **Section extraction**: Targeted analysis of relevant trial sections\n",
    "\n",
    "### Architecture Benefits:\n",
    "- **Fast execution**: Minimal overhead for high-throughput scenarios\n",
    "- **Easy debugging**: Clear agent boundaries for troubleshooting\n",
    "- **Scalable design**: Simple to add new specialized agents\n",
    "- **Memory efficiency**: Lightweight agent instances\n",
    "- **Production deployment**: Ready for real-world applications\n",
    "\n",
    "### Optimization Opportunities:\n",
    "1. **Prompt engineering**: Fine-tune individual agent prompts\n",
    "2. **Agent coordination**: Improve information passing between agents\n",
    "3. **Error analysis**: Use failed cases to improve agent reasoning\n",
    "4. **Performance tuning**: Optimize for speed vs. accuracy trade-offs\n",
    "5. **Domain knowledge**: Enhance medical expertise in agents\n",
    "\n",
    "### When to Use Atomic Agents:\n",
    "- High-performance production environments\n",
    "- Applications requiring fast startup and execution\n",
    "- Scenarios where simplicity and modularity are important\n",
    "- Systems needing lightweight agent coordination\n",
    "- Projects prioritizing deployment efficiency\n",
    "\n",
    "Atomic Agents provides an excellent balance of performance, simplicity, and modularity, making it ideal for production clinical NLP applications where speed and reliability are crucial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",
   "name": "python3"
  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.8.10"\n  }\n },\n "nbformat": 4,\n "nbformat_minor": 5\n}