#!/usr/bin/env python
# coding: utf-8

# [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/0x-yuan/clintrial-nlp/blob/main/langchain_baseline.ipynb)
# 
# # LangChain/LangGraph æ¡†æ¶åŸºç·š - è‡¨åºŠè©¦é©— NLP
# 
# ## æ¦‚è¿°
# 
# æœ¬notebookå±•ç¤ºå¦‚ä½•ä½¿ç”¨LangChainå’ŒLangGraphå»ºæ§‹ä¸€å€‹æœ‰ç‹€æ…‹çš„ã€åŸºæ–¼åœ–çš„ä»£ç†ç³»çµ±ï¼Œç”¨æ–¼è‡¨åºŠè©¦é©—è‡ªç„¶èªè¨€æ¨ç†(NLI)ã€‚LangChainæä¾›äº†æœ€æˆç†Ÿå’ŒåŠŸèƒ½è±å¯Œçš„LLMæ‡‰ç”¨ç”Ÿæ…‹ç³»çµ±ã€‚
# 
# ## ğŸ“š å­¸ç¿’ç›®æ¨™
# å®Œæˆæœ¬æ•™å­¸å¾Œï¼Œæ‚¨å°‡å­¸æœƒï¼š
# - ç†è§£ LangChain ç”Ÿæ…‹ç³»çµ±å’Œ LangGraph çš„ç‹€æ…‹ç®¡ç†
# - å»ºæ§‹åŸºæ–¼åœ–çš„å·¥ä½œæµç¨‹å’Œç‹€æ…‹ç®¡ç†
# - å¯¦ä½œè¤‡é›œçš„å¤šæ­¥é©Ÿæ¨ç†æµç¨‹
# - ä½¿ç”¨ SQLite æª¢æŸ¥é»é€²è¡Œå°è©±æŒçºŒæ€§
# 
# ### ç‚ºä»€éº¼é¸æ“‡ LangChain/LangGraphï¼Ÿ
# - **æˆç†Ÿç”Ÿæ…‹ç³»çµ±**: æœ€å®Œå–„çš„LLMæ‡‰ç”¨æ¡†æ¶
# - **è±å¯Œæ•´åˆ**: å»£æ³›çš„å·¥å…·å’Œæœå‹™æ•´åˆ
# - **æœ‰ç‹€æ…‹å·¥ä½œæµç¨‹**: LangGraphæ”¯æ´è¤‡é›œçš„æœ‰ç‹€æ…‹ä»£ç†äº’å‹•
# - **é€²éšæ¨¡å¼**: æ”¯æ´è¤‡é›œçš„æ¨ç†å’Œæ±ºç­–æ¨¡å¼  
# - **ç¤¾ç¾¤æ”¯æ´**: é¾å¤§ç¤¾ç¾¤å’Œè±å¯Œæ–‡æª”
# - **ç”Ÿç”¢å°±ç·’**: åœ¨çœ¾å¤šå¯¦éš›æ‡‰ç”¨ä¸­ç¶“éå¯¦æˆ°é©—è­‰
# 
# ### ğŸ”„ ä½¿ç”¨ LangGraph çš„ä»£ç†æ¶æ§‹
# æˆ‘å€‘å°‡å¯¦ä½œä¸€å€‹åŸºæ–¼åœ–çš„å·¥ä½œæµç¨‹ï¼ŒåŒ…å«æœ‰ç‹€æ…‹ä»£ç†ï¼š
# 1. **è‡¨åºŠè³‡æ–™æå–å™¨**: è™•ç†å’Œçµæ§‹åŒ–è©¦é©—è³‡æ–™
# 2. **é†«ç™‚åˆ†æç¯€é»**: å°ˆæ¥­é†«ç™‚æ¨ç†
# 3. **çµ±è¨ˆåˆ†æç¯€é»**: æ•¸å€¼å’Œçµ±è¨ˆé©—è­‰
# 4. **é‚è¼¯é©—è­‰ç¯€é»**: é‚è¼¯ä¸€è‡´æ€§æª¢æŸ¥
# 5. **æ±ºç­–ç¶œåˆç¯€é»**: æœ€çµ‚è˜Šå«åˆ†é¡
# 6. **ç‹€æ…‹ç®¡ç†**: æ•´å€‹åˆ†æå·¥ä½œæµç¨‹çš„æŒä¹…ç‹€æ…‹
# 
# > ğŸ”— **é—œéµæ¦‚å¿µ**: LangGraph å°‡å·¥ä½œæµç¨‹è¦–ç‚ºæœ‰å‘åœ–ï¼Œå…¶ä¸­æ¯å€‹ç¯€é»éƒ½æ˜¯ä¸€å€‹åŠŸèƒ½ï¼Œé‚Šä»£è¡¨è³‡æ–™æµå‹•ã€‚é€™ä½¿å¾—è¤‡é›œçš„å¤šæ­¥é©Ÿæ¨ç†è®Šå¾—å¯ç®¡ç†å’Œå¯è¿½è¹¤ã€‚

# In[ ]:


# ğŸ”§ Colab ç’°å¢ƒè¨­ç½® - ä¸€éµå®‰è£ LangChain/LangGraph ç›¸é—œå¥—ä»¶
# é€™å€‹cellæœƒéœé»˜å®‰è£æ‰€æœ‰LangChainç”Ÿæ…‹ç³»çµ±æ‰€éœ€çš„å¥—ä»¶
get_ipython().system('pip install -q langchain langchain-google-genai langgraph python-dotenv pandas tqdm')
get_ipython().system('pip install -q langchain-core langchain-community gdown')

print("âœ… LangChain/LangGraph ç”Ÿæ…‹ç³»çµ±å®‰è£å®Œæˆï¼å¯ä»¥é–‹å§‹å»ºæ§‹æœ‰ç‹€æ…‹çš„ä»£ç†å·¥ä½œæµç¨‹äº†")


# In[ ]:


# ğŸ“¥ å¾ Google Drive ä¸‹è¼‰è¨“ç·´è³‡æ–™
# é€™å€‹cellæœƒè‡ªå‹•ä¸‹è¼‰ä¸¦è§£å£“ç¸® clinicaltrial-nlp.zipï¼Œç¢ºä¿åœ¨Colabä¸­å¯ä»¥ç›´æ¥é‹è¡Œ
import os
import gdown
import zipfile
import shutil

# Google Drive zip æª”æ¡ˆ ID
file_id = "15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR"
zip_url = f"https://drive.google.com/uc?id={file_id}"
zip_filename = "clinicaltrial-nlp.zip"

# æª¢æŸ¥æ˜¯å¦å·²æœ‰è¨“ç·´è³‡æ–™
if not os.path.exists("training_data"):
    print("ğŸ“¥ å¾ Google Drive ä¸‹è¼‰ clinicaltrial-nlp.zip...")
    print("âš ï¸ å¦‚æœä¸‹è¼‰å¤±æ•—ï¼Œè«‹ç¢ºèª:")
    print("1. Google Drive é€£çµçš„æ¬Šé™è¨­å®šç‚º 'çŸ¥é“é€£çµçš„ä½¿ç”¨è€…'")
    print("2. ç¶²è·¯é€£ç·šæ­£å¸¸")
    print(f"3. æª”æ¡ˆé€£çµ: {zip_url}")
    
    try:
        # ä¸‹è¼‰ zip æª”æ¡ˆ
        print("ğŸ“¥ æ­£åœ¨ä¸‹è¼‰ zip æª”æ¡ˆ...")
        gdown.download(zip_url, zip_filename, quiet=False)
        
        # è§£å£“ç¸®æª”æ¡ˆ
        print("ğŸ“¦ æ­£åœ¨è§£å£“ç¸®æª”æ¡ˆ...")
        with zipfile.ZipFile(zip_filename, 'r') as zip_ref:
            zip_ref.extractall(".")
        
        # ç§»å‹• training_data åˆ°æ­£ç¢ºä½ç½®ï¼ˆå¦‚æœåœ¨å­è³‡æ–™å¤¾ä¸­ï¼‰
        if os.path.exists("clintrial-nlp/training_data") and not os.path.exists("training_data"):
            print("ğŸ“ ç§»å‹• training_data åˆ°æ­£ç¢ºä½ç½®...")
            shutil.move("clintrial-nlp/training_data", "training_data")
            # æ¸…ç†è§£å£“ç¸®çš„è³‡æ–™å¤¾
            if os.path.exists("clintrial-nlp"):
                shutil.rmtree("clintrial-nlp")
            if os.path.exists("__MACOSX"):  # æ¸…ç† macOS ç”¢ç”Ÿçš„éš±è—æª”æ¡ˆ
                shutil.rmtree("__MACOSX")
        
        # æ¸…ç† zip æª”æ¡ˆ
        os.remove(zip_filename)
        print("âœ… è¨“ç·´è³‡æ–™ä¸‹è¼‰ä¸¦è§£å£“ç¸®å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ ä¸‹è¼‰å¤±æ•—: {e}")
        print("\nğŸ”§ æ‰‹å‹•è§£æ±ºæ–¹æ¡ˆ:")
        print("1. é»æ“Šæ­¤é€£çµä¸‹è¼‰ zip æª”æ¡ˆ:")
        print("   https://drive.google.com/file/d/15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR/view?usp=sharing")
        print("2. ä¸Šå‚³ zip æª”æ¡ˆåˆ° Colab")
        print("3. è§£å£“ç¸®å¾Œé‡æ–°åŸ·è¡Œå¾ŒçºŒçš„ cells")
        
        # å‰µå»ºä¸€å€‹æç¤ºæª”æ¡ˆ
        os.makedirs("training_data", exist_ok=True)
        with open("training_data/DOWNLOAD_INSTRUCTIONS.txt", "w", encoding="utf-8") as f:
            f.write("è«‹æ‰‹å‹•ä¸‹è¼‰ä¸¦è§£å£“ç¸® clinicaltrial-nlp.zip:\n")
            f.write("https://drive.google.com/file/d/15GA5XI39DDxQ5QkIZXsFbApx1yEvCpcR/view?usp=sharing\n")
        
        print("\nğŸ“ å·²å‰µå»ºä¸‹è¼‰æŒ‡ç¤ºæª”æ¡ˆæ–¼ training_data/DOWNLOAD_INSTRUCTIONS.txt")
else:
    print("âœ… è¨“ç·´è³‡æ–™å·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰")

# æª¢æŸ¥ä¸‹è¼‰çš„è³‡æ–™çµæ§‹
if os.path.exists("training_data"):
    contents = os.listdir("training_data")
    print(f"ğŸ“‚ è³‡æ–™å¤¾å…§å®¹: {contents}")
    if os.path.exists("training_data/CT json"):
        ct_files = len([f for f in os.listdir("training_data/CT json") if f.endswith('.json')])
        print(f"ğŸ“„ æ‰¾åˆ° {ct_files} å€‹è‡¨åºŠè©¦é©—JSONæª”æ¡ˆ")
    else:
        print("âš ï¸ æ‰¾ä¸åˆ° 'CT json' å­è³‡æ–™å¤¾ï¼Œè«‹æª¢æŸ¥ä¸‹è¼‰æ˜¯å¦å®Œæ•´")


# In[ ]:


# ğŸ§ª æº–å‚™æ¸¬è©¦è³‡æ–™é›†
import json

def create_test_data_if_needed():
    if not os.path.exists("test.json"):
        try:
            with open("training_data/train.json", "r", encoding="utf-8") as f:
                train_data = json.load(f)
            test_data = dict(list(train_data.items())[:100])
            with open("test.json", "w", encoding="utf-8") as f:
                json.dump(test_data, f, indent=2, ensure_ascii=False)
            print(f"âœ… å·²å‰µå»ºæ¸¬è©¦è³‡æ–™é›†ï¼ŒåŒ…å« {len(test_data)} å€‹æ¨£æœ¬")
        except Exception as e:
            print(f"âŒ å‰µå»ºæ¸¬è©¦è³‡æ–™å¤±æ•—: {e}")
    else:
        print("âœ… test.json å·²å­˜åœ¨")

create_test_data_if_needed()


# ## ç’°å¢ƒè¨­ç½®å’Œå®‰è£
# 
# é¦–å…ˆï¼Œè®“æˆ‘å€‘è¨­ç½®ç’°å¢ƒä¸¦åŒ¯å…¥å¿…è¦çš„å‡½å¼åº«ï¼š
# 
# > ğŸ“ **èªªæ˜**: LangChain ç”Ÿæ…‹ç³»çµ±åŒ…å«å¤šå€‹çµ„ä»¶ï¼Œæˆ‘å€‘éœ€è¦åŒ¯å…¥æ ¸å¿ƒåŠŸèƒ½ã€Google Geminiæ•´åˆä»¥åŠLangGraphç‹€æ…‹ç®¡ç†ã€‚

# In[ ]:


# Load environment variables
from dotenv import load_dotenv
import os

load_dotenv()
print("âœ… Environment loaded")


# In[ ]:


# åŒ¯å…¥å¿…è¦çš„å‡½å¼åº«
# LangChain ç”Ÿæ…‹ç³»çµ±æä¾›è±å¯Œçš„LLMæ‡‰ç”¨é–‹ç™¼å·¥å…·
import json
import pandas as pd
from tqdm import tqdm
from typing import Dict, List, Any, Optional, TypedDict, Annotated
import warnings
warnings.filterwarnings('ignore')

# LangChain æ ¸å¿ƒçµ„ä»¶
from langchain_google_genai import ChatGoogleGenerativeAI  # Google Geminiæ•´åˆ
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage  # è¨Šæ¯é¡å‹
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate  # æç¤ºæ¨¡æ¿
from langchain_core.output_parsers import StrOutputParser  # è¼¸å‡ºè§£æå™¨
from langchain_core.runnables import RunnablePassthrough  # å¯é‹è¡Œéˆ
from langchain.schema import Document  # æ–‡æª”çµæ§‹
from langchain.text_splitter import RecursiveCharacterTextSplitter  # æ–‡å­—åˆ†å‰²å™¨

# LangGraph ç‹€æ…‹ç®¡ç†çµ„ä»¶
from langgraph.graph import StateGraph, END  # ç‹€æ…‹åœ–å’ŒçµæŸç¯€é»
from langgraph.checkpoint.sqlite import SqliteSaver  # SQLiteæª¢æŸ¥é»ä¿å­˜å™¨
from langgraph.prebuilt import ToolExecutor  # å·¥å…·åŸ·è¡Œå™¨
import operator  # é‹ç®—ç¬¦

print("âœ… æ‰€æœ‰LangChain/LangGraphçµ„ä»¶åŒ¯å…¥æˆåŠŸ")

# ğŸ§© æ¶æ§‹èªªæ˜ï¼šLangChainæä¾›æ¨¡çµ„åŒ–çµ„ä»¶ï¼ŒLangGraphå¢åŠ ç‹€æ…‹ç®¡ç†å’Œåœ–åŸ·è¡Œèƒ½åŠ›


# ## Data Loading and Utilities
# 
# Let's create utility functions for loading and processing clinical trial data:

# In[ ]:


def load_clinical_trial(trial_id: str) -> Dict[str, Any]:
    """Load clinical trial data from JSON file.
    
    Args:
        trial_id: The NCT identifier for the clinical trial
        
    Returns:
        Dictionary containing trial data or error information
    """
    try:
        file_path = os.path.join("training_data", "CT json", f"{trial_id}.json")
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        return data
    except FileNotFoundError:
        return {"error": f"Clinical trial {trial_id} not found"}
    except Exception as e:
        return {"error": f"Error loading {trial_id}: {str(e)}"}

def load_dataset(filepath: str) -> Dict[str, Any]:
    """Load training or test dataset.
    
    Args:
        filepath: Path to the JSON dataset file
        
    Returns:
        Dictionary containing the dataset
    """
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading dataset: {e}")
        return {}

def create_trial_documents(trial_data: Dict[str, Any]) -> List[Document]:
    """Create LangChain documents from trial data for better processing.
    
    Args:
        trial_data: Clinical trial data dictionary
        
    Returns:
        List of Document objects for LangChain processing
    """
    if "error" in trial_data:
        return [Document(page_content=f"Error: {trial_data['error']}", metadata={"section": "error"})]
    
    documents = []
    trial_id = trial_data.get("Clinical Trial ID", "Unknown")
    
    # Create documents for each section
    sections = {
        "Eligibility": trial_data.get("Eligibility", []),
        "Intervention": trial_data.get("Intervention", []),
        "Results": trial_data.get("Results", []),
        "Adverse_Events": trial_data.get("Adverse_Events", [])
    }
    
    for section_name, section_data in sections.items():
        if section_data:
            if isinstance(section_data, list):
                content = "\n".join(str(item) for item in section_data)
            else:
                content = str(section_data)
            
            documents.append(Document(
                page_content=content,
                metadata={
                    "trial_id": trial_id,
                    "section": section_name
                }
            ))
    
    return documents

# Test utilities
sample_trial = load_clinical_trial("NCT00066573")
sample_docs = create_trial_documents(sample_trial)
print(f"âœ… Data utilities ready. Sample trial: {sample_trial.get('Clinical Trial ID', 'Error')}")
print(f"Created {len(sample_docs)} documents from sample trial")


# ## Model Configuration
# 
# Set up the ChatGoogleGenerativeAI model for LangChain:

# In[ ]:


# Initialize ChatGoogleGenerativeAI model with flexible API key support
import os

# Support both GEMINI_API_KEY and GOOGLE_API_KEY environment variables
api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")

if not api_key:
    print("âš ï¸ Please set GEMINI_API_KEY or GOOGLE_API_KEY environment variable")
    print("You can set it in Colab's 'Secrets' panel or use:")
    print("import os")
    print("os.environ['GEMINI_API_KEY'] = 'your-api-key'")
    print("or")
    print("os.environ['GOOGLE_API_KEY'] = 'your-api-key'")
    raise ValueError("Missing API key")
else:
    print(f"âœ… Found API key: {api_key[:8]}...{api_key[-4:]}")

# Test API connection
try:
    import google.generativeai as genai
    genai.configure(api_key=api_key)
    test_model = genai.GenerativeModel("gemini-2.5-flash")
    test_response = test_model.generate_content("Hello, respond with 'API test successful'")
    print(f"âœ… API connection test successful: {test_response.text[:50]}...")
except Exception as e:
    print(f"âŒ API connection test failed: {e}")
    raise

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.1,  # Low temperature for consistent results
    google_api_key=api_key
)

# Initialize checkpointer for state persistence
checkpointer = SqliteSaver.from_conn_string(":memory:")

print("âœ… Model and checkpointer configured")


# ## ç‹€æ…‹å®šç¾©
# 
# ç‚ºæˆ‘å€‘çš„LangGraphå·¥ä½œæµç¨‹å®šç¾©ç‹€æ…‹çµæ§‹ï¼š
# 
# > ğŸ”„ **ç‹€æ…‹ç®¡ç†èªªæ˜**: LangGraphçš„æ ¸å¿ƒæ¦‚å¿µæ˜¯ç¶­è­·ä¸€å€‹åœ¨æ•´å€‹å·¥ä½œæµç¨‹ä¸­æŒçºŒå­˜åœ¨çš„ç‹€æ…‹ã€‚é€™å€‹ç‹€æ…‹åŒ…å«æ‰€æœ‰åˆ†ææ­¥é©Ÿçš„è¼¸å…¥ã€ä¸­é–“çµæœå’Œæœ€çµ‚è¼¸å‡ºã€‚

# In[ ]:


class ClinicalAnalysisState(TypedDict):
    """è‡¨åºŠè©¦é©—åˆ†æå·¥ä½œæµç¨‹çš„ç‹€æ…‹æ¶æ§‹ã€‚"""
    
    # è¼¸å…¥è³‡æ–™
    statement: str  # è¦åˆ†æçš„é™³è¿°
    primary_trial_id: str  # ä¸»è¦è©¦é©—ID
    secondary_trial_id: Optional[str]  # æ¬¡è¦è©¦é©—IDï¼ˆæ¯”è¼ƒæ™‚ä½¿ç”¨ï¼‰
    focus_section: Optional[str]  # é—œæ³¨çš„è©¦é©—å€æ®µ
    
    # è©¦é©—è³‡æ–™
    primary_trial_data: Dict[str, Any]  # ä¸»è¦è©¦é©—çš„å®Œæ•´è³‡æ–™
    secondary_trial_data: Optional[Dict[str, Any]]  # æ¬¡è¦è©¦é©—è³‡æ–™
    trial_documents: List[Document]  # LangChainæ–‡æª”æ ¼å¼çš„è©¦é©—è³‡æ–™
    
    # åˆ†æçµæœ
    medical_analysis: Optional[str]  # é†«ç™‚å°ˆå®¶åˆ†æçµæœ
    statistical_analysis: Optional[str]  # çµ±è¨ˆåˆ†æçµæœ
    logical_analysis: Optional[str]  # é‚è¼¯åˆ†æçµæœ
    
    # æœ€çµ‚æ±ºç­–
    final_decision: Optional[str]  # æœ€çµ‚çš„è˜Šå«/çŸ›ç›¾æ±ºç­–
    confidence_score: Optional[float]  # ä¿¡å¿ƒåˆ†æ•¸(0-1)
    
    # å·¥ä½œæµç¨‹æ§åˆ¶
    next_action: Optional[str]  # ä¸‹ä¸€å€‹è¦åŸ·è¡Œçš„å‹•ä½œ
    error_messages: Annotated[List[str], operator.add]  # éŒ¯èª¤è¨Šæ¯ç´¯ç©

print("âœ… ç‹€æ…‹æ¶æ§‹å®šç¾©å®Œæˆ")

# ğŸ“Š ç‹€æ…‹èªªæ˜ï¼šé€™å€‹TypedDictå®šç¾©äº†å·¥ä½œæµç¨‹ä¸­æ¯å€‹ç¯€é»å¯ä»¥è®€å–å’Œä¿®æ”¹çš„è³‡æ–™çµæ§‹


# ## Node Definitions
# 
# Define the analysis nodes for our LangGraph workflow:

# In[ ]:


def clinical_data_extractor(state: ClinicalAnalysisState) -> ClinicalAnalysisState:
    """Extract and structure clinical trial data."""
    
    try:
        # Load primary trial data
        primary_data = load_clinical_trial(state["primary_trial_id"])
        state["primary_trial_data"] = primary_data
        
        # Load secondary trial data if provided
        if state["secondary_trial_id"]:
            secondary_data = load_clinical_trial(state["secondary_trial_id"])
            state["secondary_trial_data"] = secondary_data
        
        # Create documents for processing
        documents = create_trial_documents(primary_data)
        if state["secondary_trial_data"]:
            documents.extend(create_trial_documents(state["secondary_trial_data"]))
        
        state["trial_documents"] = documents
        state["next_action"] = "medical_analysis"
        
    except Exception as e:
        state["error_messages"].append(f"Data extraction error: {str(e)}")
        state["next_action"] = "end"
    
    return state

print("âœ… Data extractor node defined")


# In[ ]:


def medical_analysis_node(state: ClinicalAnalysisState) -> ClinicalAnalysisState:
    """Perform medical analysis of the statement against trial data."""
    
    try:
        # Create medical analysis prompt
        medical_prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""
You are a Medical Expert specializing in clinical trial analysis.
Your role is to analyze statements from a medical perspective and assess their accuracy against clinical trial evidence.

Focus on:
- Medical terminology accuracy
- Clinical relevance and significance
- Medical plausibility of claims
- Clinical context and implications
- Medical evidence alignment

Provide a thorough medical analysis and end with:
MEDICAL_VERDICT: [SUPPORTS/CONTRADICTS/UNCLEAR] - brief reasoning
            """.strip()),
            HumanMessage(content="""
STATEMENT TO ANALYZE: "{statement}"

CLINICAL TRIAL EVIDENCE:
{trial_evidence}

Please provide your medical analysis of this statement against the trial evidence.
            """.strip())
        ])
        
        # Prepare trial evidence
        trial_evidence = ""
        for doc in state["trial_documents"]:
            if state["focus_section"] and doc.metadata.get("section") != state["focus_section"]:
                continue
            trial_evidence += f"\n{doc.metadata.get('section', 'Unknown')}:\n{doc.page_content}\n"
        
        # Run medical analysis
        medical_chain = medical_prompt | llm | StrOutputParser()
        medical_result = medical_chain.invoke({
            "statement": state["statement"],
            "trial_evidence": trial_evidence
        })
        
        state["medical_analysis"] = medical_result
        state["next_action"] = "statistical_analysis"
        
    except Exception as e:
        state["error_messages"].append(f"Medical analysis error: {str(e)}")
        state["next_action"] = "statistical_analysis"  # Continue with next analysis
    
    return state

print("âœ… Medical analysis node defined")


# In[ ]:


def statistical_analysis_node(state: ClinicalAnalysisState) -> ClinicalAnalysisState:
    """Perform statistical and numerical analysis."""
    
    try:
        # Create statistical analysis prompt
        statistical_prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""
You are a Statistical Analyst specializing in clinical trial data analysis.
Your role is to analyze numerical claims, statistics, and quantitative relationships in clinical trials.

Focus on:
- Numerical accuracy and verification
- Statistical significance and validity
- Quantitative relationships and comparisons
- Data calculations and mathematical reasoning
- Confidence intervals and error margins

Perform detailed calculations and end with:
STATISTICAL_VERDICT: [ACCURATE/INACCURATE/PARTIALLY_ACCURATE] - numerical reasoning
            """.strip()),
            HumanMessage(content="""
STATEMENT TO ANALYZE: "{statement}"

CLINICAL TRIAL DATA:
{trial_evidence}

Please perform statistical analysis of the numerical claims in this statement.
            """.strip())
        ])
        
        # Prepare trial evidence (focus on Results section for statistical data)
        trial_evidence = ""
        for doc in state["trial_documents"]:
            # Prioritize Results and statistical sections
            if doc.metadata.get("section") in ["Results", "Adverse_Events"] or not state["focus_section"]:
                trial_evidence += f"\n{doc.metadata.get('section', 'Unknown')}:\n{doc.page_content}\n"
        
        # Run statistical analysis
        statistical_chain = statistical_prompt | llm | StrOutputParser()
        statistical_result = statistical_chain.invoke({
            "statement": state["statement"],
            "trial_evidence": trial_evidence
        })
        
        state["statistical_analysis"] = statistical_result
        state["next_action"] = "logical_analysis"
        
    except Exception as e:
        state["error_messages"].append(f"Statistical analysis error: {str(e)}")
        state["next_action"] = "logical_analysis"  # Continue with next analysis
    
    return state

print("âœ… Statistical analysis node defined")


# In[ ]:


def logical_analysis_node(state: ClinicalAnalysisState) -> ClinicalAnalysisState:
    """Perform logical consistency analysis."""
    
    try:
        # Create logical analysis prompt
        logical_prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""
You are a Logic Analyst specializing in reasoning validation and consistency checking.
Your role is to validate logical relationships, consistency, and reasoning soundness.

Focus on:
- Logical structure and coherence
- Cause-and-effect relationships
- Internal consistency
- Validity of inferences
- Detection of logical fallacies
- Reasoning pattern analysis

Provide logical analysis and end with:
LOGICAL_VERDICT: [SOUND/UNSOUND/QUESTIONABLE] - logical reasoning
            """.strip()),
            HumanMessage(content="""
STATEMENT TO ANALYZE: "{statement}"

EVIDENCE CONTEXT:
{trial_evidence}

Please analyze the logical consistency and reasoning of this statement.
            """.strip())
        ])
        
        # Prepare trial evidence
        trial_evidence = ""
        for doc in state["trial_documents"]:
            if state["focus_section"] and doc.metadata.get("section") != state["focus_section"]:
                continue
            trial_evidence += f"\n{doc.metadata.get('section', 'Unknown')}:\n{doc.page_content[:500]}...\n"
        
        # Run logical analysis
        logical_chain = logical_prompt | llm | StrOutputParser()
        logical_result = logical_chain.invoke({
            "statement": state["statement"],
            "trial_evidence": trial_evidence
        })
        
        state["logical_analysis"] = logical_result
        state["next_action"] = "decision_synthesis"
        
    except Exception as e:
        state["error_messages"].append(f"Logical analysis error: {str(e)}")
        state["next_action"] = "decision_synthesis"  # Continue to final decision
    
    return state

print("âœ… Logical analysis node defined")


# In[ ]:


def decision_synthesis_node(state: ClinicalAnalysisState) -> ClinicalAnalysisState:
    """Synthesize all analyses and make final decision."""
    
    try:
        # Create decision synthesis prompt
        decision_prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content="""
You are the Decision Synthesizer responsible for making final entailment classifications.
Your role is to synthesize expert analyses and determine the final verdict.

Classification Rules:
- ENTAILMENT: Statement is directly supported by the trial evidence
- CONTRADICTION: Statement is refuted or contradicted by the trial evidence

Weigh all evidence types:
- Medical expert analysis (clinical accuracy)
- Statistical analysis (numerical validity)
- Logical analysis (reasoning soundness)

Provide reasoning and confidence, then end with:
FINAL_DECISION: [Entailment/Contradiction]
CONFIDENCE: [0.0-1.0]
            """.strip()),
            HumanMessage(content="""
ORIGINAL STATEMENT: "{statement}"

MEDICAL ANALYSIS:
{medical_analysis}

STATISTICAL ANALYSIS:
{statistical_analysis}

LOGICAL ANALYSIS:
{logical_analysis}

Based on these expert analyses, provide your final entailment decision.
            """.strip())
        ])
        
        # Run decision synthesis
        decision_chain = decision_prompt | llm | StrOutputParser()
        decision_result = decision_chain.invoke({
            "statement": state["statement"],
            "medical_analysis": state.get("medical_analysis", "Not available"),
            "statistical_analysis": state.get("statistical_analysis", "Not available"),
            "logical_analysis": state.get("logical_analysis", "Not available")
        })
        
        # Parse decision and confidence
        if "FINAL_DECISION: Entailment" in decision_result:
            final_decision = "Entailment"
        elif "FINAL_DECISION: Contradiction" in decision_result:
            final_decision = "Contradiction"
        else:
            # Fallback parsing
            if "entailment" in decision_result.lower() and "contradiction" not in decision_result.lower():
                final_decision = "Entailment"
            else:
                final_decision = "Contradiction"
        
        # Extract confidence score
        confidence = 0.5  # Default
        try:
            if "CONFIDENCE:" in decision_result:
                confidence_str = decision_result.split("CONFIDENCE:")[1].strip().split()[0]
                confidence = float(confidence_str)
        except:
            pass
        
        state["final_decision"] = final_decision
        state["confidence_score"] = confidence
        state["next_action"] = "end"
        
    except Exception as e:
        state["error_messages"].append(f"Decision synthesis error: {str(e)}")
        state["final_decision"] = "Contradiction"  # Conservative fallback
        state["confidence_score"] = 0.1
        state["next_action"] = "end"
    
    return state

print("âœ… Decision synthesis node defined")


# ## å·¥ä½œæµç¨‹å®šç¾©
# 
# é€éé€£æ¥æ‰€æœ‰ç¯€é»ä¾†å‰µå»ºLangGraphå·¥ä½œæµç¨‹ï¼š
# 
# > ğŸ”— **åœ–çµæ§‹èªªæ˜**: LangGraphå·¥ä½œæµç¨‹å°±åƒä¸€å€‹æµç¨‹åœ–ï¼Œæ¯å€‹ç¯€é»ä»£è¡¨ä¸€å€‹åˆ†ææ­¥é©Ÿï¼Œé‚Šç·£å®šç¾©è³‡æ–™å¦‚ä½•åœ¨ç¯€é»é–“æµå‹•ã€‚é€™ç¢ºä¿äº†å¯è¿½è¹¤å’Œå¯é‡ç¾çš„åˆ†æéç¨‹ã€‚

# In[ ]:


def create_clinical_analysis_workflow():
    """Create the clinical analysis workflow using LangGraph."""
    
    # Create the StateGraph
    workflow = StateGraph(ClinicalAnalysisState)
    
    # Add nodes
    workflow.add_node("data_extraction", clinical_data_extractor)
    workflow.add_node("medical_analysis", medical_analysis_node)
    workflow.add_node("statistical_analysis", statistical_analysis_node)
    workflow.add_node("logical_analysis", logical_analysis_node)
    workflow.add_node("decision_synthesis", decision_synthesis_node)
    
    # Add edges
    workflow.set_entry_point("data_extraction")
    workflow.add_edge("data_extraction", "medical_analysis")
    workflow.add_edge("medical_analysis", "statistical_analysis")
    workflow.add_edge("statistical_analysis", "logical_analysis")
    workflow.add_edge("logical_analysis", "decision_synthesis")
    workflow.add_edge("decision_synthesis", END)
    
    # Compile the workflow
    app = workflow.compile(checkpointer=checkpointer)
    
    return app

# Create the workflow
clinical_workflow = create_clinical_analysis_workflow()

print("âœ… LangGraph workflow created")


# ## Analysis Pipeline
# 
# Create the main pipeline function that uses our LangGraph workflow:

# In[ ]:


def langchain_analysis_pipeline(statement: str, primary_id: str, secondary_id: Optional[str] = None, 
                               section_id: Optional[str] = None, verbose: bool = False) -> str:
    """
    Run the complete LangChain/LangGraph analysis pipeline.
    
    Args:
        statement: The natural language statement to analyze
        primary_id: Primary clinical trial ID
        secondary_id: Secondary trial ID for comparison statements
        section_id: Relevant section of the trial
        verbose: Whether to print intermediate results
        
    Returns:
        Final decision: 'Entailment' or 'Contradiction'
    """
    
    try:
        if verbose:
            print(f"ğŸ“„ Analyzing: {statement[:100]}...")
            print(f"ğŸ¥ Primary Trial: {primary_id}")
            if secondary_id:
                print(f"ğŸ¥ Secondary Trial: {secondary_id}")
        
        # Create initial state
        initial_state = {
            "statement": statement,
            "primary_trial_id": primary_id,
            "secondary_trial_id": secondary_id,
            "focus_section": section_id,
            "primary_trial_data": {},
            "secondary_trial_data": None,
            "trial_documents": [],
            "medical_analysis": None,
            "statistical_analysis": None,
            "logical_analysis": None,
            "final_decision": None,
            "confidence_score": None,
            "next_action": None,
            "error_messages": []
        }
        
        # Run the workflow
        config = {"configurable": {"thread_id": f"analysis_{hash(statement)}"[:10]}}
        result = clinical_workflow.invoke(initial_state, config)
        
        if verbose:
            print(f"ğŸ©º Medical Analysis: {'âœ…' if result.get('medical_analysis') else 'âŒ'}")
            print(f"ğŸ“Š Statistical Analysis: {'âœ…' if result.get('statistical_analysis') else 'âŒ'}")
            print(f"ğŸ§  Logical Analysis: {'âœ…' if result.get('logical_analysis') else 'âŒ'}")
            print(f"âš–ï¸ Final Decision: {result.get('final_decision', 'Unknown')}")
            print(f"ğŸ¯ Confidence: {result.get('confidence_score', 0.0):.2f}")
            
            if result.get("error_messages"):
                print(f"âš ï¸ Errors: {len(result['error_messages'])}")
            print("-" * 50)
        
        return result.get("final_decision", "Contradiction")
        
    except Exception as e:
        if verbose:
            print(f"âŒ Error in LangChain pipeline: {e}")
        return "Contradiction"  # Conservative fallback

print("âœ… LangChain analysis pipeline ready")


# ## Test Example
# 
# Let's test our LangChain/LangGraph system:

# In[ ]:


# Test with a sample statement
test_statement = "there is a 13.2% difference between the results from the two the primary trial cohorts"
test_primary_id = "NCT00066573"

print(f"Testing LangChain/LangGraph system with statement:")
print(f"'{test_statement}'")
print(f"Primary trial: {test_primary_id}")
print("\n" + "="*80)

# Run the analysis with verbose output
result = langchain_analysis_pipeline(
    statement=test_statement,
    primary_id=test_primary_id,
    section_id="Results",
    verbose=True
)

print(f"\nğŸ¯ LANGCHAIN RESULT: {result}")
print("="*80)


# ## Evaluation on Training Data
# 
# Let's evaluate our LangChain/LangGraph system on training data:

# In[ ]:


# Load training data
train_data = load_dataset("training_data/train.json")
print(f"Loaded {len(train_data)} training examples")

# Evaluate on a sample (adjust sample_size as needed)
sample_size = 20
examples = list(train_data.items())[:sample_size]

print(f"\nEvaluating LangChain/LangGraph system on {len(examples)} examples...")

results = []
correct = 0

for i, (uuid, example) in enumerate(tqdm(examples, desc="LangChain Processing")):
    try:
        statement = example.get("Statement")
        primary_id = example.get("Primary_id")
        secondary_id = example.get("Secondary_id")
        section_id = example.get("Section_id")
        expected = example.get("Label")
        
        if not statement or not primary_id:
            results.append({
                "uuid": uuid,
                "expected": expected,
                "predicted": "SKIPPED",
                "correct": False
            })
            continue
        
        # Get prediction from LangChain/LangGraph system
        predicted = langchain_analysis_pipeline(
            statement=statement,
            primary_id=primary_id,
            secondary_id=secondary_id,
            section_id=section_id,
            verbose=False
        )
        
        # Check if correct
        is_correct = (predicted.strip() == expected.strip())
        if is_correct:
            correct += 1
            
        results.append({
            "uuid": uuid,
            "statement": statement[:100] + "..." if len(statement) > 100 else statement,
            "expected": expected,
            "predicted": predicted,
            "correct": is_correct
        })
        
        status = "âœ…" if is_correct else "âŒ"
        print(f"Example {i+1:2d}: {expected:12} -> {predicted:12} {status}")
        
    except Exception as e:
        print(f"Error processing example {i+1}: {e}")
        results.append({
            "uuid": uuid,
            "expected": expected,
            "predicted": "ERROR",
            "correct": False
        })

# Calculate accuracy
accuracy = correct / len(examples) if examples else 0
print(f"\nğŸ“Š LangChain/LangGraph Results:")
print(f"Accuracy: {accuracy:.2%} ({correct}/{len(examples)})")

# Store results for comparison
langchain_results = results.copy()


# ## State Inspection
# 
# Let's inspect the stateful capabilities of our LangGraph workflow:

# In[ ]:


# Demonstrate state persistence and inspection
print("ğŸ” LangGraph State Inspection:")
print("=" * 50)

# Run a simple analysis and inspect intermediate states
test_config = {"configurable": {"thread_id": "demo_analysis"}}
test_state = {
    "statement": "The primary endpoint was met",
    "primary_trial_id": "NCT00066573",
    "secondary_trial_id": None,
    "focus_section": "Results",
    "primary_trial_data": {},
    "secondary_trial_data": None,
    "trial_documents": [],
    "medical_analysis": None,
    "statistical_analysis": None,
    "logical_analysis": None,
    "final_decision": None,
    "confidence_score": None,
    "next_action": None,
    "error_messages": []
}

try:
    # Stream the workflow execution to see intermediate states
    print("Streaming workflow execution:")
    for step in clinical_workflow.stream(test_state, test_config):
        for node_name, node_output in step.items():
            print(f"\nğŸ“ Node: {node_name}")
            if node_output.get("final_decision"):
                print(f"   Decision: {node_output['final_decision']}")
                print(f"   Confidence: {node_output.get('confidence_score', 'N/A')}")
            if node_output.get("error_messages"):
                print(f"   Errors: {len(node_output['error_messages'])}")
            print(f"   Next: {node_output.get('next_action', 'N/A')}")
    
    # Get final state
    final_state = clinical_workflow.get_state(test_config)
    print(f"\nğŸ¯ Final State Summary:")
    print(f"   Thread ID: {test_config['configurable']['thread_id']}")
    print(f"   Final Decision: {final_state.values.get('final_decision')}")
    print(f"   Total Errors: {len(final_state.values.get('error_messages', []))}")
    
except Exception as e:
    print(f"Error in state inspection: {e}")

print("\nâœ… State inspection completed")


# ## Generate Submission File
# 
# Let's generate a submission file using our LangChain/LangGraph system:

# In[ ]:


def generate_langchain_submission(test_file="test.json", output_file="langchain_submission.json", sample_size=None):
    """
    Generate submission file using LangChain/LangGraph system.
    
    Args:
        test_file: Path to test data
        output_file: Output submission file
        sample_size: Number of examples to process (None for all)
    """
    
    # Load test data
    test_data = load_dataset(test_file)
    if not test_data:
        print(f"âŒ Could not load test data from {test_file}")
        return
    
    examples = list(test_data.items())
    if sample_size:
        examples = examples[:sample_size]
        
    print(f"ğŸš€ Generating LangChain/LangGraph predictions for {len(examples)} examples...")
    
    submission = {}
    
    for i, (uuid, example) in enumerate(tqdm(examples, desc="LangChain Processing")):
        try:
            statement = example.get("Statement")
            primary_id = example.get("Primary_id")
            secondary_id = example.get("Secondary_id")
            section_id = example.get("Section_id")
            
            if not statement or not primary_id:
                submission[uuid] = {"Prediction": "Contradiction"}  # Default fallback
                continue
                
            # Get prediction from LangChain/LangGraph system
            prediction = langchain_analysis_pipeline(
                statement=statement,
                primary_id=primary_id,
                secondary_id=secondary_id,
                section_id=section_id,
                verbose=False
            )
            
            submission[uuid] = {"Prediction": prediction}
            
        except Exception as e:
            print(f"Error processing {uuid}: {e}")
            submission[uuid] = {"Prediction": "Contradiction"}  # Conservative fallback
    
    # Save submission file
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(submission, f, indent=2)
    
    print(f"âœ… LangChain submission saved to {output_file}")
    return submission

# Generate submission for a small sample
langchain_submission = generate_langchain_submission(
    test_file="test.json", 
    output_file="langchain_submission.json",
    sample_size=10  # Adjust as needed
)

print(f"Generated predictions for {len(langchain_submission)} examples")


# ## Workflow Visualization
# 
# Let's visualize our LangGraph workflow structure:

# In[ ]:


# Display workflow information
print("ğŸ”„ LangGraph Workflow Structure:")
print("=" * 50)

workflow_steps = [
    "1. Data Extraction â†’ Load and structure clinical trial data",
    "2. Medical Analysis â†’ Expert medical reasoning and assessment",
    "3. Statistical Analysis â†’ Numerical validation and calculations",
    "4. Logical Analysis â†’ Reasoning consistency and soundness",
    "5. Decision Synthesis â†’ Final entailment classification"
]

for step in workflow_steps:
    print(f"   {step}")

print("\nğŸ“Š State Management Features:")
state_features = [
    "â€¢ Persistent state across workflow steps",
    "â€¢ Error tracking and recovery mechanisms",
    "â€¢ Confidence scoring and decision rationale",
    "â€¢ Intermediate result storage and inspection",
    "â€¢ Thread-based conversation management"
]

for feature in state_features:
    print(f"   {feature}")

print("\nâœ… Workflow visualization complete")


# ## çµè«–èˆ‡æ´å¯Ÿ
# 
# ### LangChain/LangGraph æ¡†æ¶å„ªå‹¢ï¼š
# 1. **æˆç†Ÿç”Ÿæ…‹ç³»çµ±**: æœ€å®Œå–„ä¸”åŠŸèƒ½è±å¯Œçš„LLMæ‡‰ç”¨æ¡†æ¶
# 2. **æœ‰ç‹€æ…‹å·¥ä½œæµç¨‹**: LangGraphæ”¯æ´è¤‡é›œçš„æœ‰ç‹€æ…‹ä»£ç†äº’å‹•
# 3. **è±å¯Œæ•´åˆ**: å»£æ³›çš„å·¥å…·ç”Ÿæ…‹ç³»çµ±å’Œæœå‹™æ•´åˆ
# 4. **é€²éšæ¨¡å¼**: æ”¯æ´ç²¾å¯†çš„æ¨ç†å’Œæ±ºç­–æ¨¡å¼
# 5. **ç¤¾ç¾¤æ”¯æ´**: é¾å¤§ç¤¾ç¾¤ã€è±å¯Œæ–‡æª”å’Œå¯¦éš›ç¯„ä¾‹
# 6. **ç”Ÿç”¢å°±ç·’**: åœ¨çœ¾å¤šä¼æ¥­æ‡‰ç”¨ä¸­ç¶“éå¯¦æˆ°è€ƒé©—
# 
# ### é—œéµåŠŸèƒ½å±•ç¤ºï¼š
# - **åŸºæ–¼åœ–çš„å·¥ä½œæµç¨‹**: çµæ§‹åŒ–ã€æœ‰ç‹€æ…‹çš„åˆ†æç®¡é“
# - **ç‹€æ…‹æŒä¹…æ€§**: SQLiteæª¢æŸ¥é»ç”¨æ–¼å°è©±æŒçºŒæ€§
# - **ç¯€é»å°ˆæ¥­åŒ–**: é‡å°ä¸åŒé ˜åŸŸçš„å°ˆé–€åˆ†æç¯€é»
# - **éŒ¯èª¤è™•ç†**: å¼·å¥çš„éŒ¯èª¤è¿½è¹¤å’Œæ¢å¾©æ©Ÿåˆ¶
# - **ä¸²æµæ”¯æ´**: å³æ™‚å·¥ä½œæµç¨‹åŸ·è¡Œç›£æ§
# - **é…ç½®ç®¡ç†**: åŸºæ–¼ç·šç¨‹çš„ç‹€æ…‹ç®¡ç†
# 
# ### æ¶æ§‹å„ªå‹¢ï¼š
# - **å½ˆæ€§å·¥ä½œæµç¨‹**: æ˜“æ–¼ä¿®æ”¹å’Œæ“´å±•åˆ†æç®¡é“
# - **ç‹€æ…‹ç®¡ç†**: è·¨è¤‡é›œå¤šæ­¥é©Ÿæµç¨‹çš„æŒä¹…ä¸Šä¸‹æ–‡
# - **é™¤éŒ¯æ”¯æ´**: æ¸…æ™°çš„ç‹€æ…‹æª¢æŸ¥å’Œä¸­é–“çµæœè¿½è¹¤
# - **å¯æ“´å±•è¨­è¨ˆ**: ä¼æ¥­éƒ¨ç½²çš„ç”Ÿç”¢å°±ç·’æ¶æ§‹
# - **æ•´åˆå°±ç·’**: èˆ‡å¤–éƒ¨å·¥å…·å’Œæœå‹™çš„ç„¡ç¸«æ•´åˆ
# 
# ### å„ªåŒ–æ©Ÿæœƒï¼š
# 1. **æç¤ºå·¥ç¨‹**: å¾®èª¿æ¯å€‹åˆ†æç¯€é»çš„æç¤º
# 2. **æ¢ä»¶è·¯ç”±**: ç‚ºä¸åŒé™³è¿°é¡å‹æ·»åŠ æ¢ä»¶é‚è¼¯
# 3. **å¹³è¡Œè™•ç†**: åœ¨å¯èƒ½çš„åœ°æ–¹å¯¦ä½œå¹³è¡Œåˆ†æç¯€é»
# 4. **å·¥å…·æ•´åˆ**: åˆ©ç”¨LangChainå»£æ³›çš„å·¥å…·ç”Ÿæ…‹ç³»çµ±
# 5. **é€²éšæ¨¡å¼**: å¯¦ä½œè‡ªæˆ‘åæ€å’Œè¿­ä»£æ”¹é€²
# 
# ### ä½•æ™‚ä½¿ç”¨ LangChain/LangGraphï¼š
# - éœ€è¦ç‹€æ…‹ç®¡ç†çš„è¤‡é›œå¤šæ­¥é©Ÿæ¨ç†å·¥ä½œæµç¨‹
# - éœ€è¦å»£æ³›å·¥å…·å’Œæœå‹™æ•´åˆçš„æ‡‰ç”¨
# - éœ€è¦å¼·å¥ç”Ÿç”¢å°±ç·’æ¡†æ¶çš„ä¼æ¥­ç³»çµ±
# - ç¤¾ç¾¤æ”¯æ´å’Œæ–‡æª”è‡³é—œé‡è¦çš„å°ˆæ¡ˆ
# - éœ€è¦ç²¾å¯†å·¥ä½œæµç¨‹æ¨¡å¼å’Œå®¢è£½åŒ–çš„å ´æ™¯
# 
# ### æ¡†æ¶æ¯”è¼ƒç¸½çµï¼š
# - **vs AutoGen**: æ›´é©åˆçµæ§‹åŒ–å·¥ä½œæµç¨‹ï¼ŒAutoGenæ›´é©åˆè‡ªç”±å½¢å¼å°è©±
# - **vs Atomic Agents**: æ›´å…¨é¢ä½†è¼ƒé‡ï¼ŒAtomicæ›´é©åˆç´”æ•ˆèƒ½
# - **vs Agno**: æ›´å»£æ³›çš„ç”Ÿæ…‹ç³»çµ±ï¼ŒAgnoåœ¨å…§å»ºè¨˜æ†¶é«”å’ŒçŸ¥è­˜æ–¹é¢æ›´ä½³
# 
# ### LangChain/LangGraph ç¨ç‰¹å„ªå‹¢ï¼š
# 1. **åŸºæ–¼åœ–çš„æ¨ç†**: åŸç”Ÿæ”¯æ´è¤‡é›œçš„æ¢ä»¶å·¥ä½œæµç¨‹
# 2. **ç‹€æ…‹æŒä¹…æ€§**: å…§å»ºæª¢æŸ¥é»å’Œå°è©±æŒçºŒæ€§
# 3. **ç”Ÿæ…‹ç³»çµ±æˆç†Ÿåº¦**: å»£æ³›çš„å·¥å…·ã€æ•´åˆå’Œç¤¾ç¾¤æ”¯æ´
# 4. **ä¼æ¥­åŠŸèƒ½**: å…·å‚™ç›£æ§ã€è¨˜éŒ„å’Œé™¤éŒ¯çš„ç”Ÿç”¢å°±ç·’
# 5. **å½ˆæ€§**: é«˜åº¦å¯å®¢è£½åŒ–çš„å·¥ä½œæµç¨‹å’Œæ•´åˆæ¨¡å¼
# 
# ## ğŸ“ å­¸ç¿’é‡é»ç¸½çµ
# - **ç‹€æ…‹åœ–æ¦‚å¿µ**: å·¥ä½œæµç¨‹ä½œç‚ºæœ‰å‘åœ–ï¼Œç¯€é»æ˜¯åŠŸèƒ½ï¼Œé‚Šæ˜¯è³‡æ–™æµ
# - **æŒä¹…ç‹€æ…‹**: åœ¨æ•´å€‹åˆ†æéç¨‹ä¸­ç¶­è­·ä¸Šä¸‹æ–‡å’Œä¸­é–“çµæœ
# - **æ¨¡çµ„åŒ–è¨­è¨ˆ**: æ¯å€‹ç¯€é»å°ˆæ³¨æ–¼ç‰¹å®šåˆ†æä»»å‹™
# - **ä¼æ¥­å°±ç·’**: ç”Ÿç”¢ç’°å¢ƒçš„ç›£æ§ã€éŒ¯èª¤è™•ç†å’Œå¯æ“´å±•æ€§
# 
# LangChain/LangGraphåœ¨è¤‡é›œçš„ç”Ÿç”¢ç’°å¢ƒä¸­è¡¨ç¾å“è¶Šï¼Œç‰¹åˆ¥é©åˆéœ€è¦çµæ§‹åŒ–å·¥ä½œæµç¨‹ã€ç‹€æ…‹ç®¡ç†å’Œå»£æ³›æ•´åˆçš„è‡¨åºŠNLPæ‡‰ç”¨ã€‚å®ƒæ˜¯éœ€è¦ç²¾å¯†æ¨ç†æ¨¡å¼å’Œå¼·å¥åŸºç¤è¨­æ–½æ”¯æ´çš„ä¼æ¥­ç´šæ‡‰ç”¨çš„ç†æƒ³é¸æ“‡ã€‚
